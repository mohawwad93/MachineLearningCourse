{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Overview of the Problem set\n",
    "\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading boston dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset description :  .. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset description : \", boston_dataset.DESCR[:1226])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name   of the features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "Shape  of the dataset :  (506, 13)\n",
      "Number of the samples :  506\n",
      "Number of the features:  13\n",
      "Number of the targets :  (506,)\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "print(\"Name   of the features: \", boston_dataset.feature_names)\n",
    "print(\"Shape  of the dataset : \", boston_dataset.data.shape)\n",
    "print(\"Number of the samples : \", boston_dataset.data.shape[0])\n",
    "print(\"Number of the features: \", boston_dataset.data.shape[1])\n",
    "print(\"Number of the targets : \", boston_dataset.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a portion of the dataset\n",
    "target_col_name = 'PRICE'\n",
    "df = pd.DataFrame(np.hstack([boston_dataset.data, boston_dataset.target[:, np.newaxis]]), \n",
    "                  columns=np.hstack([boston_dataset.feature_names, target_col_name]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You should reshape iris target shape from (506,) to (506,1) to treat it as column vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, itâ€™s a good practice to see if there are any missing values in the data. We count the number of missing values for each feature using isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "PRICE      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "#df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the train samples :  455\n",
      "Number of the test samples :  51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df[boston_dataset.feature_names], df[target_col_name], train_size=0.9, random_state=42)\n",
    "print(\"Number of the train samples : \", X_train.shape[0])\n",
    "print(\"Number of the test samples : \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Feature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.095926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119136</td>\n",
       "      <td>0.652807</td>\n",
       "      <td>0.353244</td>\n",
       "      <td>0.374205</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.055344</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>0.988224</td>\n",
       "      <td>0.078918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.013751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>0.914521</td>\n",
       "      <td>0.258918</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.616976</td>\n",
       "      <td>0.704428</td>\n",
       "      <td>0.156999</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.375954</td>\n",
       "      <td>0.882979</td>\n",
       "      <td>0.996672</td>\n",
       "      <td>0.163907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.119630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390947</td>\n",
       "      <td>0.748994</td>\n",
       "      <td>0.511843</td>\n",
       "      <td>0.158445</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.983358</td>\n",
       "      <td>0.039459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.041191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.527112</td>\n",
       "      <td>0.504634</td>\n",
       "      <td>0.260264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.979121</td>\n",
       "      <td>0.244205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM   ZN     INDUS  CHAS       NOX        RM       AGE       DIS  \\\n",
       "281  0.000315  0.2  0.095926   0.0  0.119136  0.652807  0.353244  0.374205   \n",
       "22   0.013751  0.0  0.274074   0.0  0.314815  0.494539  0.914521  0.258918   \n",
       "101  0.001183  0.0  0.289630   0.0  0.277778  0.616976  0.704428  0.156999   \n",
       "268  0.005973  0.2  0.119630   0.0  0.390947  0.748994  0.511843  0.158445   \n",
       "485  0.041191  0.0  0.642963   0.0  0.407407  0.527112  0.504634  0.260264   \n",
       "\n",
       "          RAD       TAX   PTRATIO         B     LSTAT  \n",
       "281  0.173913  0.055344  0.244681  0.988224  0.078918  \n",
       "22   0.130435  0.229008  0.893617  1.000000  0.468819  \n",
       "101  0.173913  0.375954  0.882979  0.996672  0.163907  \n",
       "268  0.173913  0.146947  0.042553  0.983358  0.039459  \n",
       "485  1.000000  0.914122  0.808511  0.979121  0.244205  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scale =  MinMaxScaler()\n",
    "X_train = min_max_scale.fit_transform(X_train)\n",
    "X_test = min_max_scale.transform(X_test)\n",
    "X_train = pd.DataFrame(data=X_train, index=Y_train.index, columns= boston_dataset.feature_names)\n",
    "X_test = pd.DataFrame(data=X_test, index=Y_test.index, columns= boston_dataset.feature_names)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Neural Network Model\n",
    "\n",
    "We are going to build and train a Neural Network with a single hidden layer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               3584      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "clf = Sequential()\n",
    "\n",
    "clf.add(Dense(units=256, activation='relu', input_shape=(13,)))\n",
    "clf.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "clf.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])\n",
    "\n",
    "clf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 364 samples, validate on 91 samples\n",
      "Epoch 1/1000\n",
      "364/364 [==============================] - 0s 1ms/step - loss: 568.2957 - mae: 21.8376 - val_loss: 477.4724 - val_mae: 20.1454\n",
      "Epoch 2/1000\n",
      "364/364 [==============================] - 0s 291us/step - loss: 497.2311 - mae: 20.0537 - val_loss: 413.4433 - val_mae: 18.4233\n",
      "Epoch 3/1000\n",
      "364/364 [==============================] - 0s 282us/step - loss: 432.5927 - mae: 18.2664 - val_loss: 352.6557 - val_mae: 16.6220\n",
      "Epoch 4/1000\n",
      "364/364 [==============================] - 0s 311us/step - loss: 370.4911 - mae: 16.4277 - val_loss: 294.1167 - val_mae: 14.8261\n",
      "Epoch 5/1000\n",
      "364/364 [==============================] - 0s 478us/step - loss: 311.6099 - mae: 14.6119 - val_loss: 240.1513 - val_mae: 12.9728\n",
      "Epoch 6/1000\n",
      "364/364 [==============================] - 0s 447us/step - loss: 259.0745 - mae: 12.9645 - val_loss: 194.2596 - val_mae: 11.2833\n",
      "Epoch 7/1000\n",
      "364/364 [==============================] - 0s 445us/step - loss: 215.4189 - mae: 11.5743 - val_loss: 158.6699 - val_mae: 9.9979\n",
      "Epoch 8/1000\n",
      "364/364 [==============================] - 0s 434us/step - loss: 181.8871 - mae: 10.4785 - val_loss: 131.7235 - val_mae: 9.0757\n",
      "Epoch 9/1000\n",
      "364/364 [==============================] - 0s 224us/step - loss: 157.3676 - mae: 9.7457 - val_loss: 114.4520 - val_mae: 8.4107\n",
      "Epoch 10/1000\n",
      "364/364 [==============================] - 0s 287us/step - loss: 139.9650 - mae: 9.0950 - val_loss: 101.4470 - val_mae: 7.8701\n",
      "Epoch 11/1000\n",
      "364/364 [==============================] - 0s 280us/step - loss: 126.5957 - mae: 8.5913 - val_loss: 92.2376 - val_mae: 7.4348\n",
      "Epoch 12/1000\n",
      "364/364 [==============================] - 0s 220us/step - loss: 116.4745 - mae: 8.2280 - val_loss: 84.7040 - val_mae: 6.9934\n",
      "Epoch 13/1000\n",
      "364/364 [==============================] - 0s 234us/step - loss: 107.3830 - mae: 7.8375 - val_loss: 78.2608 - val_mae: 6.6140\n",
      "Epoch 14/1000\n",
      "364/364 [==============================] - 0s 308us/step - loss: 99.5434 - mae: 7.4528 - val_loss: 71.8470 - val_mae: 6.2513\n",
      "Epoch 15/1000\n",
      "364/364 [==============================] - 0s 219us/step - loss: 91.6294 - mae: 7.1137 - val_loss: 66.0497 - val_mae: 5.9183\n",
      "Epoch 16/1000\n",
      "364/364 [==============================] - 0s 264us/step - loss: 84.4995 - mae: 6.7777 - val_loss: 60.7409 - val_mae: 5.5752\n",
      "Epoch 17/1000\n",
      "364/364 [==============================] - 0s 469us/step - loss: 77.6843 - mae: 6.4278 - val_loss: 56.2729 - val_mae: 5.4585\n",
      "Epoch 18/1000\n",
      "364/364 [==============================] - 0s 325us/step - loss: 72.8370 - mae: 6.2354 - val_loss: 53.0381 - val_mae: 5.2492\n",
      "Epoch 19/1000\n",
      "364/364 [==============================] - 0s 300us/step - loss: 68.4545 - mae: 6.0527 - val_loss: 49.5226 - val_mae: 4.9013\n",
      "Epoch 20/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 64.7909 - mae: 5.7634 - val_loss: 47.6729 - val_mae: 4.9353\n",
      "Epoch 21/1000\n",
      "364/364 [==============================] - 0s 325us/step - loss: 61.7790 - mae: 5.7293 - val_loss: 44.6141 - val_mae: 4.5858\n",
      "Epoch 22/1000\n",
      "364/364 [==============================] - 0s 394us/step - loss: 59.3645 - mae: 5.4921 - val_loss: 43.0853 - val_mae: 4.5266\n",
      "Epoch 23/1000\n",
      "364/364 [==============================] - 0s 269us/step - loss: 57.6722 - mae: 5.4059 - val_loss: 41.9025 - val_mae: 4.4493\n",
      "Epoch 24/1000\n",
      "364/364 [==============================] - 0s 462us/step - loss: 55.7524 - mae: 5.3353 - val_loss: 40.8447 - val_mae: 4.3887\n",
      "Epoch 25/1000\n",
      "364/364 [==============================] - 0s 330us/step - loss: 54.3086 - mae: 5.2697 - val_loss: 39.9856 - val_mae: 4.4237\n",
      "Epoch 26/1000\n",
      "364/364 [==============================] - 0s 326us/step - loss: 52.4596 - mae: 5.2136 - val_loss: 39.3777 - val_mae: 4.4497\n",
      "Epoch 27/1000\n",
      "364/364 [==============================] - 0s 234us/step - loss: 51.3218 - mae: 5.2327 - val_loss: 38.0746 - val_mae: 4.1395\n",
      "Epoch 28/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 50.2332 - mae: 5.0772 - val_loss: 37.0160 - val_mae: 4.1305\n",
      "Epoch 29/1000\n",
      "364/364 [==============================] - 0s 338us/step - loss: 48.7243 - mae: 4.9689 - val_loss: 36.2382 - val_mae: 4.2039\n",
      "Epoch 30/1000\n",
      "364/364 [==============================] - 0s 439us/step - loss: 48.1001 - mae: 5.0281 - val_loss: 35.3219 - val_mae: 4.0357\n",
      "Epoch 31/1000\n",
      "364/364 [==============================] - 0s 239us/step - loss: 46.4815 - mae: 4.8478 - val_loss: 35.1867 - val_mae: 4.1918\n",
      "Epoch 32/1000\n",
      "364/364 [==============================] - 0s 276us/step - loss: 45.1543 - mae: 4.8951 - val_loss: 34.0670 - val_mae: 3.9181\n",
      "Epoch 33/1000\n",
      "364/364 [==============================] - 0s 349us/step - loss: 44.5441 - mae: 4.7868 - val_loss: 33.2027 - val_mae: 3.9865\n",
      "Epoch 34/1000\n",
      "364/364 [==============================] - 0s 327us/step - loss: 43.3765 - mae: 4.7635 - val_loss: 32.5742 - val_mae: 3.9627\n",
      "Epoch 35/1000\n",
      "364/364 [==============================] - 0s 331us/step - loss: 42.2669 - mae: 4.6731 - val_loss: 32.3869 - val_mae: 4.0205\n",
      "Epoch 36/1000\n",
      "364/364 [==============================] - 0s 256us/step - loss: 41.2428 - mae: 4.6845 - val_loss: 31.2623 - val_mae: 3.7302\n",
      "Epoch 37/1000\n",
      "364/364 [==============================] - 0s 279us/step - loss: 40.5245 - mae: 4.5740 - val_loss: 30.5391 - val_mae: 3.7700\n",
      "Epoch 38/1000\n",
      "364/364 [==============================] - 0s 280us/step - loss: 39.2509 - mae: 4.4927 - val_loss: 30.2050 - val_mae: 3.8058\n",
      "Epoch 39/1000\n",
      "364/364 [==============================] - 0s 280us/step - loss: 38.4349 - mae: 4.4805 - val_loss: 29.4509 - val_mae: 3.6597\n",
      "Epoch 40/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 37.5182 - mae: 4.4097 - val_loss: 29.0142 - val_mae: 3.5523\n",
      "Epoch 41/1000\n",
      "364/364 [==============================] - 0s 293us/step - loss: 36.8314 - mae: 4.3321 - val_loss: 29.1811 - val_mae: 3.7783\n",
      "Epoch 42/1000\n",
      "364/364 [==============================] - 0s 310us/step - loss: 36.0058 - mae: 4.3490 - val_loss: 28.1120 - val_mae: 3.5310\n",
      "Epoch 43/1000\n",
      "364/364 [==============================] - 0s 257us/step - loss: 35.4870 - mae: 4.2573 - val_loss: 28.1971 - val_mae: 3.6614\n",
      "Epoch 44/1000\n",
      "364/364 [==============================] - 0s 285us/step - loss: 34.6747 - mae: 4.2123 - val_loss: 28.1452 - val_mae: 3.6922\n",
      "Epoch 45/1000\n",
      "364/364 [==============================] - 0s 264us/step - loss: 33.9993 - mae: 4.2290 - val_loss: 27.0158 - val_mae: 3.4518\n",
      "Epoch 46/1000\n",
      "364/364 [==============================] - 0s 302us/step - loss: 33.1395 - mae: 4.1204 - val_loss: 26.6730 - val_mae: 3.3606\n",
      "Epoch 47/1000\n",
      "364/364 [==============================] - 0s 353us/step - loss: 32.3585 - mae: 4.0588 - val_loss: 26.6973 - val_mae: 3.4976\n",
      "Epoch 48/1000\n",
      "364/364 [==============================] - 0s 296us/step - loss: 32.2635 - mae: 4.1151 - val_loss: 26.2114 - val_mae: 3.3952\n",
      "Epoch 49/1000\n",
      "364/364 [==============================] - 0s 264us/step - loss: 31.2112 - mae: 4.0196 - val_loss: 26.0266 - val_mae: 3.3894\n",
      "Epoch 50/1000\n",
      "364/364 [==============================] - 0s 340us/step - loss: 30.7880 - mae: 3.9646 - val_loss: 26.5667 - val_mae: 3.5263\n",
      "Epoch 51/1000\n",
      "364/364 [==============================] - 0s 209us/step - loss: 30.2862 - mae: 3.9444 - val_loss: 27.2048 - val_mae: 3.6392\n",
      "Epoch 52/1000\n",
      "364/364 [==============================] - 0s 384us/step - loss: 29.4976 - mae: 3.9734 - val_loss: 25.1571 - val_mae: 3.2728\n",
      "Epoch 53/1000\n",
      "364/364 [==============================] - 0s 232us/step - loss: 29.4832 - mae: 3.8973 - val_loss: 25.0042 - val_mae: 3.2424\n",
      "Epoch 54/1000\n",
      "364/364 [==============================] - 0s 285us/step - loss: 28.8200 - mae: 3.8426 - val_loss: 25.1350 - val_mae: 3.1934\n",
      "Epoch 55/1000\n",
      "364/364 [==============================] - 0s 349us/step - loss: 28.6163 - mae: 3.8033 - val_loss: 24.8793 - val_mae: 3.1871\n",
      "Epoch 56/1000\n",
      "364/364 [==============================] - 0s 334us/step - loss: 28.1903 - mae: 3.7666 - val_loss: 24.8585 - val_mae: 3.2344\n",
      "Epoch 57/1000\n",
      "364/364 [==============================] - 0s 279us/step - loss: 27.6569 - mae: 3.7388 - val_loss: 24.8632 - val_mae: 3.2379\n",
      "Epoch 58/1000\n",
      "364/364 [==============================] - 0s 427us/step - loss: 27.3749 - mae: 3.7322 - val_loss: 24.9345 - val_mae: 3.2616\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 364us/step - loss: 27.1165 - mae: 3.7174 - val_loss: 24.8560 - val_mae: 3.2508\n",
      "Epoch 60/1000\n",
      "364/364 [==============================] - 0s 311us/step - loss: 26.7083 - mae: 3.6995 - val_loss: 24.8863 - val_mae: 3.2615\n",
      "Epoch 61/1000\n",
      "364/364 [==============================] - 0s 337us/step - loss: 26.5287 - mae: 3.7003 - val_loss: 24.3789 - val_mae: 3.1645\n",
      "Epoch 62/1000\n",
      "364/364 [==============================] - 0s 276us/step - loss: 26.4009 - mae: 3.6496 - val_loss: 24.2038 - val_mae: 3.1312\n",
      "Epoch 63/1000\n",
      "364/364 [==============================] - 0s 186us/step - loss: 26.2048 - mae: 3.6164 - val_loss: 24.5171 - val_mae: 3.1935\n",
      "Epoch 64/1000\n",
      "364/364 [==============================] - 0s 341us/step - loss: 25.8600 - mae: 3.6505 - val_loss: 24.1467 - val_mae: 3.1202\n",
      "Epoch 65/1000\n",
      "364/364 [==============================] - 0s 182us/step - loss: 25.8561 - mae: 3.6065 - val_loss: 24.1864 - val_mae: 3.1114\n",
      "Epoch 66/1000\n",
      "364/364 [==============================] - 0s 172us/step - loss: 25.6864 - mae: 3.6143 - val_loss: 24.3439 - val_mae: 3.1191\n",
      "Epoch 67/1000\n",
      "364/364 [==============================] - 0s 199us/step - loss: 25.5268 - mae: 3.5578 - val_loss: 24.2112 - val_mae: 3.1083\n",
      "Epoch 68/1000\n",
      "364/364 [==============================] - 0s 188us/step - loss: 25.2559 - mae: 3.5631 - val_loss: 24.4014 - val_mae: 3.1644\n",
      "Epoch 69/1000\n",
      "364/364 [==============================] - 0s 289us/step - loss: 24.9569 - mae: 3.5823 - val_loss: 24.1593 - val_mae: 3.0981\n",
      "Epoch 70/1000\n",
      "364/364 [==============================] - 0s 275us/step - loss: 24.9612 - mae: 3.5270 - val_loss: 25.0964 - val_mae: 3.2891\n",
      "Epoch 71/1000\n",
      "364/364 [==============================] - 0s 258us/step - loss: 24.7163 - mae: 3.5714 - val_loss: 24.4037 - val_mae: 3.1545\n",
      "Epoch 72/1000\n",
      "364/364 [==============================] - 0s 251us/step - loss: 24.6345 - mae: 3.5554 - val_loss: 24.2465 - val_mae: 3.1207\n",
      "Epoch 73/1000\n",
      "364/364 [==============================] - 0s 247us/step - loss: 24.2945 - mae: 3.4473 - val_loss: 25.5969 - val_mae: 3.3851\n",
      "Epoch 74/1000\n",
      "364/364 [==============================] - 0s 215us/step - loss: 24.6347 - mae: 3.5944 - val_loss: 24.1145 - val_mae: 3.0943\n",
      "Epoch 75/1000\n",
      "364/364 [==============================] - 0s 188us/step - loss: 24.0537 - mae: 3.5213 - val_loss: 24.0119 - val_mae: 3.0913\n",
      "Epoch 76/1000\n",
      "364/364 [==============================] - 0s 208us/step - loss: 24.2760 - mae: 3.4858 - val_loss: 24.4168 - val_mae: 3.1681\n",
      "Epoch 77/1000\n",
      "364/364 [==============================] - 0s 289us/step - loss: 24.1922 - mae: 3.5051 - val_loss: 24.3782 - val_mae: 3.1454\n",
      "Epoch 78/1000\n",
      "364/364 [==============================] - 0s 250us/step - loss: 24.0700 - mae: 3.4914 - val_loss: 25.2783 - val_mae: 3.3277\n",
      "Epoch 79/1000\n",
      "364/364 [==============================] - 0s 248us/step - loss: 24.0248 - mae: 3.5233 - val_loss: 24.0361 - val_mae: 3.0800\n",
      "Epoch 80/1000\n",
      "364/364 [==============================] - 0s 168us/step - loss: 23.9556 - mae: 3.4418 - val_loss: 25.4235 - val_mae: 3.3518\n",
      "Epoch 81/1000\n",
      "364/364 [==============================] - 0s 188us/step - loss: 24.0038 - mae: 3.5098 - val_loss: 24.1902 - val_mae: 3.0897\n",
      "Epoch 82/1000\n",
      "364/364 [==============================] - 0s 189us/step - loss: 23.5803 - mae: 3.4415 - val_loss: 24.1649 - val_mae: 3.0948\n",
      "Epoch 83/1000\n",
      "364/364 [==============================] - 0s 227us/step - loss: 23.4512 - mae: 3.4756 - val_loss: 24.0043 - val_mae: 3.0714\n",
      "Epoch 84/1000\n",
      "364/364 [==============================] - 0s 225us/step - loss: 23.6602 - mae: 3.4737 - val_loss: 24.0308 - val_mae: 3.0974\n",
      "Epoch 85/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 23.4286 - mae: 3.4173 - val_loss: 24.0166 - val_mae: 3.0728\n",
      "Epoch 86/1000\n",
      "364/364 [==============================] - 0s 191us/step - loss: 23.3272 - mae: 3.4425 - val_loss: 23.9073 - val_mae: 3.0706\n",
      "Epoch 87/1000\n",
      "364/364 [==============================] - 0s 218us/step - loss: 23.3704 - mae: 3.4354 - val_loss: 24.0498 - val_mae: 3.0891\n",
      "Epoch 88/1000\n",
      "364/364 [==============================] - 0s 212us/step - loss: 23.1404 - mae: 3.4036 - val_loss: 23.9466 - val_mae: 3.0727\n",
      "Epoch 89/1000\n",
      "364/364 [==============================] - 0s 226us/step - loss: 23.1479 - mae: 3.4025 - val_loss: 24.4770 - val_mae: 3.1896\n",
      "Epoch 90/1000\n",
      "364/364 [==============================] - 0s 197us/step - loss: 23.0590 - mae: 3.4014 - val_loss: 25.0902 - val_mae: 3.3338\n",
      "Epoch 91/1000\n",
      "364/364 [==============================] - 0s 214us/step - loss: 23.2684 - mae: 3.4809 - val_loss: 23.9336 - val_mae: 3.0757\n",
      "Epoch 92/1000\n",
      "364/364 [==============================] - 0s 199us/step - loss: 22.9025 - mae: 3.3837 - val_loss: 25.0488 - val_mae: 3.3197\n",
      "Epoch 93/1000\n",
      "364/364 [==============================] - 0s 215us/step - loss: 22.9562 - mae: 3.3981 - val_loss: 26.6353 - val_mae: 3.6041\n",
      "Epoch 94/1000\n",
      "364/364 [==============================] - 0s 304us/step - loss: 23.0971 - mae: 3.4582 - val_loss: 24.6855 - val_mae: 3.2617\n",
      "Epoch 95/1000\n",
      "364/364 [==============================] - 0s 236us/step - loss: 22.7119 - mae: 3.4102 - val_loss: 24.0997 - val_mae: 3.1254\n",
      "Epoch 96/1000\n",
      "364/364 [==============================] - 0s 291us/step - loss: 22.8330 - mae: 3.3898 - val_loss: 25.4778 - val_mae: 3.4183\n",
      "Epoch 97/1000\n",
      "364/364 [==============================] - 0s 237us/step - loss: 22.8604 - mae: 3.4258 - val_loss: 23.6763 - val_mae: 3.0627\n",
      "Epoch 98/1000\n",
      "364/364 [==============================] - 0s 228us/step - loss: 22.6370 - mae: 3.3918 - val_loss: 23.8009 - val_mae: 3.0766\n",
      "Epoch 99/1000\n",
      "364/364 [==============================] - 0s 161us/step - loss: 22.7317 - mae: 3.3902 - val_loss: 24.4442 - val_mae: 3.1996\n",
      "Epoch 100/1000\n",
      "364/364 [==============================] - 0s 218us/step - loss: 22.7459 - mae: 3.3853 - val_loss: 23.9571 - val_mae: 3.1068\n",
      "Epoch 101/1000\n",
      "364/364 [==============================] - 0s 249us/step - loss: 22.5814 - mae: 3.3925 - val_loss: 23.7432 - val_mae: 3.0731\n",
      "Epoch 102/1000\n",
      "364/364 [==============================] - 0s 292us/step - loss: 22.3577 - mae: 3.3450 - val_loss: 24.0521 - val_mae: 3.1359\n",
      "Epoch 103/1000\n",
      "364/364 [==============================] - 0s 232us/step - loss: 22.4237 - mae: 3.3528 - val_loss: 24.1077 - val_mae: 3.1566\n",
      "Epoch 104/1000\n",
      "364/364 [==============================] - 0s 240us/step - loss: 22.2914 - mae: 3.3642 - val_loss: 23.6958 - val_mae: 3.0762\n",
      "Epoch 105/1000\n",
      "364/364 [==============================] - 0s 272us/step - loss: 22.4304 - mae: 3.3303 - val_loss: 24.7551 - val_mae: 3.2930\n",
      "Epoch 106/1000\n",
      "364/364 [==============================] - 0s 256us/step - loss: 22.3023 - mae: 3.3842 - val_loss: 23.5243 - val_mae: 3.0451\n",
      "Epoch 107/1000\n",
      "364/364 [==============================] - 0s 182us/step - loss: 22.1906 - mae: 3.3016 - val_loss: 24.3171 - val_mae: 3.2000\n",
      "Epoch 108/1000\n",
      "364/364 [==============================] - 0s 434us/step - loss: 22.3019 - mae: 3.3732 - val_loss: 25.1717 - val_mae: 3.3788\n",
      "Epoch 109/1000\n",
      "364/364 [==============================] - 0s 171us/step - loss: 22.1817 - mae: 3.3661 - val_loss: 23.8682 - val_mae: 3.1369\n",
      "Epoch 110/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 22.0886 - mae: 3.3480 - val_loss: 23.3706 - val_mae: 3.0444\n",
      "Epoch 111/1000\n",
      "364/364 [==============================] - 0s 203us/step - loss: 21.8972 - mae: 3.2968 - val_loss: 25.2407 - val_mae: 3.4153\n",
      "Epoch 112/1000\n",
      "364/364 [==============================] - 0s 269us/step - loss: 21.9627 - mae: 3.4004 - val_loss: 23.5117 - val_mae: 3.0787\n",
      "Epoch 113/1000\n",
      "364/364 [==============================] - 0s 272us/step - loss: 21.9271 - mae: 3.3442 - val_loss: 23.3687 - val_mae: 3.0594\n",
      "Epoch 114/1000\n",
      "364/364 [==============================] - 0s 349us/step - loss: 22.0690 - mae: 3.3157 - val_loss: 23.1972 - val_mae: 3.0349\n",
      "Epoch 115/1000\n",
      "364/364 [==============================] - 0s 357us/step - loss: 21.7337 - mae: 3.2761 - val_loss: 25.6367 - val_mae: 3.5027\n",
      "Epoch 116/1000\n",
      "364/364 [==============================] - 0s 329us/step - loss: 21.9676 - mae: 3.3704 - val_loss: 23.5786 - val_mae: 3.1435\n",
      "Epoch 117/1000\n",
      "364/364 [==============================] - 0s 374us/step - loss: 22.1032 - mae: 3.3255 - val_loss: 23.6484 - val_mae: 3.1641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/1000\n",
      "364/364 [==============================] - 0s 321us/step - loss: 21.7902 - mae: 3.3270 - val_loss: 23.2481 - val_mae: 3.0744\n",
      "Epoch 119/1000\n",
      "364/364 [==============================] - 0s 404us/step - loss: 21.6147 - mae: 3.2913 - val_loss: 23.1364 - val_mae: 3.0490\n",
      "Epoch 120/1000\n",
      "364/364 [==============================] - 0s 383us/step - loss: 21.6047 - mae: 3.2782 - val_loss: 23.1343 - val_mae: 3.0501\n",
      "Epoch 121/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 21.6296 - mae: 3.2662 - val_loss: 24.0131 - val_mae: 3.2256\n",
      "Epoch 122/1000\n",
      "364/364 [==============================] - 0s 309us/step - loss: 21.7004 - mae: 3.3169 - val_loss: 22.9210 - val_mae: 3.0270\n",
      "Epoch 123/1000\n",
      "364/364 [==============================] - 0s 218us/step - loss: 21.4726 - mae: 3.2684 - val_loss: 23.9985 - val_mae: 3.2266\n",
      "Epoch 124/1000\n",
      "364/364 [==============================] - 0s 208us/step - loss: 21.6759 - mae: 3.3266 - val_loss: 22.9347 - val_mae: 3.0242\n",
      "Epoch 125/1000\n",
      "364/364 [==============================] - 0s 218us/step - loss: 21.3573 - mae: 3.2637 - val_loss: 23.5138 - val_mae: 3.1485\n",
      "Epoch 126/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 21.4046 - mae: 3.2730 - val_loss: 22.9298 - val_mae: 3.0524\n",
      "Epoch 127/1000\n",
      "364/364 [==============================] - 0s 437us/step - loss: 21.3281 - mae: 3.2516 - val_loss: 23.0409 - val_mae: 3.0773\n",
      "Epoch 128/1000\n",
      "364/364 [==============================] - 0s 547us/step - loss: 21.1629 - mae: 3.2588 - val_loss: 23.6584 - val_mae: 3.1952\n",
      "Epoch 129/1000\n",
      "364/364 [==============================] - 0s 156us/step - loss: 21.5363 - mae: 3.3214 - val_loss: 22.6090 - val_mae: 2.9970\n",
      "Epoch 130/1000\n",
      "364/364 [==============================] - ETA: 0s - loss: 25.2624 - mae: 3.74 - 0s 178us/step - loss: 21.3462 - mae: 3.2255 - val_loss: 23.4864 - val_mae: 3.1544\n",
      "Epoch 131/1000\n",
      "364/364 [==============================] - 0s 305us/step - loss: 21.0894 - mae: 3.2477 - val_loss: 23.1332 - val_mae: 3.0974\n",
      "Epoch 132/1000\n",
      "364/364 [==============================] - 0s 295us/step - loss: 21.3179 - mae: 3.2552 - val_loss: 22.7956 - val_mae: 3.0325\n",
      "Epoch 133/1000\n",
      "364/364 [==============================] - 0s 337us/step - loss: 20.9349 - mae: 3.2320 - val_loss: 24.9939 - val_mae: 3.4494\n",
      "Epoch 134/1000\n",
      "364/364 [==============================] - 0s 167us/step - loss: 21.5418 - mae: 3.3328 - val_loss: 23.1857 - val_mae: 3.1374\n",
      "Epoch 135/1000\n",
      "364/364 [==============================] - 0s 168us/step - loss: 20.9210 - mae: 3.2640 - val_loss: 22.5090 - val_mae: 3.0097\n",
      "Epoch 136/1000\n",
      "364/364 [==============================] - 0s 152us/step - loss: 21.0173 - mae: 3.2509 - val_loss: 22.5463 - val_mae: 3.0130\n",
      "Epoch 137/1000\n",
      "364/364 [==============================] - 0s 174us/step - loss: 21.0546 - mae: 3.2617 - val_loss: 22.5801 - val_mae: 3.0190\n",
      "Epoch 138/1000\n",
      "364/364 [==============================] - 0s 143us/step - loss: 20.7788 - mae: 3.2427 - val_loss: 22.5279 - val_mae: 3.0038\n",
      "Epoch 139/1000\n",
      "364/364 [==============================] - 0s 223us/step - loss: 21.1573 - mae: 3.2439 - val_loss: 22.4223 - val_mae: 3.0057\n",
      "Epoch 140/1000\n",
      "364/364 [==============================] - 0s 124us/step - loss: 21.0454 - mae: 3.2501 - val_loss: 22.6489 - val_mae: 3.0493\n",
      "Epoch 141/1000\n",
      "364/364 [==============================] - 0s 433us/step - loss: 21.0200 - mae: 3.2102 - val_loss: 23.2272 - val_mae: 3.1744\n",
      "Epoch 142/1000\n",
      "364/364 [==============================] - 0s 298us/step - loss: 20.8501 - mae: 3.2250 - val_loss: 22.1873 - val_mae: 2.9840\n",
      "Epoch 143/1000\n",
      "364/364 [==============================] - 0s 250us/step - loss: 20.9241 - mae: 3.2242 - val_loss: 22.4687 - val_mae: 3.0413\n",
      "Epoch 144/1000\n",
      "364/364 [==============================] - 0s 221us/step - loss: 20.6877 - mae: 3.2101 - val_loss: 22.9591 - val_mae: 3.1480\n",
      "Epoch 145/1000\n",
      "364/364 [==============================] - 0s 288us/step - loss: 20.7869 - mae: 3.2368 - val_loss: 22.0640 - val_mae: 2.9714\n",
      "Epoch 146/1000\n",
      "364/364 [==============================] - 0s 334us/step - loss: 20.5598 - mae: 3.1775 - val_loss: 23.0502 - val_mae: 3.1574\n",
      "Epoch 147/1000\n",
      "364/364 [==============================] - 0s 370us/step - loss: 20.5559 - mae: 3.2389 - val_loss: 22.1503 - val_mae: 2.9811\n",
      "Epoch 148/1000\n",
      "364/364 [==============================] - 0s 191us/step - loss: 20.5263 - mae: 3.1860 - val_loss: 24.6119 - val_mae: 3.4356\n",
      "Epoch 149/1000\n",
      "364/364 [==============================] - 0s 220us/step - loss: 20.5759 - mae: 3.2636 - val_loss: 22.0755 - val_mae: 2.9797\n",
      "Epoch 150/1000\n",
      "364/364 [==============================] - 0s 448us/step - loss: 20.7727 - mae: 3.2093 - val_loss: 22.0345 - val_mae: 2.9750\n",
      "Epoch 151/1000\n",
      "364/364 [==============================] - 0s 361us/step - loss: 20.4891 - mae: 3.1964 - val_loss: 21.9586 - val_mae: 2.9660\n",
      "Epoch 152/1000\n",
      "364/364 [==============================] - 0s 189us/step - loss: 20.4153 - mae: 3.1633 - val_loss: 22.0613 - val_mae: 2.9884\n",
      "Epoch 153/1000\n",
      "364/364 [==============================] - 0s 267us/step - loss: 20.3828 - mae: 3.1749 - val_loss: 21.7669 - val_mae: 2.9408\n",
      "Epoch 154/1000\n",
      "364/364 [==============================] - 0s 135us/step - loss: 20.3838 - mae: 3.1744 - val_loss: 21.7582 - val_mae: 2.9445\n",
      "Epoch 155/1000\n",
      "364/364 [==============================] - 0s 202us/step - loss: 20.2964 - mae: 3.1407 - val_loss: 23.3433 - val_mae: 3.2577\n",
      "Epoch 156/1000\n",
      "364/364 [==============================] - 0s 198us/step - loss: 20.3804 - mae: 3.2006 - val_loss: 21.7030 - val_mae: 2.9476\n",
      "Epoch 157/1000\n",
      "364/364 [==============================] - 0s 211us/step - loss: 20.2655 - mae: 3.1727 - val_loss: 21.8793 - val_mae: 2.9774\n",
      "Epoch 158/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 20.1572 - mae: 3.1604 - val_loss: 22.0893 - val_mae: 3.0213\n",
      "Epoch 159/1000\n",
      "364/364 [==============================] - 0s 348us/step - loss: 20.1886 - mae: 3.1745 - val_loss: 21.5821 - val_mae: 2.9253\n",
      "Epoch 160/1000\n",
      "364/364 [==============================] - 0s 482us/step - loss: 19.8915 - mae: 3.1327 - val_loss: 21.6316 - val_mae: 2.9340\n",
      "Epoch 161/1000\n",
      "364/364 [==============================] - 0s 503us/step - loss: 20.1549 - mae: 3.1364 - val_loss: 22.8140 - val_mae: 3.1764\n",
      "Epoch 162/1000\n",
      "364/364 [==============================] - 0s 322us/step - loss: 20.1860 - mae: 3.1950 - val_loss: 21.6187 - val_mae: 2.9335\n",
      "Epoch 163/1000\n",
      "364/364 [==============================] - 0s 226us/step - loss: 20.1451 - mae: 3.1576 - val_loss: 21.7316 - val_mae: 2.9616\n",
      "Epoch 164/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 20.1154 - mae: 3.1618 - val_loss: 21.8554 - val_mae: 2.9892\n",
      "Epoch 165/1000\n",
      "364/364 [==============================] - 0s 369us/step - loss: 19.9515 - mae: 3.1851 - val_loss: 21.9102 - val_mae: 2.9502\n",
      "Epoch 166/1000\n",
      "364/364 [==============================] - 0s 322us/step - loss: 19.9175 - mae: 3.1079 - val_loss: 21.8048 - val_mae: 2.9811\n",
      "Epoch 167/1000\n",
      "364/364 [==============================] - 0s 302us/step - loss: 19.7195 - mae: 3.1068 - val_loss: 22.3850 - val_mae: 3.1085\n",
      "Epoch 168/1000\n",
      "364/364 [==============================] - 0s 336us/step - loss: 19.8857 - mae: 3.1418 - val_loss: 21.5078 - val_mae: 2.9248\n",
      "Epoch 169/1000\n",
      "364/364 [==============================] - 0s 258us/step - loss: 19.9791 - mae: 3.1533 - val_loss: 21.3719 - val_mae: 2.8998\n",
      "Epoch 170/1000\n",
      "364/364 [==============================] - 0s 279us/step - loss: 19.7424 - mae: 3.1084 - val_loss: 21.4922 - val_mae: 2.9369\n",
      "Epoch 171/1000\n",
      "364/364 [==============================] - 0s 344us/step - loss: 19.4092 - mae: 3.1675 - val_loss: 21.3647 - val_mae: 2.8929\n",
      "Epoch 172/1000\n",
      "364/364 [==============================] - 0s 275us/step - loss: 19.7293 - mae: 3.0989 - val_loss: 21.7160 - val_mae: 2.9988\n",
      "Epoch 173/1000\n",
      "364/364 [==============================] - 0s 287us/step - loss: 19.5147 - mae: 3.1119 - val_loss: 21.4472 - val_mae: 2.9409\n",
      "Epoch 174/1000\n",
      "364/364 [==============================] - 0s 353us/step - loss: 19.5997 - mae: 3.1368 - val_loss: 21.4934 - val_mae: 2.9590\n",
      "Epoch 175/1000\n",
      "364/364 [==============================] - 0s 242us/step - loss: 19.1644 - mae: 3.1197 - val_loss: 22.8473 - val_mae: 3.0427\n",
      "Epoch 176/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 352us/step - loss: 20.1966 - mae: 3.0719 - val_loss: 21.0668 - val_mae: 2.8980\n",
      "Epoch 177/1000\n",
      "364/364 [==============================] - 0s 699us/step - loss: 19.2667 - mae: 3.0523 - val_loss: 23.8410 - val_mae: 3.4098\n",
      "Epoch 178/1000\n",
      "364/364 [==============================] - 0s 374us/step - loss: 19.7111 - mae: 3.1529 - val_loss: 22.3072 - val_mae: 3.1567\n",
      "Epoch 179/1000\n",
      "364/364 [==============================] - 0s 240us/step - loss: 19.4515 - mae: 3.1461 - val_loss: 21.0520 - val_mae: 2.8985\n",
      "Epoch 180/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 19.2669 - mae: 3.0723 - val_loss: 21.3101 - val_mae: 2.9475\n",
      "Epoch 181/1000\n",
      "364/364 [==============================] - 0s 333us/step - loss: 19.3886 - mae: 3.1035 - val_loss: 22.0454 - val_mae: 3.1016\n",
      "Epoch 182/1000\n",
      "364/364 [==============================] - 0s 250us/step - loss: 19.3620 - mae: 3.1283 - val_loss: 21.0740 - val_mae: 2.8950\n",
      "Epoch 183/1000\n",
      "364/364 [==============================] - 0s 307us/step - loss: 19.4094 - mae: 3.0688 - val_loss: 21.4321 - val_mae: 2.9842\n",
      "Epoch 184/1000\n",
      "364/364 [==============================] - 0s 282us/step - loss: 19.1406 - mae: 3.0633 - val_loss: 22.0021 - val_mae: 3.1152\n",
      "Epoch 185/1000\n",
      "364/364 [==============================] - 0s 231us/step - loss: 19.0137 - mae: 3.1100 - val_loss: 20.9000 - val_mae: 2.8964\n",
      "Epoch 186/1000\n",
      "364/364 [==============================] - 0s 355us/step - loss: 19.0843 - mae: 3.0682 - val_loss: 20.8475 - val_mae: 2.8898\n",
      "Epoch 187/1000\n",
      "364/364 [==============================] - 0s 287us/step - loss: 19.0481 - mae: 3.0691 - val_loss: 20.9392 - val_mae: 2.9122\n",
      "Epoch 188/1000\n",
      "364/364 [==============================] - 0s 291us/step - loss: 18.9524 - mae: 3.0613 - val_loss: 20.9361 - val_mae: 2.9042\n",
      "Epoch 189/1000\n",
      "364/364 [==============================] - 0s 403us/step - loss: 18.7750 - mae: 3.0765 - val_loss: 21.0203 - val_mae: 2.9507\n",
      "Epoch 190/1000\n",
      "364/364 [==============================] - 0s 337us/step - loss: 18.8861 - mae: 3.0651 - val_loss: 21.3444 - val_mae: 3.0300\n",
      "Epoch 191/1000\n",
      "364/364 [==============================] - 0s 282us/step - loss: 18.8140 - mae: 3.0589 - val_loss: 20.5883 - val_mae: 2.8878\n",
      "Epoch 192/1000\n",
      "364/364 [==============================] - 0s 296us/step - loss: 18.8917 - mae: 3.0302 - val_loss: 22.5815 - val_mae: 3.2573\n",
      "Epoch 193/1000\n",
      "364/364 [==============================] - 0s 244us/step - loss: 19.0126 - mae: 3.1114 - val_loss: 20.5478 - val_mae: 2.8897\n",
      "Epoch 194/1000\n",
      "364/364 [==============================] - 0s 204us/step - loss: 18.7568 - mae: 3.0444 - val_loss: 20.6511 - val_mae: 2.8535\n",
      "Epoch 195/1000\n",
      "364/364 [==============================] - 0s 254us/step - loss: 19.1039 - mae: 3.0361 - val_loss: 20.5899 - val_mae: 2.9036\n",
      "Epoch 196/1000\n",
      "364/364 [==============================] - 0s 448us/step - loss: 18.8023 - mae: 3.0684 - val_loss: 20.3871 - val_mae: 2.8443\n",
      "Epoch 197/1000\n",
      "364/364 [==============================] - 0s 328us/step - loss: 18.6863 - mae: 3.0250 - val_loss: 20.4866 - val_mae: 2.8821\n",
      "Epoch 198/1000\n",
      "364/364 [==============================] - 0s 508us/step - loss: 18.5546 - mae: 3.0124 - val_loss: 22.3847 - val_mae: 3.2325\n",
      "Epoch 199/1000\n",
      "364/364 [==============================] - 0s 435us/step - loss: 18.5167 - mae: 3.0631 - val_loss: 20.4444 - val_mae: 2.8517\n",
      "Epoch 200/1000\n",
      "364/364 [==============================] - 0s 127us/step - loss: 18.4888 - mae: 3.0177 - val_loss: 21.1470 - val_mae: 3.0102\n",
      "Epoch 201/1000\n",
      "364/364 [==============================] - 0s 242us/step - loss: 18.6297 - mae: 3.0249 - val_loss: 21.3008 - val_mae: 3.0574\n",
      "Epoch 202/1000\n",
      "364/364 [==============================] - 0s 142us/step - loss: 18.6188 - mae: 3.0495 - val_loss: 21.9558 - val_mae: 3.1662\n",
      "Epoch 203/1000\n",
      "364/364 [==============================] - 0s 183us/step - loss: 18.4290 - mae: 3.0789 - val_loss: 20.1730 - val_mae: 2.8225\n",
      "Epoch 204/1000\n",
      "364/364 [==============================] - 0s 413us/step - loss: 18.5438 - mae: 3.0421 - val_loss: 20.4629 - val_mae: 2.8938\n",
      "Epoch 205/1000\n",
      "364/364 [==============================] - 0s 520us/step - loss: 18.3485 - mae: 3.0292 - val_loss: 20.2658 - val_mae: 2.8274\n",
      "Epoch 206/1000\n",
      "364/364 [==============================] - 0s 231us/step - loss: 18.4621 - mae: 2.9858 - val_loss: 21.6003 - val_mae: 3.1191\n",
      "Epoch 207/1000\n",
      "364/364 [==============================] - 0s 458us/step - loss: 18.3530 - mae: 3.0352 - val_loss: 20.8712 - val_mae: 2.9825\n",
      "Epoch 208/1000\n",
      "364/364 [==============================] - 0s 564us/step - loss: 18.3571 - mae: 3.0134 - val_loss: 20.2174 - val_mae: 2.8535\n",
      "Epoch 209/1000\n",
      "364/364 [==============================] - 0s 324us/step - loss: 18.1821 - mae: 2.9867 - val_loss: 21.7010 - val_mae: 3.1506\n",
      "Epoch 210/1000\n",
      "364/364 [==============================] - 0s 462us/step - loss: 18.2396 - mae: 3.0102 - val_loss: 20.1651 - val_mae: 2.8739\n",
      "Epoch 211/1000\n",
      "364/364 [==============================] - 0s 343us/step - loss: 18.0219 - mae: 3.0050 - val_loss: 19.9640 - val_mae: 2.8137\n",
      "Epoch 212/1000\n",
      "364/364 [==============================] - 0s 402us/step - loss: 18.0142 - mae: 2.9461 - val_loss: 21.7695 - val_mae: 3.1599\n",
      "Epoch 213/1000\n",
      "364/364 [==============================] - 0s 307us/step - loss: 18.2475 - mae: 3.0320 - val_loss: 19.9110 - val_mae: 2.7984\n",
      "Epoch 214/1000\n",
      "364/364 [==============================] - 0s 356us/step - loss: 18.0312 - mae: 2.9364 - val_loss: 20.9032 - val_mae: 3.0186\n",
      "Epoch 215/1000\n",
      "364/364 [==============================] - 0s 419us/step - loss: 18.2214 - mae: 3.0245 - val_loss: 20.1884 - val_mae: 2.9003\n",
      "Epoch 216/1000\n",
      "364/364 [==============================] - 0s 353us/step - loss: 17.9427 - mae: 2.9913 - val_loss: 21.1266 - val_mae: 3.0810\n",
      "Epoch 217/1000\n",
      "364/364 [==============================] - 0s 233us/step - loss: 17.6206 - mae: 3.0258 - val_loss: 19.9952 - val_mae: 2.7880\n",
      "Epoch 218/1000\n",
      "364/364 [==============================] - 0s 333us/step - loss: 17.9390 - mae: 2.9829 - val_loss: 20.6670 - val_mae: 2.9793\n",
      "Epoch 219/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 18.1158 - mae: 2.9987 - val_loss: 19.8513 - val_mae: 2.7963\n",
      "Epoch 220/1000\n",
      "364/364 [==============================] - 0s 324us/step - loss: 17.8175 - mae: 2.9518 - val_loss: 19.8421 - val_mae: 2.8089\n",
      "Epoch 221/1000\n",
      "364/364 [==============================] - 0s 312us/step - loss: 18.0234 - mae: 2.9657 - val_loss: 20.1088 - val_mae: 2.8837\n",
      "Epoch 222/1000\n",
      "364/364 [==============================] - 0s 163us/step - loss: 17.9804 - mae: 2.9726 - val_loss: 19.5808 - val_mae: 2.7693\n",
      "Epoch 223/1000\n",
      "364/364 [==============================] - 0s 207us/step - loss: 17.8313 - mae: 2.9322 - val_loss: 19.6213 - val_mae: 2.7557\n",
      "Epoch 224/1000\n",
      "364/364 [==============================] - 0s 229us/step - loss: 17.6884 - mae: 2.9254 - val_loss: 19.6304 - val_mae: 2.7911\n",
      "Epoch 225/1000\n",
      "364/364 [==============================] - 0s 280us/step - loss: 17.8885 - mae: 2.9827 - val_loss: 19.5834 - val_mae: 2.7850\n",
      "Epoch 226/1000\n",
      "364/364 [==============================] - 0s 291us/step - loss: 17.4687 - mae: 2.9173 - val_loss: 19.4977 - val_mae: 2.7800\n",
      "Epoch 227/1000\n",
      "364/364 [==============================] - 0s 348us/step - loss: 17.6884 - mae: 2.9318 - val_loss: 19.3666 - val_mae: 2.7497\n",
      "Epoch 228/1000\n",
      "364/364 [==============================] - 0s 304us/step - loss: 17.5219 - mae: 2.9215 - val_loss: 19.4775 - val_mae: 2.7406\n",
      "Epoch 229/1000\n",
      "364/364 [==============================] - 0s 418us/step - loss: 17.8161 - mae: 2.9413 - val_loss: 19.4121 - val_mae: 2.7520\n",
      "Epoch 230/1000\n",
      "364/364 [==============================] - 0s 339us/step - loss: 17.6195 - mae: 2.9413 - val_loss: 19.3757 - val_mae: 2.7475\n",
      "Epoch 231/1000\n",
      "364/364 [==============================] - 0s 321us/step - loss: 17.5506 - mae: 2.9221 - val_loss: 19.6608 - val_mae: 2.8454\n",
      "Epoch 232/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 17.3849 - mae: 2.9253 - val_loss: 19.3459 - val_mae: 2.7572\n",
      "Epoch 233/1000\n",
      "364/364 [==============================] - 0s 350us/step - loss: 17.5786 - mae: 2.9062 - val_loss: 21.0370 - val_mae: 3.1045\n",
      "Epoch 234/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 179us/step - loss: 17.4402 - mae: 3.0038 - val_loss: 19.2730 - val_mae: 2.7624\n",
      "Epoch 235/1000\n",
      "364/364 [==============================] - 0s 286us/step - loss: 17.3276 - mae: 2.9116 - val_loss: 19.6436 - val_mae: 2.8664\n",
      "Epoch 236/1000\n",
      "364/364 [==============================] - 0s 254us/step - loss: 17.2900 - mae: 2.9120 - val_loss: 19.1647 - val_mae: 2.7445\n",
      "Epoch 237/1000\n",
      "364/364 [==============================] - 0s 364us/step - loss: 17.4358 - mae: 2.9037 - val_loss: 19.7126 - val_mae: 2.8827\n",
      "Epoch 238/1000\n",
      "364/364 [==============================] - 0s 307us/step - loss: 17.2235 - mae: 2.9213 - val_loss: 19.1879 - val_mae: 2.7420\n",
      "Epoch 239/1000\n",
      "364/364 [==============================] - 0s 261us/step - loss: 17.1955 - mae: 2.8919 - val_loss: 19.3271 - val_mae: 2.8048\n",
      "Epoch 240/1000\n",
      "364/364 [==============================] - 0s 197us/step - loss: 17.4786 - mae: 2.9718 - val_loss: 19.2369 - val_mae: 2.7721\n",
      "Epoch 241/1000\n",
      "364/364 [==============================] - 0s 105us/step - loss: 17.2882 - mae: 2.9029 - val_loss: 19.0906 - val_mae: 2.7154\n",
      "Epoch 242/1000\n",
      "364/364 [==============================] - 0s 271us/step - loss: 17.1675 - mae: 2.9042 - val_loss: 19.0772 - val_mae: 2.7419\n",
      "Epoch 243/1000\n",
      "364/364 [==============================] - 0s 189us/step - loss: 17.2113 - mae: 2.9158 - val_loss: 19.5046 - val_mae: 2.8648\n",
      "Epoch 244/1000\n",
      "364/364 [==============================] - 0s 278us/step - loss: 16.9383 - mae: 2.8924 - val_loss: 19.2433 - val_mae: 2.8157\n",
      "Epoch 245/1000\n",
      "364/364 [==============================] - 0s 348us/step - loss: 16.8827 - mae: 2.8894 - val_loss: 20.2920 - val_mae: 3.0033\n",
      "Epoch 246/1000\n",
      "364/364 [==============================] - 0s 294us/step - loss: 17.1761 - mae: 2.9316 - val_loss: 18.9160 - val_mae: 2.7421\n",
      "Epoch 247/1000\n",
      "364/364 [==============================] - 0s 266us/step - loss: 17.0514 - mae: 2.8827 - val_loss: 20.1313 - val_mae: 2.9842\n",
      "Epoch 248/1000\n",
      "364/364 [==============================] - 0s 253us/step - loss: 16.9688 - mae: 2.9277 - val_loss: 19.4106 - val_mae: 2.8566\n",
      "Epoch 249/1000\n",
      "364/364 [==============================] - 0s 277us/step - loss: 16.8284 - mae: 2.8796 - val_loss: 19.3749 - val_mae: 2.8614\n",
      "Epoch 250/1000\n",
      "364/364 [==============================] - 0s 203us/step - loss: 16.7411 - mae: 2.8372 - val_loss: 21.6625 - val_mae: 3.2577\n",
      "Epoch 251/1000\n",
      "364/364 [==============================] - 0s 308us/step - loss: 17.3607 - mae: 2.9808 - val_loss: 18.8314 - val_mae: 2.7564\n",
      "Epoch 252/1000\n",
      "364/364 [==============================] - 0s 308us/step - loss: 16.7307 - mae: 2.8556 - val_loss: 19.4447 - val_mae: 2.8882\n",
      "Epoch 253/1000\n",
      "364/364 [==============================] - 0s 353us/step - loss: 16.7015 - mae: 2.8806 - val_loss: 18.7674 - val_mae: 2.7085\n",
      "Epoch 254/1000\n",
      "364/364 [==============================] - 0s 280us/step - loss: 16.8601 - mae: 2.8349 - val_loss: 19.0194 - val_mae: 2.8020\n",
      "Epoch 255/1000\n",
      "364/364 [==============================] - 0s 306us/step - loss: 16.8509 - mae: 2.8773 - val_loss: 19.4575 - val_mae: 2.8912\n",
      "Epoch 256/1000\n",
      "364/364 [==============================] - 0s 179us/step - loss: 16.8815 - mae: 2.9125 - val_loss: 18.8113 - val_mae: 2.7735\n",
      "Epoch 257/1000\n",
      "364/364 [==============================] - 0s 216us/step - loss: 16.7204 - mae: 2.8776 - val_loss: 18.5580 - val_mae: 2.6686\n",
      "Epoch 258/1000\n",
      "364/364 [==============================] - 0s 308us/step - loss: 16.7850 - mae: 2.8592 - val_loss: 18.5651 - val_mae: 2.6854\n",
      "Epoch 259/1000\n",
      "364/364 [==============================] - 0s 209us/step - loss: 16.6278 - mae: 2.8616 - val_loss: 18.7264 - val_mae: 2.7239\n",
      "Epoch 260/1000\n",
      "364/364 [==============================] - 0s 485us/step - loss: 16.6232 - mae: 2.8621 - val_loss: 18.4934 - val_mae: 2.6993\n",
      "Epoch 261/1000\n",
      "364/364 [==============================] - 0s 402us/step - loss: 16.6177 - mae: 2.8589 - val_loss: 19.4791 - val_mae: 2.9131\n",
      "Epoch 262/1000\n",
      "364/364 [==============================] - 0s 388us/step - loss: 16.8322 - mae: 2.8898 - val_loss: 19.2267 - val_mae: 2.8805\n",
      "Epoch 263/1000\n",
      "364/364 [==============================] - 0s 352us/step - loss: 16.1427 - mae: 2.8689 - val_loss: 18.6018 - val_mae: 2.6623\n",
      "Epoch 264/1000\n",
      "364/364 [==============================] - 0s 359us/step - loss: 16.4084 - mae: 2.8393 - val_loss: 18.3955 - val_mae: 2.6470\n",
      "Epoch 265/1000\n",
      "364/364 [==============================] - 0s 392us/step - loss: 16.4729 - mae: 2.8362 - val_loss: 18.8901 - val_mae: 2.8177\n",
      "Epoch 266/1000\n",
      "364/364 [==============================] - 0s 398us/step - loss: 16.5648 - mae: 2.8644 - val_loss: 18.2932 - val_mae: 2.6471\n",
      "Epoch 267/1000\n",
      "364/364 [==============================] - 0s 247us/step - loss: 16.3919 - mae: 2.8231 - val_loss: 18.4145 - val_mae: 2.6434\n",
      "Epoch 268/1000\n",
      "364/364 [==============================] - 0s 299us/step - loss: 16.3729 - mae: 2.7892 - val_loss: 19.1788 - val_mae: 2.8695\n",
      "Epoch 269/1000\n",
      "364/364 [==============================] - 0s 320us/step - loss: 16.7369 - mae: 2.8594 - val_loss: 18.5013 - val_mae: 2.7493\n",
      "Epoch 270/1000\n",
      "364/364 [==============================] - 0s 237us/step - loss: 16.1785 - mae: 2.8185 - val_loss: 19.0879 - val_mae: 2.8545\n",
      "Epoch 271/1000\n",
      "364/364 [==============================] - 0s 257us/step - loss: 16.1918 - mae: 2.8245 - val_loss: 18.6727 - val_mae: 2.7859\n",
      "Epoch 272/1000\n",
      "364/364 [==============================] - 0s 317us/step - loss: 16.3712 - mae: 2.8220 - val_loss: 18.6385 - val_mae: 2.7846\n",
      "Epoch 273/1000\n",
      "364/364 [==============================] - 0s 287us/step - loss: 16.2383 - mae: 2.8078 - val_loss: 19.2813 - val_mae: 2.8843\n",
      "Epoch 274/1000\n",
      "364/364 [==============================] - 0s 285us/step - loss: 16.2422 - mae: 2.8822 - val_loss: 18.3272 - val_mae: 2.6817\n",
      "Epoch 275/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 16.2386 - mae: 2.8132 - val_loss: 18.8266 - val_mae: 2.8124\n",
      "Epoch 276/1000\n",
      "364/364 [==============================] - 0s 508us/step - loss: 16.1400 - mae: 2.8312 - val_loss: 19.5530 - val_mae: 2.9469\n",
      "Epoch 277/1000\n",
      "364/364 [==============================] - 0s 418us/step - loss: 16.0316 - mae: 2.8296 - val_loss: 18.8608 - val_mae: 2.8399\n",
      "Epoch 278/1000\n",
      "364/364 [==============================] - 0s 383us/step - loss: 16.2214 - mae: 2.8348 - val_loss: 19.2478 - val_mae: 2.9043\n",
      "Epoch 279/1000\n",
      "364/364 [==============================] - 0s 489us/step - loss: 16.2143 - mae: 2.8415 - val_loss: 19.0746 - val_mae: 2.8755\n",
      "Epoch 280/1000\n",
      "364/364 [==============================] - 0s 451us/step - loss: 16.0458 - mae: 2.8346 - val_loss: 18.5973 - val_mae: 2.8060\n",
      "Epoch 281/1000\n",
      "364/364 [==============================] - 0s 357us/step - loss: 16.0521 - mae: 2.8376 - val_loss: 17.8832 - val_mae: 2.6301\n",
      "Epoch 282/1000\n",
      "364/364 [==============================] - 0s 362us/step - loss: 16.1745 - mae: 2.8109 - val_loss: 17.8960 - val_mae: 2.6072\n",
      "Epoch 283/1000\n",
      "364/364 [==============================] - 0s 389us/step - loss: 16.0923 - mae: 2.8182 - val_loss: 18.2286 - val_mae: 2.7368\n",
      "Epoch 284/1000\n",
      "364/364 [==============================] - 0s 241us/step - loss: 16.0452 - mae: 2.7934 - val_loss: 18.7579 - val_mae: 2.8422\n",
      "Epoch 285/1000\n",
      "364/364 [==============================] - 0s 317us/step - loss: 16.0206 - mae: 2.8309 - val_loss: 18.6673 - val_mae: 2.8309\n",
      "Epoch 286/1000\n",
      "364/364 [==============================] - 0s 369us/step - loss: 15.9553 - mae: 2.8125 - val_loss: 18.4799 - val_mae: 2.8028\n",
      "Epoch 287/1000\n",
      "364/364 [==============================] - 0s 334us/step - loss: 15.8515 - mae: 2.8325 - val_loss: 17.8502 - val_mae: 2.6117\n",
      "Epoch 288/1000\n",
      "364/364 [==============================] - 0s 408us/step - loss: 16.0204 - mae: 2.8183 - val_loss: 18.1095 - val_mae: 2.6076\n",
      "Epoch 289/1000\n",
      "364/364 [==============================] - 0s 291us/step - loss: 15.6830 - mae: 2.7352 - val_loss: 19.2008 - val_mae: 2.9031\n",
      "Epoch 290/1000\n",
      "364/364 [==============================] - 0s 189us/step - loss: 15.9044 - mae: 2.8512 - val_loss: 18.0073 - val_mae: 2.6350\n",
      "Epoch 291/1000\n",
      "364/364 [==============================] - 0s 439us/step - loss: 15.7611 - mae: 2.7718 - val_loss: 18.3283 - val_mae: 2.7639\n",
      "Epoch 292/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 233us/step - loss: 15.8245 - mae: 2.7959 - val_loss: 17.8593 - val_mae: 2.6427\n",
      "Epoch 293/1000\n",
      "364/364 [==============================] - 0s 329us/step - loss: 15.6873 - mae: 2.7893 - val_loss: 17.9249 - val_mae: 2.6131\n",
      "Epoch 294/1000\n",
      "364/364 [==============================] - 0s 152us/step - loss: 15.7175 - mae: 2.7573 - val_loss: 18.2251 - val_mae: 2.7502\n",
      "Epoch 295/1000\n",
      "364/364 [==============================] - 0s 338us/step - loss: 15.7382 - mae: 2.7677 - val_loss: 18.2133 - val_mae: 2.7500\n",
      "Epoch 296/1000\n",
      "364/364 [==============================] - 0s 248us/step - loss: 15.5783 - mae: 2.7770 - val_loss: 17.9032 - val_mae: 2.7110\n",
      "Epoch 297/1000\n",
      "364/364 [==============================] - 0s 181us/step - loss: 15.6759 - mae: 2.7974 - val_loss: 17.6293 - val_mae: 2.6166\n",
      "Epoch 298/1000\n",
      "364/364 [==============================] - 0s 256us/step - loss: 15.6230 - mae: 2.7651 - val_loss: 17.7898 - val_mae: 2.7032\n",
      "Epoch 299/1000\n",
      "364/364 [==============================] - 0s 303us/step - loss: 15.7226 - mae: 2.7780 - val_loss: 18.4171 - val_mae: 2.8155\n",
      "Epoch 300/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 15.7868 - mae: 2.8177 - val_loss: 17.6003 - val_mae: 2.6188\n",
      "Epoch 301/1000\n",
      "364/364 [==============================] - 0s 282us/step - loss: 15.4364 - mae: 2.7366 - val_loss: 18.6532 - val_mae: 2.8587\n",
      "Epoch 302/1000\n",
      "364/364 [==============================] - 0s 172us/step - loss: 15.8101 - mae: 2.8097 - val_loss: 17.5565 - val_mae: 2.6397\n",
      "Epoch 303/1000\n",
      "364/364 [==============================] - 0s 187us/step - loss: 15.4737 - mae: 2.7726 - val_loss: 17.4527 - val_mae: 2.6239\n",
      "Epoch 304/1000\n",
      "364/364 [==============================] - 0s 223us/step - loss: 15.4814 - mae: 2.7795 - val_loss: 17.5753 - val_mae: 2.6538\n",
      "Epoch 305/1000\n",
      "364/364 [==============================] - 0s 353us/step - loss: 15.5610 - mae: 2.7845 - val_loss: 17.4319 - val_mae: 2.5637\n",
      "Epoch 306/1000\n",
      "364/364 [==============================] - 0s 264us/step - loss: 15.4184 - mae: 2.7708 - val_loss: 17.4093 - val_mae: 2.5616\n",
      "Epoch 307/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 15.4985 - mae: 2.7448 - val_loss: 17.6856 - val_mae: 2.6800\n",
      "Epoch 308/1000\n",
      "364/364 [==============================] - 0s 287us/step - loss: 15.4118 - mae: 2.7753 - val_loss: 17.4986 - val_mae: 2.6292\n",
      "Epoch 309/1000\n",
      "364/364 [==============================] - 0s 192us/step - loss: 15.3124 - mae: 2.7280 - val_loss: 18.2688 - val_mae: 2.8110\n",
      "Epoch 310/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 15.3398 - mae: 2.7546 - val_loss: 17.3333 - val_mae: 2.5639\n",
      "Epoch 311/1000\n",
      "364/364 [==============================] - 0s 254us/step - loss: 15.2528 - mae: 2.7361 - val_loss: 17.2759 - val_mae: 2.5735\n",
      "Epoch 312/1000\n",
      "364/364 [==============================] - 0s 214us/step - loss: 15.3944 - mae: 2.7407 - val_loss: 17.4829 - val_mae: 2.6540\n",
      "Epoch 313/1000\n",
      "364/364 [==============================] - 0s 508us/step - loss: 15.1974 - mae: 2.7341 - val_loss: 17.5796 - val_mae: 2.5668\n",
      "Epoch 314/1000\n",
      "364/364 [==============================] - 0s 384us/step - loss: 15.3774 - mae: 2.7099 - val_loss: 17.5509 - val_mae: 2.6759\n",
      "Epoch 315/1000\n",
      "364/364 [==============================] - 0s 284us/step - loss: 15.2737 - mae: 2.7592 - val_loss: 17.1874 - val_mae: 2.5487\n",
      "Epoch 316/1000\n",
      "364/364 [==============================] - 0s 352us/step - loss: 15.1346 - mae: 2.7150 - val_loss: 17.3682 - val_mae: 2.6199\n",
      "Epoch 317/1000\n",
      "364/364 [==============================] - 0s 315us/step - loss: 15.4500 - mae: 2.7445 - val_loss: 17.3243 - val_mae: 2.6032\n",
      "Epoch 318/1000\n",
      "364/364 [==============================] - 0s 313us/step - loss: 15.1431 - mae: 2.6969 - val_loss: 18.9006 - val_mae: 2.9462\n",
      "Epoch 319/1000\n",
      "364/364 [==============================] - 0s 340us/step - loss: 15.1156 - mae: 2.7495 - val_loss: 18.1027 - val_mae: 2.8133\n",
      "Epoch 320/1000\n",
      "364/364 [==============================] - 0s 299us/step - loss: 15.0073 - mae: 2.7593 - val_loss: 17.1380 - val_mae: 2.5534\n",
      "Epoch 321/1000\n",
      "364/364 [==============================] - 0s 461us/step - loss: 15.2986 - mae: 2.7617 - val_loss: 17.9200 - val_mae: 2.7781\n",
      "Epoch 322/1000\n",
      "364/364 [==============================] - 0s 315us/step - loss: 15.0745 - mae: 2.7467 - val_loss: 18.0358 - val_mae: 2.8090\n",
      "Epoch 323/1000\n",
      "364/364 [==============================] - 0s 335us/step - loss: 15.0716 - mae: 2.7435 - val_loss: 17.1441 - val_mae: 2.6227\n",
      "Epoch 324/1000\n",
      "364/364 [==============================] - 0s 454us/step - loss: 15.2151 - mae: 2.7177 - val_loss: 17.9110 - val_mae: 2.7966\n",
      "Epoch 325/1000\n",
      "364/364 [==============================] - 0s 345us/step - loss: 15.1455 - mae: 2.7703 - val_loss: 16.9845 - val_mae: 2.5415\n",
      "Epoch 326/1000\n",
      "364/364 [==============================] - 0s 309us/step - loss: 15.1671 - mae: 2.7199 - val_loss: 16.9410 - val_mae: 2.5780\n",
      "Epoch 327/1000\n",
      "364/364 [==============================] - 0s 349us/step - loss: 15.0830 - mae: 2.7281 - val_loss: 17.7121 - val_mae: 2.7545\n",
      "Epoch 328/1000\n",
      "364/364 [==============================] - 0s 411us/step - loss: 14.8868 - mae: 2.7494 - val_loss: 17.2453 - val_mae: 2.5427\n",
      "Epoch 329/1000\n",
      "364/364 [==============================] - 0s 329us/step - loss: 14.8412 - mae: 2.6429 - val_loss: 18.4859 - val_mae: 2.8845\n",
      "Epoch 330/1000\n",
      "364/364 [==============================] - 0s 177us/step - loss: 15.0352 - mae: 2.7492 - val_loss: 16.8703 - val_mae: 2.5593\n",
      "Epoch 331/1000\n",
      "364/364 [==============================] - 0s 298us/step - loss: 15.0453 - mae: 2.7164 - val_loss: 16.8127 - val_mae: 2.5403\n",
      "Epoch 332/1000\n",
      "364/364 [==============================] - 0s 329us/step - loss: 15.0212 - mae: 2.7376 - val_loss: 16.8913 - val_mae: 2.5586\n",
      "Epoch 333/1000\n",
      "364/364 [==============================] - 0s 545us/step - loss: 14.6323 - mae: 2.6941 - val_loss: 16.9938 - val_mae: 2.6050\n",
      "Epoch 334/1000\n",
      "364/364 [==============================] - 0s 257us/step - loss: 14.7741 - mae: 2.6783 - val_loss: 18.1367 - val_mae: 2.8526\n",
      "Epoch 335/1000\n",
      "364/364 [==============================] - 0s 292us/step - loss: 14.7024 - mae: 2.7173 - val_loss: 17.0293 - val_mae: 2.5343\n",
      "Epoch 336/1000\n",
      "364/364 [==============================] - 0s 386us/step - loss: 14.8525 - mae: 2.6618 - val_loss: 16.7693 - val_mae: 2.5700\n",
      "Epoch 337/1000\n",
      "364/364 [==============================] - 0s 550us/step - loss: 14.5959 - mae: 2.6966 - val_loss: 17.2002 - val_mae: 2.5501\n",
      "Epoch 338/1000\n",
      "364/364 [==============================] - 0s 249us/step - loss: 14.8168 - mae: 2.6998 - val_loss: 16.7387 - val_mae: 2.5162\n",
      "Epoch 339/1000\n",
      "364/364 [==============================] - 0s 477us/step - loss: 14.8413 - mae: 2.7131 - val_loss: 16.7687 - val_mae: 2.5694\n",
      "Epoch 340/1000\n",
      "364/364 [==============================] - ETA: 0s - loss: 17.9860 - mae: 2.81 - 0s 357us/step - loss: 14.7328 - mae: 2.6921 - val_loss: 16.9264 - val_mae: 2.6024\n",
      "Epoch 341/1000\n",
      "364/364 [==============================] - 0s 321us/step - loss: 14.7123 - mae: 2.7123 - val_loss: 16.7075 - val_mae: 2.5486\n",
      "Epoch 342/1000\n",
      "364/364 [==============================] - 0s 295us/step - loss: 14.6053 - mae: 2.6863 - val_loss: 16.8572 - val_mae: 2.5738\n",
      "Epoch 343/1000\n",
      "364/364 [==============================] - 0s 367us/step - loss: 14.6604 - mae: 2.6912 - val_loss: 17.5225 - val_mae: 2.7309\n",
      "Epoch 344/1000\n",
      "364/364 [==============================] - 0s 332us/step - loss: 14.8396 - mae: 2.7188 - val_loss: 17.4272 - val_mae: 2.7282\n",
      "Epoch 345/1000\n",
      "364/364 [==============================] - 0s 485us/step - loss: 14.6960 - mae: 2.7219 - val_loss: 17.0070 - val_mae: 2.5307\n",
      "Epoch 346/1000\n",
      "364/364 [==============================] - 0s 418us/step - loss: 14.5811 - mae: 2.6550 - val_loss: 16.6930 - val_mae: 2.5464\n",
      "Epoch 347/1000\n",
      "364/364 [==============================] - 0s 342us/step - loss: 14.5807 - mae: 2.6756 - val_loss: 17.1798 - val_mae: 2.6973\n",
      "Epoch 348/1000\n",
      "364/364 [==============================] - 0s 257us/step - loss: 14.5656 - mae: 2.6765 - val_loss: 16.6632 - val_mae: 2.5778\n",
      "Epoch 349/1000\n",
      "364/364 [==============================] - 0s 216us/step - loss: 14.5277 - mae: 2.7010 - val_loss: 16.8401 - val_mae: 2.6235\n",
      "Epoch 350/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 266us/step - loss: 14.5089 - mae: 2.6695 - val_loss: 16.6476 - val_mae: 2.5778\n",
      "Epoch 351/1000\n",
      "364/364 [==============================] - 0s 208us/step - loss: 14.5271 - mae: 2.6722 - val_loss: 16.7483 - val_mae: 2.6234\n",
      "Epoch 352/1000\n",
      "364/364 [==============================] - 0s 265us/step - loss: 14.4691 - mae: 2.6599 - val_loss: 16.4168 - val_mae: 2.5221\n",
      "Epoch 353/1000\n",
      "364/364 [==============================] - 0s 370us/step - loss: 14.4633 - mae: 2.6688 - val_loss: 16.6202 - val_mae: 2.5731\n",
      "Epoch 354/1000\n",
      "364/364 [==============================] - 0s 337us/step - loss: 14.4128 - mae: 2.6706 - val_loss: 16.6438 - val_mae: 2.5778\n",
      "Epoch 355/1000\n",
      "364/364 [==============================] - 0s 418us/step - loss: 14.6827 - mae: 2.6944 - val_loss: 16.7711 - val_mae: 2.6225\n",
      "Epoch 356/1000\n",
      "364/364 [==============================] - 0s 326us/step - loss: 14.3913 - mae: 2.6694 - val_loss: 16.8557 - val_mae: 2.6415\n",
      "Epoch 357/1000\n",
      "364/364 [==============================] - 0s 269us/step - loss: 14.3396 - mae: 2.6609 - val_loss: 16.7419 - val_mae: 2.6285\n",
      "Epoch 358/1000\n",
      "364/364 [==============================] - 0s 342us/step - loss: 14.2498 - mae: 2.6898 - val_loss: 16.5937 - val_mae: 2.5841\n",
      "Epoch 359/1000\n",
      "364/364 [==============================] - 0s 358us/step - loss: 14.2511 - mae: 2.6604 - val_loss: 16.6486 - val_mae: 2.6003\n",
      "Epoch 360/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 14.3734 - mae: 2.6740 - val_loss: 16.7903 - val_mae: 2.6395\n",
      "Epoch 361/1000\n",
      "364/364 [==============================] - 0s 234us/step - loss: 14.4872 - mae: 2.6815 - val_loss: 16.3846 - val_mae: 2.4896\n",
      "Epoch 362/1000\n",
      "364/364 [==============================] - 0s 257us/step - loss: 14.4253 - mae: 2.6551 - val_loss: 16.9186 - val_mae: 2.6600\n",
      "Epoch 363/1000\n",
      "364/364 [==============================] - 0s 317us/step - loss: 14.2044 - mae: 2.6562 - val_loss: 16.5960 - val_mae: 2.5743\n",
      "Epoch 364/1000\n",
      "364/364 [==============================] - 0s 237us/step - loss: 14.4436 - mae: 2.6487 - val_loss: 16.4383 - val_mae: 2.5286\n",
      "Epoch 365/1000\n",
      "364/364 [==============================] - 0s 193us/step - loss: 14.1651 - mae: 2.6233 - val_loss: 16.5908 - val_mae: 2.5811\n",
      "Epoch 366/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 14.2589 - mae: 2.6381 - val_loss: 16.4689 - val_mae: 2.5587\n",
      "Epoch 367/1000\n",
      "364/364 [==============================] - 0s 253us/step - loss: 14.2658 - mae: 2.6534 - val_loss: 16.7737 - val_mae: 2.5226\n",
      "Epoch 368/1000\n",
      "364/364 [==============================] - 0s 501us/step - loss: 14.1654 - mae: 2.6070 - val_loss: 16.6914 - val_mae: 2.6294\n",
      "Epoch 369/1000\n",
      "364/364 [==============================] - 0s 336us/step - loss: 14.1571 - mae: 2.6538 - val_loss: 16.5764 - val_mae: 2.5982\n",
      "Epoch 370/1000\n",
      "364/364 [==============================] - 0s 466us/step - loss: 14.3113 - mae: 2.6517 - val_loss: 16.7745 - val_mae: 2.6658\n",
      "Epoch 371/1000\n",
      "364/364 [==============================] - 0s 404us/step - loss: 14.1162 - mae: 2.6471 - val_loss: 16.2659 - val_mae: 2.5194\n",
      "Epoch 372/1000\n",
      "364/364 [==============================] - 0s 365us/step - loss: 14.1105 - mae: 2.6297 - val_loss: 16.2116 - val_mae: 2.4946\n",
      "Epoch 373/1000\n",
      "364/364 [==============================] - 0s 277us/step - loss: 14.1005 - mae: 2.6252 - val_loss: 16.9546 - val_mae: 2.7066\n",
      "Epoch 374/1000\n",
      "364/364 [==============================] - 0s 200us/step - loss: 14.0171 - mae: 2.6510 - val_loss: 16.2123 - val_mae: 2.4822\n",
      "Epoch 375/1000\n",
      "364/364 [==============================] - 0s 276us/step - loss: 14.2255 - mae: 2.6245 - val_loss: 16.4242 - val_mae: 2.5831\n",
      "Epoch 376/1000\n",
      "364/364 [==============================] - 0s 347us/step - loss: 14.0195 - mae: 2.6477 - val_loss: 16.4336 - val_mae: 2.4918\n",
      "Epoch 377/1000\n",
      "364/364 [==============================] - 0s 281us/step - loss: 14.2473 - mae: 2.6316 - val_loss: 16.1936 - val_mae: 2.4743\n",
      "Epoch 378/1000\n",
      "364/364 [==============================] - 0s 246us/step - loss: 13.9894 - mae: 2.6351 - val_loss: 16.5350 - val_mae: 2.6200\n",
      "Epoch 379/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 13.9413 - mae: 2.6359 - val_loss: 16.2641 - val_mae: 2.4829\n",
      "Epoch 380/1000\n",
      "364/364 [==============================] - 0s 300us/step - loss: 14.1807 - mae: 2.6566 - val_loss: 16.8464 - val_mae: 2.6944\n",
      "Epoch 381/1000\n",
      "364/364 [==============================] - 0s 177us/step - loss: 13.8349 - mae: 2.6034 - val_loss: 16.1392 - val_mae: 2.4872\n",
      "Epoch 382/1000\n",
      "364/364 [==============================] - 0s 240us/step - loss: 13.9360 - mae: 2.5974 - val_loss: 16.4869 - val_mae: 2.6203\n",
      "Epoch 383/1000\n",
      "364/364 [==============================] - 0s 234us/step - loss: 13.9278 - mae: 2.6340 - val_loss: 16.1128 - val_mae: 2.4766\n",
      "Epoch 384/1000\n",
      "364/364 [==============================] - 0s 216us/step - loss: 13.9205 - mae: 2.6154 - val_loss: 16.1720 - val_mae: 2.5271\n",
      "Epoch 385/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 13.9910 - mae: 2.6170 - val_loss: 16.4018 - val_mae: 2.5777\n",
      "Epoch 386/1000\n",
      "364/364 [==============================] - 0s 337us/step - loss: 13.7930 - mae: 2.6103 - val_loss: 17.1287 - val_mae: 2.7568\n",
      "Epoch 387/1000\n",
      "364/364 [==============================] - 0s 288us/step - loss: 14.1477 - mae: 2.6641 - val_loss: 16.2375 - val_mae: 2.5376\n",
      "Epoch 388/1000\n",
      "364/364 [==============================] - 0s 237us/step - loss: 13.9007 - mae: 2.6147 - val_loss: 16.2370 - val_mae: 2.5871\n",
      "Epoch 389/1000\n",
      "364/364 [==============================] - 0s 170us/step - loss: 13.7970 - mae: 2.5985 - val_loss: 16.2514 - val_mae: 2.4919\n",
      "Epoch 390/1000\n",
      "364/364 [==============================] - 0s 272us/step - loss: 14.1112 - mae: 2.6086 - val_loss: 15.9625 - val_mae: 2.4934\n",
      "Epoch 391/1000\n",
      "364/364 [==============================] - 0s 358us/step - loss: 13.8938 - mae: 2.6169 - val_loss: 15.8936 - val_mae: 2.4811\n",
      "Epoch 392/1000\n",
      "364/364 [==============================] - ETA: 0s - loss: 14.3282 - mae: 2.66 - 0s 242us/step - loss: 13.7355 - mae: 2.6216 - val_loss: 15.9966 - val_mae: 2.4703\n",
      "Epoch 393/1000\n",
      "364/364 [==============================] - 0s 217us/step - loss: 13.8521 - mae: 2.5923 - val_loss: 16.4623 - val_mae: 2.6384\n",
      "Epoch 394/1000\n",
      "364/364 [==============================] - 0s 437us/step - loss: 14.0079 - mae: 2.6301 - val_loss: 16.0128 - val_mae: 2.5096\n",
      "Epoch 395/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 13.7882 - mae: 2.5700 - val_loss: 16.6529 - val_mae: 2.6813\n",
      "Epoch 396/1000\n",
      "364/364 [==============================] - 0s 450us/step - loss: 13.8518 - mae: 2.6254 - val_loss: 15.9401 - val_mae: 2.5099\n",
      "Epoch 397/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 13.7786 - mae: 2.6188 - val_loss: 16.5130 - val_mae: 2.5174\n",
      "Epoch 398/1000\n",
      "364/364 [==============================] - 0s 355us/step - loss: 14.0754 - mae: 2.6014 - val_loss: 16.1432 - val_mae: 2.4768\n",
      "Epoch 399/1000\n",
      "364/364 [==============================] - 0s 317us/step - loss: 13.8193 - mae: 2.5974 - val_loss: 16.2090 - val_mae: 2.5639\n",
      "Epoch 400/1000\n",
      "364/364 [==============================] - 0s 250us/step - loss: 13.6999 - mae: 2.5889 - val_loss: 16.5265 - val_mae: 2.6438\n",
      "Epoch 401/1000\n",
      "364/364 [==============================] - 0s 242us/step - loss: 13.7812 - mae: 2.6053 - val_loss: 16.1670 - val_mae: 2.5380\n",
      "Epoch 402/1000\n",
      "364/364 [==============================] - 0s 237us/step - loss: 13.7202 - mae: 2.5922 - val_loss: 16.4815 - val_mae: 2.6358\n",
      "Epoch 403/1000\n",
      "364/364 [==============================] - 0s 247us/step - loss: 13.7881 - mae: 2.6229 - val_loss: 16.0826 - val_mae: 2.4752\n",
      "Epoch 404/1000\n",
      "364/364 [==============================] - 0s 295us/step - loss: 13.7980 - mae: 2.5782 - val_loss: 16.0230 - val_mae: 2.5011\n",
      "Epoch 405/1000\n",
      "364/364 [==============================] - 0s 231us/step - loss: 13.6473 - mae: 2.5812 - val_loss: 17.3122 - val_mae: 2.8437\n",
      "Epoch 406/1000\n",
      "364/364 [==============================] - 0s 290us/step - loss: 13.6941 - mae: 2.6485 - val_loss: 16.0260 - val_mae: 2.4750\n",
      "Epoch 407/1000\n",
      "364/364 [==============================] - 0s 183us/step - loss: 13.7947 - mae: 2.5942 - val_loss: 15.8033 - val_mae: 2.4627\n",
      "Epoch 408/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 273us/step - loss: 13.7817 - mae: 2.6191 - val_loss: 16.3067 - val_mae: 2.4942\n",
      "Epoch 409/1000\n",
      "364/364 [==============================] - 0s 116us/step - loss: 13.8021 - mae: 2.5675 - val_loss: 15.9093 - val_mae: 2.4778\n",
      "Epoch 410/1000\n",
      "364/364 [==============================] - 0s 229us/step - loss: 13.6801 - mae: 2.5906 - val_loss: 16.2751 - val_mae: 2.6244\n",
      "Epoch 411/1000\n",
      "364/364 [==============================] - 0s 207us/step - loss: 13.7346 - mae: 2.6095 - val_loss: 15.9542 - val_mae: 2.5337\n",
      "Epoch 412/1000\n",
      "364/364 [==============================] - 0s 149us/step - loss: 13.6805 - mae: 2.5977 - val_loss: 16.0652 - val_mae: 2.5807\n",
      "Epoch 413/1000\n",
      "364/364 [==============================] - 0s 229us/step - loss: 13.5204 - mae: 2.5981 - val_loss: 15.8705 - val_mae: 2.4621\n",
      "Epoch 414/1000\n",
      "364/364 [==============================] - 0s 139us/step - loss: 13.5064 - mae: 2.5654 - val_loss: 16.1633 - val_mae: 2.6054\n",
      "Epoch 415/1000\n",
      "364/364 [==============================] - 0s 207us/step - loss: 13.8116 - mae: 2.6386 - val_loss: 16.1817 - val_mae: 2.6128\n",
      "Epoch 416/1000\n",
      "364/364 [==============================] - 0s 241us/step - loss: 13.6524 - mae: 2.6094 - val_loss: 16.0242 - val_mae: 2.5595\n",
      "Epoch 417/1000\n",
      "364/364 [==============================] - 0s 163us/step - loss: 13.7255 - mae: 2.5951 - val_loss: 16.0114 - val_mae: 2.5705\n",
      "Epoch 418/1000\n",
      "364/364 [==============================] - 0s 213us/step - loss: 13.4147 - mae: 2.5832 - val_loss: 16.1680 - val_mae: 2.6074\n",
      "Epoch 419/1000\n",
      "364/364 [==============================] - 0s 151us/step - loss: 13.4900 - mae: 2.6012 - val_loss: 15.9358 - val_mae: 2.5314\n",
      "Epoch 420/1000\n",
      "364/364 [==============================] - 0s 145us/step - loss: 13.7443 - mae: 2.5918 - val_loss: 16.1509 - val_mae: 2.6098\n",
      "Epoch 421/1000\n",
      "364/364 [==============================] - 0s 160us/step - loss: 13.4877 - mae: 2.6182 - val_loss: 15.7898 - val_mae: 2.4688\n",
      "Epoch 422/1000\n",
      "364/364 [==============================] - 0s 145us/step - loss: 13.3534 - mae: 2.5664 - val_loss: 15.8645 - val_mae: 2.5168\n",
      "Epoch 423/1000\n",
      "364/364 [==============================] - 0s 150us/step - loss: 13.4375 - mae: 2.5858 - val_loss: 16.2701 - val_mae: 2.6307\n",
      "Epoch 424/1000\n",
      "364/364 [==============================] - 0s 136us/step - loss: 13.4827 - mae: 2.5893 - val_loss: 15.7585 - val_mae: 2.4646\n",
      "Epoch 425/1000\n",
      "364/364 [==============================] - 0s 150us/step - loss: 13.4498 - mae: 2.5368 - val_loss: 17.0936 - val_mae: 2.8202\n",
      "Epoch 426/1000\n",
      "364/364 [==============================] - 0s 141us/step - loss: 13.4692 - mae: 2.5965 - val_loss: 15.8754 - val_mae: 2.5436\n",
      "Epoch 427/1000\n",
      "364/364 [==============================] - 0s 140us/step - loss: 13.4069 - mae: 2.6016 - val_loss: 16.2183 - val_mae: 2.5002\n",
      "Epoch 428/1000\n",
      "364/364 [==============================] - 0s 108us/step - loss: 13.5970 - mae: 2.5914 - val_loss: 15.9731 - val_mae: 2.5525\n",
      "Epoch 429/1000\n",
      "364/364 [==============================] - 0s 108us/step - loss: 13.4970 - mae: 2.6163 - val_loss: 16.1480 - val_mae: 2.4882\n",
      "Epoch 430/1000\n",
      "364/364 [==============================] - 0s 122us/step - loss: 13.2595 - mae: 2.5202 - val_loss: 16.8461 - val_mae: 2.7566\n",
      "Epoch 431/1000\n",
      "364/364 [==============================] - 0s 109us/step - loss: 13.3294 - mae: 2.5599 - val_loss: 15.9260 - val_mae: 2.5436\n",
      "Epoch 432/1000\n",
      "364/364 [==============================] - 0s 122us/step - loss: 13.3167 - mae: 2.5889 - val_loss: 16.3285 - val_mae: 2.6517\n",
      "Epoch 433/1000\n",
      "364/364 [==============================] - 0s 112us/step - loss: 13.3547 - mae: 2.5818 - val_loss: 16.0056 - val_mae: 2.5625\n",
      "Epoch 434/1000\n",
      "364/364 [==============================] - 0s 130us/step - loss: 13.3027 - mae: 2.5729 - val_loss: 15.8671 - val_mae: 2.4686\n",
      "Epoch 435/1000\n",
      "364/364 [==============================] - 0s 105us/step - loss: 13.2873 - mae: 2.5535 - val_loss: 16.2769 - val_mae: 2.6461\n",
      "Epoch 436/1000\n",
      "364/364 [==============================] - 0s 140us/step - loss: 13.2133 - mae: 2.5280 - val_loss: 16.3983 - val_mae: 2.6832\n",
      "Epoch 437/1000\n",
      "364/364 [==============================] - 0s 114us/step - loss: 13.2102 - mae: 2.5807 - val_loss: 16.0847 - val_mae: 2.6198\n",
      "Epoch 438/1000\n",
      "364/364 [==============================] - 0s 150us/step - loss: 13.2714 - mae: 2.5555 - val_loss: 16.1529 - val_mae: 2.6441\n",
      "Epoch 439/1000\n",
      "364/364 [==============================] - 0s 156us/step - loss: 13.2452 - mae: 2.5642 - val_loss: 15.5182 - val_mae: 2.5054\n",
      "Epoch 440/1000\n",
      "364/364 [==============================] - 0s 174us/step - loss: 13.2544 - mae: 2.5540 - val_loss: 16.1677 - val_mae: 2.6554\n",
      "Epoch 441/1000\n",
      "364/364 [==============================] - 0s 186us/step - loss: 13.1768 - mae: 2.5463 - val_loss: 15.9189 - val_mae: 2.6043\n",
      "Epoch 442/1000\n",
      "364/364 [==============================] - 0s 187us/step - loss: 13.1090 - mae: 2.5675 - val_loss: 16.2270 - val_mae: 2.5254\n",
      "Epoch 443/1000\n",
      "364/364 [==============================] - 0s 198us/step - loss: 13.4008 - mae: 2.5494 - val_loss: 15.6242 - val_mae: 2.4779\n",
      "Epoch 444/1000\n",
      "364/364 [==============================] - 0s 180us/step - loss: 13.2056 - mae: 2.5725 - val_loss: 16.0335 - val_mae: 2.6135\n",
      "Epoch 445/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 13.1600 - mae: 2.5764 - val_loss: 16.1185 - val_mae: 2.5086\n",
      "Epoch 446/1000\n",
      "364/364 [==============================] - 0s 160us/step - loss: 13.2925 - mae: 2.5495 - val_loss: 15.9889 - val_mae: 2.6234\n",
      "Epoch 447/1000\n",
      "364/364 [==============================] - 0s 186us/step - loss: 13.2171 - mae: 2.5710 - val_loss: 15.5527 - val_mae: 2.5019\n",
      "Epoch 448/1000\n",
      "364/364 [==============================] - 0s 317us/step - loss: 13.2429 - mae: 2.5469 - val_loss: 15.5116 - val_mae: 2.4554\n",
      "Epoch 449/1000\n",
      "364/364 [==============================] - 0s 240us/step - loss: 13.1176 - mae: 2.5145 - val_loss: 15.5039 - val_mae: 2.4560\n",
      "Epoch 450/1000\n",
      "364/364 [==============================] - 0s 248us/step - loss: 13.3804 - mae: 2.5410 - val_loss: 15.4517 - val_mae: 2.4676\n",
      "Epoch 451/1000\n",
      "364/364 [==============================] - 0s 191us/step - loss: 13.0982 - mae: 2.5299 - val_loss: 15.4859 - val_mae: 2.4565\n",
      "Epoch 452/1000\n",
      "364/364 [==============================] - 0s 182us/step - loss: 13.1249 - mae: 2.5642 - val_loss: 15.5988 - val_mae: 2.4619\n",
      "Epoch 453/1000\n",
      "364/364 [==============================] - 0s 203us/step - loss: 13.1942 - mae: 2.5505 - val_loss: 16.0023 - val_mae: 2.6198\n",
      "Epoch 454/1000\n",
      "364/364 [==============================] - 0s 164us/step - loss: 13.2519 - mae: 2.5668 - val_loss: 16.4125 - val_mae: 2.7050\n",
      "Epoch 455/1000\n",
      "364/364 [==============================] - 0s 156us/step - loss: 13.0958 - mae: 2.5448 - val_loss: 15.5064 - val_mae: 2.4887\n",
      "Epoch 456/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 13.0491 - mae: 2.5408 - val_loss: 15.7890 - val_mae: 2.5888\n",
      "Epoch 457/1000\n",
      "364/364 [==============================] - 0s 171us/step - loss: 13.0802 - mae: 2.5671 - val_loss: 15.4212 - val_mae: 2.4628\n",
      "Epoch 458/1000\n",
      "364/364 [==============================] - 0s 152us/step - loss: 13.1066 - mae: 2.5339 - val_loss: 15.6698 - val_mae: 2.5433\n",
      "Epoch 459/1000\n",
      "364/364 [==============================] - 0s 192us/step - loss: 13.2183 - mae: 2.5422 - val_loss: 15.6700 - val_mae: 2.5490\n",
      "Epoch 460/1000\n",
      "364/364 [==============================] - 0s 290us/step - loss: 13.1060 - mae: 2.5513 - val_loss: 15.4369 - val_mae: 2.4556\n",
      "Epoch 461/1000\n",
      "364/364 [==============================] - 0s 266us/step - loss: 13.0923 - mae: 2.5228 - val_loss: 15.3396 - val_mae: 2.4508\n",
      "Epoch 462/1000\n",
      "364/364 [==============================] - 0s 190us/step - loss: 12.9804 - mae: 2.5296 - val_loss: 16.0235 - val_mae: 2.6308\n",
      "Epoch 463/1000\n",
      "364/364 [==============================] - 0s 171us/step - loss: 12.9497 - mae: 2.5047 - val_loss: 15.7313 - val_mae: 2.5515\n",
      "Epoch 464/1000\n",
      "364/364 [==============================] - 0s 246us/step - loss: 12.9219 - mae: 2.5415 - val_loss: 15.7677 - val_mae: 2.4786\n",
      "Epoch 465/1000\n",
      "364/364 [==============================] - 0s 232us/step - loss: 12.9000 - mae: 2.5282 - val_loss: 15.6194 - val_mae: 2.5282\n",
      "Epoch 466/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 203us/step - loss: 13.0408 - mae: 2.5242 - val_loss: 16.2334 - val_mae: 2.7001\n",
      "Epoch 467/1000\n",
      "364/364 [==============================] - 0s 192us/step - loss: 12.9275 - mae: 2.5377 - val_loss: 15.4315 - val_mae: 2.4995\n",
      "Epoch 468/1000\n",
      "364/364 [==============================] - 0s 165us/step - loss: 12.8420 - mae: 2.5109 - val_loss: 15.4783 - val_mae: 2.4969\n",
      "Epoch 469/1000\n",
      "364/364 [==============================] - 0s 161us/step - loss: 12.9773 - mae: 2.5183 - val_loss: 15.4450 - val_mae: 2.4678\n",
      "Epoch 470/1000\n",
      "364/364 [==============================] - 0s 175us/step - loss: 12.8559 - mae: 2.5179 - val_loss: 15.2352 - val_mae: 2.4483\n",
      "Epoch 471/1000\n",
      "364/364 [==============================] - 0s 157us/step - loss: 12.8734 - mae: 2.5252 - val_loss: 15.3345 - val_mae: 2.4490\n",
      "Epoch 472/1000\n",
      "364/364 [==============================] - 0s 232us/step - loss: 12.7400 - mae: 2.4784 - val_loss: 15.4772 - val_mae: 2.5077\n",
      "Epoch 473/1000\n",
      "364/364 [==============================] - 0s 187us/step - loss: 12.8162 - mae: 2.5097 - val_loss: 15.4654 - val_mae: 2.4632\n",
      "Epoch 474/1000\n",
      "364/364 [==============================] - 0s 148us/step - loss: 13.0199 - mae: 2.5162 - val_loss: 16.5033 - val_mae: 2.7573\n",
      "Epoch 475/1000\n",
      "364/364 [==============================] - 0s 162us/step - loss: 12.8613 - mae: 2.5467 - val_loss: 15.9524 - val_mae: 2.6262\n",
      "Epoch 476/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 12.8253 - mae: 2.5233 - val_loss: 15.5814 - val_mae: 2.5282\n",
      "Epoch 477/1000\n",
      "364/364 [==============================] - 0s 190us/step - loss: 12.7652 - mae: 2.5009 - val_loss: 15.6208 - val_mae: 2.5335\n",
      "Epoch 478/1000\n",
      "364/364 [==============================] - 0s 161us/step - loss: 12.7610 - mae: 2.5176 - val_loss: 16.7996 - val_mae: 2.8126\n",
      "Epoch 479/1000\n",
      "364/364 [==============================] - 0s 171us/step - loss: 12.9540 - mae: 2.5371 - val_loss: 15.2985 - val_mae: 2.4729\n",
      "Epoch 480/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 12.7294 - mae: 2.5265 - val_loss: 15.3167 - val_mae: 2.4670\n",
      "Epoch 481/1000\n",
      "364/364 [==============================] - 0s 140us/step - loss: 12.6936 - mae: 2.5108 - val_loss: 15.3893 - val_mae: 2.4652\n",
      "Epoch 482/1000\n",
      "364/364 [==============================] - 0s 191us/step - loss: 12.6989 - mae: 2.5074 - val_loss: 15.3932 - val_mae: 2.4793\n",
      "Epoch 483/1000\n",
      "364/364 [==============================] - 0s 231us/step - loss: 12.6597 - mae: 2.5035 - val_loss: 15.7831 - val_mae: 2.6248\n",
      "Epoch 484/1000\n",
      "364/364 [==============================] - 0s 191us/step - loss: 12.8759 - mae: 2.5061 - val_loss: 15.8145 - val_mae: 2.6304\n",
      "Epoch 485/1000\n",
      "364/364 [==============================] - 0s 148us/step - loss: 12.6810 - mae: 2.5217 - val_loss: 15.3048 - val_mae: 2.4615\n",
      "Epoch 486/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 12.8886 - mae: 2.5252 - val_loss: 15.3811 - val_mae: 2.5071\n",
      "Epoch 487/1000\n",
      "364/364 [==============================] - 0s 268us/step - loss: 12.6494 - mae: 2.5039 - val_loss: 15.5136 - val_mae: 2.4862\n",
      "Epoch 488/1000\n",
      "364/364 [==============================] - 0s 247us/step - loss: 12.6742 - mae: 2.4820 - val_loss: 16.0889 - val_mae: 2.6945\n",
      "Epoch 489/1000\n",
      "364/364 [==============================] - 0s 201us/step - loss: 12.7283 - mae: 2.5192 - val_loss: 15.1802 - val_mae: 2.4802\n",
      "Epoch 490/1000\n",
      "364/364 [==============================] - 0s 193us/step - loss: 12.7254 - mae: 2.5139 - val_loss: 15.2696 - val_mae: 2.4761\n",
      "Epoch 491/1000\n",
      "364/364 [==============================] - 0s 215us/step - loss: 12.5880 - mae: 2.4951 - val_loss: 15.2517 - val_mae: 2.4574\n",
      "Epoch 492/1000\n",
      "364/364 [==============================] - 0s 215us/step - loss: 12.7268 - mae: 2.4837 - val_loss: 15.4173 - val_mae: 2.5189\n",
      "Epoch 493/1000\n",
      "364/364 [==============================] - 0s 407us/step - loss: 12.7112 - mae: 2.5106 - val_loss: 15.2849 - val_mae: 2.4895\n",
      "Epoch 494/1000\n",
      "364/364 [==============================] - 0s 268us/step - loss: 12.5356 - mae: 2.4964 - val_loss: 15.2721 - val_mae: 2.4769\n",
      "Epoch 495/1000\n",
      "364/364 [==============================] - 0s 303us/step - loss: 12.9594 - mae: 2.5478 - val_loss: 15.2536 - val_mae: 2.4814\n",
      "Epoch 496/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 12.5568 - mae: 2.4920 - val_loss: 15.7877 - val_mae: 2.6094\n",
      "Epoch 497/1000\n",
      "364/364 [==============================] - 0s 185us/step - loss: 12.5154 - mae: 2.4984 - val_loss: 15.8210 - val_mae: 2.6353\n",
      "Epoch 498/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 12.6703 - mae: 2.5073 - val_loss: 15.4547 - val_mae: 2.5461\n",
      "Epoch 499/1000\n",
      "364/364 [==============================] - 0s 217us/step - loss: 12.6700 - mae: 2.5024 - val_loss: 15.5647 - val_mae: 2.5726\n",
      "Epoch 500/1000\n",
      "364/364 [==============================] - 0s 239us/step - loss: 12.6623 - mae: 2.5044 - val_loss: 15.1733 - val_mae: 2.4875\n",
      "Epoch 501/1000\n",
      "364/364 [==============================] - 0s 170us/step - loss: 12.5818 - mae: 2.4689 - val_loss: 15.5186 - val_mae: 2.5739\n",
      "Epoch 502/1000\n",
      "364/364 [==============================] - 0s 135us/step - loss: 12.4898 - mae: 2.4779 - val_loss: 16.5899 - val_mae: 2.7991\n",
      "Epoch 503/1000\n",
      "364/364 [==============================] - 0s 145us/step - loss: 12.6945 - mae: 2.5391 - val_loss: 15.1520 - val_mae: 2.4799\n",
      "Epoch 504/1000\n",
      "364/364 [==============================] - 0s 172us/step - loss: 12.5419 - mae: 2.4818 - val_loss: 16.1091 - val_mae: 2.7120\n",
      "Epoch 505/1000\n",
      "364/364 [==============================] - 0s 160us/step - loss: 12.5208 - mae: 2.4996 - val_loss: 15.0285 - val_mae: 2.4541\n",
      "Epoch 506/1000\n",
      "364/364 [==============================] - 0s 274us/step - loss: 12.5960 - mae: 2.5106 - val_loss: 15.5174 - val_mae: 2.4997\n",
      "Epoch 507/1000\n",
      "364/364 [==============================] - 0s 168us/step - loss: 12.5962 - mae: 2.4909 - val_loss: 15.3906 - val_mae: 2.4869\n",
      "Epoch 508/1000\n",
      "364/364 [==============================] - 0s 164us/step - loss: 12.6165 - mae: 2.4705 - val_loss: 15.4306 - val_mae: 2.5669\n",
      "Epoch 509/1000\n",
      "364/364 [==============================] - 0s 172us/step - loss: 12.3644 - mae: 2.4881 - val_loss: 15.6471 - val_mae: 2.5193\n",
      "Epoch 510/1000\n",
      "364/364 [==============================] - 0s 231us/step - loss: 12.5345 - mae: 2.4840 - val_loss: 15.1009 - val_mae: 2.4819\n",
      "Epoch 511/1000\n",
      "364/364 [==============================] - 0s 157us/step - loss: 12.4759 - mae: 2.4743 - val_loss: 16.2960 - val_mae: 2.7564\n",
      "Epoch 512/1000\n",
      "364/364 [==============================] - 0s 243us/step - loss: 12.4140 - mae: 2.5092 - val_loss: 15.1588 - val_mae: 2.4875\n",
      "Epoch 513/1000\n",
      "364/364 [==============================] - 0s 239us/step - loss: 12.3015 - mae: 2.4584 - val_loss: 15.1984 - val_mae: 2.4871\n",
      "Epoch 514/1000\n",
      "364/364 [==============================] - 0s 283us/step - loss: 12.5372 - mae: 2.4897 - val_loss: 15.1226 - val_mae: 2.4706\n",
      "Epoch 515/1000\n",
      "364/364 [==============================] - 0s 262us/step - loss: 12.6498 - mae: 2.5080 - val_loss: 15.2191 - val_mae: 2.5026\n",
      "Epoch 516/1000\n",
      "364/364 [==============================] - 0s 246us/step - loss: 12.3363 - mae: 2.4747 - val_loss: 15.3751 - val_mae: 2.5472\n",
      "Epoch 517/1000\n",
      "364/364 [==============================] - 0s 227us/step - loss: 12.5128 - mae: 2.5034 - val_loss: 14.9837 - val_mae: 2.4626\n",
      "Epoch 518/1000\n",
      "364/364 [==============================] - 0s 207us/step - loss: 12.3505 - mae: 2.4413 - val_loss: 15.1053 - val_mae: 2.4734\n",
      "Epoch 519/1000\n",
      "364/364 [==============================] - 0s 217us/step - loss: 12.3649 - mae: 2.4587 - val_loss: 15.4479 - val_mae: 2.4917\n",
      "Epoch 520/1000\n",
      "364/364 [==============================] - 0s 196us/step - loss: 12.2168 - mae: 2.4379 - val_loss: 15.2927 - val_mae: 2.5283\n",
      "Epoch 521/1000\n",
      "364/364 [==============================] - 0s 236us/step - loss: 12.5340 - mae: 2.4872 - val_loss: 15.2856 - val_mae: 2.5121\n",
      "Epoch 522/1000\n",
      "364/364 [==============================] - 0s 169us/step - loss: 12.2471 - mae: 2.4707 - val_loss: 16.3957 - val_mae: 2.6223\n",
      "Epoch 523/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 12.6024 - mae: 2.5312 - val_loss: 15.3615 - val_mae: 2.5559\n",
      "Epoch 524/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 248us/step - loss: 12.3426 - mae: 2.4727 - val_loss: 15.5925 - val_mae: 2.6070\n",
      "Epoch 525/1000\n",
      "364/364 [==============================] - 0s 320us/step - loss: 12.2015 - mae: 2.4587 - val_loss: 15.2544 - val_mae: 2.5378\n",
      "Epoch 526/1000\n",
      "364/364 [==============================] - 0s 181us/step - loss: 12.2766 - mae: 2.4703 - val_loss: 16.3750 - val_mae: 2.7663\n",
      "Epoch 527/1000\n",
      "364/364 [==============================] - 0s 233us/step - loss: 12.3760 - mae: 2.4820 - val_loss: 15.3058 - val_mae: 2.4873\n",
      "Epoch 528/1000\n",
      "364/364 [==============================] - 0s 200us/step - loss: 12.4479 - mae: 2.4766 - val_loss: 15.1794 - val_mae: 2.5073\n",
      "Epoch 529/1000\n",
      "364/364 [==============================] - 0s 239us/step - loss: 11.9719 - mae: 2.4365 - val_loss: 17.5103 - val_mae: 2.9810\n",
      "Epoch 530/1000\n",
      "364/364 [==============================] - 0s 176us/step - loss: 12.3024 - mae: 2.4566 - val_loss: 15.6083 - val_mae: 2.6233\n",
      "Epoch 531/1000\n",
      "364/364 [==============================] - 0s 256us/step - loss: 12.5403 - mae: 2.4994 - val_loss: 15.1884 - val_mae: 2.5226\n",
      "Epoch 532/1000\n",
      "364/364 [==============================] - 0s 204us/step - loss: 12.3541 - mae: 2.4646 - val_loss: 16.3394 - val_mae: 2.7722\n",
      "Epoch 533/1000\n",
      "364/364 [==============================] - 0s 260us/step - loss: 12.3380 - mae: 2.4629 - val_loss: 15.1572 - val_mae: 2.5062\n",
      "Epoch 534/1000\n",
      "364/364 [==============================] - 0s 206us/step - loss: 12.3091 - mae: 2.5062 - val_loss: 14.9366 - val_mae: 2.4524\n",
      "Epoch 535/1000\n",
      "364/364 [==============================] - 0s 244us/step - loss: 12.2334 - mae: 2.4660 - val_loss: 14.9838 - val_mae: 2.4595\n",
      "Epoch 536/1000\n",
      "364/364 [==============================] - 0s 171us/step - loss: 12.2023 - mae: 2.4554 - val_loss: 14.9912 - val_mae: 2.4974\n",
      "Epoch 537/1000\n",
      "364/364 [==============================] - 0s 185us/step - loss: 12.3183 - mae: 2.4818 - val_loss: 15.1018 - val_mae: 2.5163\n",
      "Epoch 538/1000\n",
      "364/364 [==============================] - 0s 138us/step - loss: 12.2546 - mae: 2.4576 - val_loss: 15.0607 - val_mae: 2.4698\n",
      "Epoch 539/1000\n",
      "364/364 [==============================] - 0s 271us/step - loss: 12.2429 - mae: 2.4818 - val_loss: 15.0521 - val_mae: 2.4670\n",
      "Epoch 540/1000\n",
      "364/364 [==============================] - 0s 349us/step - loss: 12.3634 - mae: 2.4739 - val_loss: 14.9360 - val_mae: 2.4765\n",
      "Epoch 541/1000\n",
      "364/364 [==============================] - 0s 297us/step - loss: 12.1785 - mae: 2.4551 - val_loss: 15.6719 - val_mae: 2.6466\n",
      "Epoch 542/1000\n",
      "364/364 [==============================] - 0s 260us/step - loss: 12.1071 - mae: 2.4846 - val_loss: 15.4182 - val_mae: 2.5184\n",
      "Epoch 543/1000\n",
      "364/364 [==============================] - 0s 166us/step - loss: 12.5096 - mae: 2.4897 - val_loss: 14.9597 - val_mae: 2.4663\n",
      "Epoch 544/1000\n",
      "364/364 [==============================] - 0s 196us/step - loss: 12.2608 - mae: 2.4637 - val_loss: 15.2078 - val_mae: 2.4911\n",
      "Epoch 545/1000\n",
      "364/364 [==============================] - 0s 189us/step - loss: 12.1419 - mae: 2.4495 - val_loss: 14.7792 - val_mae: 2.4624\n",
      "Epoch 546/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 12.2589 - mae: 2.4598 - val_loss: 15.1403 - val_mae: 2.4950\n",
      "Epoch 547/1000\n",
      "364/364 [==============================] - 0s 155us/step - loss: 12.2613 - mae: 2.4548 - val_loss: 15.0523 - val_mae: 2.5034\n",
      "Epoch 548/1000\n",
      "364/364 [==============================] - 0s 244us/step - loss: 12.0299 - mae: 2.4446 - val_loss: 14.9767 - val_mae: 2.5063\n",
      "Epoch 549/1000\n",
      "364/364 [==============================] - 0s 277us/step - loss: 12.1811 - mae: 2.4508 - val_loss: 14.8448 - val_mae: 2.4606\n",
      "Epoch 550/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 12.2694 - mae: 2.4515 - val_loss: 15.0820 - val_mae: 2.5380\n",
      "Epoch 551/1000\n",
      "364/364 [==============================] - 0s 204us/step - loss: 12.1372 - mae: 2.4721 - val_loss: 15.4009 - val_mae: 2.6262\n",
      "Epoch 552/1000\n",
      "364/364 [==============================] - 0s 205us/step - loss: 12.2058 - mae: 2.4552 - val_loss: 14.7494 - val_mae: 2.4674\n",
      "Epoch 553/1000\n",
      "364/364 [==============================] - 0s 200us/step - loss: 12.2588 - mae: 2.4482 - val_loss: 14.7988 - val_mae: 2.5024\n",
      "Epoch 554/1000\n",
      "364/364 [==============================] - 0s 197us/step - loss: 12.2114 - mae: 2.4470 - val_loss: 14.7731 - val_mae: 2.4729\n",
      "Epoch 555/1000\n",
      "364/364 [==============================] - 0s 163us/step - loss: 12.2711 - mae: 2.4510 - val_loss: 14.7375 - val_mae: 2.4592\n",
      "Epoch 556/1000\n",
      "364/364 [==============================] - 0s 138us/step - loss: 12.0994 - mae: 2.4605 - val_loss: 14.7672 - val_mae: 2.4645\n",
      "Epoch 557/1000\n",
      "364/364 [==============================] - 0s 227us/step - loss: 11.9897 - mae: 2.4478 - val_loss: 15.2808 - val_mae: 2.5790\n",
      "Epoch 558/1000\n",
      "364/364 [==============================] - 0s 246us/step - loss: 12.2004 - mae: 2.4714 - val_loss: 15.1074 - val_mae: 2.5258\n",
      "Epoch 559/1000\n",
      "364/364 [==============================] - 0s 188us/step - loss: 11.9217 - mae: 2.4419 - val_loss: 15.2834 - val_mae: 2.5919\n",
      "Epoch 560/1000\n",
      "364/364 [==============================] - 0s 212us/step - loss: 12.0443 - mae: 2.4322 - val_loss: 15.3466 - val_mae: 2.5936\n",
      "Epoch 561/1000\n",
      "364/364 [==============================] - 0s 252us/step - loss: 12.1726 - mae: 2.4505 - val_loss: 14.9076 - val_mae: 2.5090\n",
      "Epoch 562/1000\n",
      "364/364 [==============================] - 0s 239us/step - loss: 11.8877 - mae: 2.4204 - val_loss: 15.7023 - val_mae: 2.6749\n",
      "Epoch 563/1000\n",
      "364/364 [==============================] - 0s 255us/step - loss: 11.9728 - mae: 2.4470 - val_loss: 15.2950 - val_mae: 2.5719\n",
      "Epoch 564/1000\n",
      "364/364 [==============================] - 0s 212us/step - loss: 12.1155 - mae: 2.4421 - val_loss: 14.8623 - val_mae: 2.4724\n",
      "Epoch 565/1000\n",
      "364/364 [==============================] - 0s 148us/step - loss: 11.8928 - mae: 2.4254 - val_loss: 15.2783 - val_mae: 2.5600\n",
      "Epoch 566/1000\n",
      "364/364 [==============================] - 0s 166us/step - loss: 12.1522 - mae: 2.4497 - val_loss: 14.8216 - val_mae: 2.4663\n",
      "Epoch 567/1000\n",
      "364/364 [==============================] - 0s 160us/step - loss: 12.3153 - mae: 2.4602 - val_loss: 14.9470 - val_mae: 2.5302\n",
      "Epoch 568/1000\n",
      "364/364 [==============================] - 0s 242us/step - loss: 12.0556 - mae: 2.4537 - val_loss: 14.8136 - val_mae: 2.4732\n",
      "Epoch 569/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 11.8998 - mae: 2.4359 - val_loss: 14.8621 - val_mae: 2.4659\n",
      "Epoch 570/1000\n",
      "364/364 [==============================] - 0s 232us/step - loss: 12.0017 - mae: 2.4561 - val_loss: 15.0073 - val_mae: 2.4702\n",
      "Epoch 571/1000\n",
      "364/364 [==============================] - 0s 183us/step - loss: 12.1257 - mae: 2.4565 - val_loss: 14.7973 - val_mae: 2.4842\n",
      "Epoch 572/1000\n",
      "364/364 [==============================] - 0s 167us/step - loss: 12.1426 - mae: 2.4462 - val_loss: 14.8268 - val_mae: 2.4627\n",
      "Epoch 573/1000\n",
      "364/364 [==============================] - 0s 218us/step - loss: 11.9697 - mae: 2.4278 - val_loss: 14.7719 - val_mae: 2.4640\n",
      "Epoch 574/1000\n",
      "364/364 [==============================] - 0s 243us/step - loss: 11.9620 - mae: 2.4314 - val_loss: 14.9460 - val_mae: 2.4779\n",
      "Epoch 575/1000\n",
      "364/364 [==============================] - 0s 150us/step - loss: 11.7860 - mae: 2.4316 - val_loss: 14.8406 - val_mae: 2.5062\n",
      "Epoch 576/1000\n",
      "364/364 [==============================] - 0s 269us/step - loss: 11.8301 - mae: 2.4307 - val_loss: 14.7538 - val_mae: 2.4976\n",
      "Epoch 577/1000\n",
      "364/364 [==============================] - 0s 322us/step - loss: 11.9200 - mae: 2.4256 - val_loss: 14.7563 - val_mae: 2.4827\n",
      "Epoch 578/1000\n",
      "364/364 [==============================] - 0s 178us/step - loss: 12.0068 - mae: 2.4427 - val_loss: 14.7125 - val_mae: 2.4630\n",
      "Epoch 579/1000\n",
      "364/364 [==============================] - 0s 282us/step - loss: 11.9489 - mae: 2.4305 - val_loss: 14.7259 - val_mae: 2.4605\n",
      "Epoch 580/1000\n",
      "364/364 [==============================] - 0s 168us/step - loss: 11.8179 - mae: 2.4249 - val_loss: 14.6391 - val_mae: 2.4690\n",
      "Epoch 581/1000\n",
      "364/364 [==============================] - 0s 188us/step - loss: 11.7468 - mae: 2.4170 - val_loss: 16.7549 - val_mae: 2.9145\n",
      "Epoch 582/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 157us/step - loss: 11.9628 - mae: 2.4528 - val_loss: 14.7066 - val_mae: 2.4666\n",
      "Epoch 583/1000\n",
      "364/364 [==============================] - 0s 256us/step - loss: 11.9022 - mae: 2.4400 - val_loss: 15.5786 - val_mae: 2.6773\n",
      "Epoch 584/1000\n",
      "364/364 [==============================] - 0s 163us/step - loss: 11.8983 - mae: 2.4285 - val_loss: 14.9295 - val_mae: 2.5344\n",
      "Epoch 585/1000\n",
      "364/364 [==============================] - 0s 153us/step - loss: 12.1176 - mae: 2.4449 - val_loss: 15.2123 - val_mae: 2.5956\n",
      "Epoch 586/1000\n",
      "364/364 [==============================] - 0s 225us/step - loss: 11.8285 - mae: 2.4299 - val_loss: 14.6532 - val_mae: 2.4658\n",
      "Epoch 587/1000\n",
      "364/364 [==============================] - 0s 187us/step - loss: 11.9990 - mae: 2.4508 - val_loss: 14.9008 - val_mae: 2.5212\n",
      "Epoch 588/1000\n",
      "364/364 [==============================] - 0s 226us/step - loss: 11.7968 - mae: 2.4189 - val_loss: 14.8572 - val_mae: 2.5047\n",
      "Epoch 589/1000\n",
      "364/364 [==============================] - 0s 187us/step - loss: 11.8809 - mae: 2.4327 - val_loss: 14.6547 - val_mae: 2.4747\n",
      "Epoch 590/1000\n",
      "364/364 [==============================] - 0s 157us/step - loss: 11.7485 - mae: 2.4134 - val_loss: 14.7944 - val_mae: 2.4968\n",
      "Epoch 591/1000\n",
      "364/364 [==============================] - 0s 277us/step - loss: 11.7659 - mae: 2.4424 - val_loss: 14.6782 - val_mae: 2.4798\n",
      "Epoch 592/1000\n",
      "364/364 [==============================] - 0s 169us/step - loss: 11.8268 - mae: 2.4135 - val_loss: 14.7738 - val_mae: 2.5112\n",
      "Epoch 593/1000\n",
      "364/364 [==============================] - 0s 245us/step - loss: 11.8321 - mae: 2.4377 - val_loss: 14.9435 - val_mae: 2.5570\n",
      "Epoch 594/1000\n",
      "364/364 [==============================] - 0s 227us/step - loss: 11.7169 - mae: 2.4249 - val_loss: 14.6571 - val_mae: 2.4722\n",
      "Epoch 595/1000\n",
      "364/364 [==============================] - 0s 226us/step - loss: 11.7724 - mae: 2.4104 - val_loss: 15.8965 - val_mae: 2.7595\n",
      "Epoch 596/1000\n",
      "364/364 [==============================] - 0s 219us/step - loss: 11.8371 - mae: 2.4457 - val_loss: 15.0586 - val_mae: 2.5062\n",
      "Epoch 597/1000\n",
      "364/364 [==============================] - 0s 184us/step - loss: 11.8768 - mae: 2.4197 - val_loss: 14.6624 - val_mae: 2.5078\n",
      "Epoch 598/1000\n",
      "364/364 [==============================] - 0s 196us/step - loss: 11.9450 - mae: 2.4311 - val_loss: 14.7422 - val_mae: 2.4719\n",
      "Epoch 599/1000\n",
      "364/364 [==============================] - 0s 200us/step - loss: 11.7739 - mae: 2.4153 - val_loss: 14.6754 - val_mae: 2.4649\n",
      "Epoch 600/1000\n",
      "364/364 [==============================] - 0s 222us/step - loss: 11.7830 - mae: 2.3923 - val_loss: 15.4197 - val_mae: 2.6659\n",
      "Epoch 601/1000\n",
      "364/364 [==============================] - 0s 173us/step - loss: 11.7182 - mae: 2.4415 - val_loss: 14.5918 - val_mae: 2.4852\n",
      "Epoch 602/1000\n",
      "364/364 [==============================] - 0s 208us/step - loss: 11.8426 - mae: 2.4335 - val_loss: 14.5406 - val_mae: 2.4656\n",
      "Epoch 603/1000\n",
      "364/364 [==============================] - 0s 173us/step - loss: 11.8028 - mae: 2.4211 - val_loss: 14.8891 - val_mae: 2.5479\n",
      "Epoch 604/1000\n",
      "364/364 [==============================] - 0s 146us/step - loss: 11.7036 - mae: 2.4128 - val_loss: 14.6941 - val_mae: 2.4756\n",
      "Epoch 605/1000\n",
      "364/364 [==============================] - 0s 227us/step - loss: 11.5926 - mae: 2.3996 - val_loss: 14.7746 - val_mae: 2.5430\n",
      "Epoch 606/1000\n",
      "364/364 [==============================] - 0s 203us/step - loss: 11.7272 - mae: 2.4107 - val_loss: 14.4587 - val_mae: 2.4928\n",
      "Epoch 607/1000\n",
      "364/364 [==============================] - 0s 205us/step - loss: 11.5962 - mae: 2.3965 - val_loss: 15.1882 - val_mae: 2.6354\n",
      "Epoch 608/1000\n",
      "364/364 [==============================] - 0s 136us/step - loss: 11.7540 - mae: 2.3938 - val_loss: 14.5107 - val_mae: 2.4685\n",
      "Epoch 609/1000\n",
      "364/364 [==============================] - 0s 149us/step - loss: 11.6687 - mae: 2.4017 - val_loss: 14.8628 - val_mae: 2.5630\n",
      "Epoch 610/1000\n",
      "364/364 [==============================] - 0s 193us/step - loss: 11.7920 - mae: 2.4264 - val_loss: 14.4967 - val_mae: 2.4704\n",
      "Epoch 611/1000\n",
      "364/364 [==============================] - 0s 150us/step - loss: 11.7381 - mae: 2.4029 - val_loss: 14.4624 - val_mae: 2.4861\n",
      "Epoch 612/1000\n",
      "364/364 [==============================] - 0s 120us/step - loss: 11.6413 - mae: 2.4084 - val_loss: 14.5574 - val_mae: 2.4741\n",
      "Epoch 613/1000\n",
      "364/364 [==============================] - 0s 110us/step - loss: 11.7828 - mae: 2.4028 - val_loss: 14.5548 - val_mae: 2.4761\n",
      "Epoch 614/1000\n",
      "364/364 [==============================] - 0s 164us/step - loss: 11.6578 - mae: 2.4005 - val_loss: 15.5363 - val_mae: 2.6847\n",
      "Epoch 615/1000\n",
      "364/364 [==============================] - 0s 254us/step - loss: 11.7763 - mae: 2.4277 - val_loss: 14.6822 - val_mae: 2.4878\n",
      "Epoch 616/1000\n",
      "364/364 [==============================] - 0s 193us/step - loss: 11.7329 - mae: 2.4323 - val_loss: 14.5437 - val_mae: 2.4727\n",
      "Epoch 617/1000\n",
      "364/364 [==============================] - 0s 157us/step - loss: 11.6259 - mae: 2.3848 - val_loss: 15.1462 - val_mae: 2.6146\n",
      "Epoch 618/1000\n",
      "364/364 [==============================] - 0s 157us/step - loss: 11.6095 - mae: 2.3931 - val_loss: 14.4006 - val_mae: 2.4679\n",
      "Epoch 619/1000\n",
      "364/364 [==============================] - 0s 262us/step - loss: 11.6813 - mae: 2.3940 - val_loss: 14.5020 - val_mae: 2.4851\n",
      "Epoch 620/1000\n",
      "364/364 [==============================] - 0s 320us/step - loss: 11.7142 - mae: 2.4085 - val_loss: 14.4672 - val_mae: 2.4931\n",
      "Epoch 621/1000\n",
      "364/364 [==============================] - 0s 262us/step - loss: 11.5112 - mae: 2.3943 - val_loss: 14.4724 - val_mae: 2.5026\n",
      "Epoch 622/1000\n",
      "364/364 [==============================] - 0s 198us/step - loss: 11.7240 - mae: 2.4263 - val_loss: 14.6065 - val_mae: 2.5262\n",
      "Epoch 623/1000\n",
      "364/364 [==============================] - 0s 331us/step - loss: 11.8187 - mae: 2.4275 - val_loss: 14.4474 - val_mae: 2.4853\n",
      "Epoch 624/1000\n",
      "364/364 [==============================] - 0s 189us/step - loss: 11.4941 - mae: 2.3813 - val_loss: 14.5969 - val_mae: 2.5177\n",
      "Epoch 625/1000\n",
      "364/364 [==============================] - 0s 202us/step - loss: 11.5001 - mae: 2.4157 - val_loss: 14.5981 - val_mae: 2.4784\n",
      "Epoch 626/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 11.8378 - mae: 2.4028 - val_loss: 14.5313 - val_mae: 2.4722\n",
      "Epoch 627/1000\n",
      "364/364 [==============================] - 0s 129us/step - loss: 11.5339 - mae: 2.3969 - val_loss: 14.6679 - val_mae: 2.4788\n",
      "Epoch 628/1000\n",
      "364/364 [==============================] - 0s 185us/step - loss: 11.7892 - mae: 2.4040 - val_loss: 14.6842 - val_mae: 2.4855\n",
      "Epoch 629/1000\n",
      "364/364 [==============================] - 0s 320us/step - loss: 11.5018 - mae: 2.4046 - val_loss: 14.7053 - val_mae: 2.5142\n",
      "Epoch 630/1000\n",
      "364/364 [==============================] - 0s 257us/step - loss: 11.3283 - mae: 2.3717 - val_loss: 15.0448 - val_mae: 2.6025\n",
      "Epoch 631/1000\n",
      "364/364 [==============================] - 0s 326us/step - loss: 11.6468 - mae: 2.4186 - val_loss: 15.0420 - val_mae: 2.6130\n",
      "Epoch 632/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 11.6042 - mae: 2.4016 - val_loss: 14.8613 - val_mae: 2.5809\n",
      "Epoch 633/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 11.4454 - mae: 2.4054 - val_loss: 14.4529 - val_mae: 2.4792\n",
      "Epoch 634/1000\n",
      "364/364 [==============================] - 0s 211us/step - loss: 11.7548 - mae: 2.4134 - val_loss: 14.4402 - val_mae: 2.4808\n",
      "Epoch 635/1000\n",
      "364/364 [==============================] - 0s 167us/step - loss: 11.4441 - mae: 2.3716 - val_loss: 14.4956 - val_mae: 2.4805\n",
      "Epoch 636/1000\n",
      "364/364 [==============================] - 0s 168us/step - loss: 11.4489 - mae: 2.3991 - val_loss: 14.7949 - val_mae: 2.5375\n",
      "Epoch 637/1000\n",
      "364/364 [==============================] - 0s 256us/step - loss: 11.5846 - mae: 2.4104 - val_loss: 14.7029 - val_mae: 2.5227\n",
      "Epoch 638/1000\n",
      "364/364 [==============================] - 0s 242us/step - loss: 11.5131 - mae: 2.4026 - val_loss: 14.6887 - val_mae: 2.5165\n",
      "Epoch 639/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 11.5932 - mae: 2.3932 - val_loss: 14.5884 - val_mae: 2.4931\n",
      "Epoch 640/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 228us/step - loss: 11.3500 - mae: 2.3964 - val_loss: 14.6582 - val_mae: 2.4917\n",
      "Epoch 641/1000\n",
      "364/364 [==============================] - 0s 142us/step - loss: 11.4008 - mae: 2.3634 - val_loss: 14.5032 - val_mae: 2.4783\n",
      "Epoch 642/1000\n",
      "364/364 [==============================] - 0s 169us/step - loss: 11.4900 - mae: 2.3905 - val_loss: 14.5875 - val_mae: 2.4801\n",
      "Epoch 643/1000\n",
      "364/364 [==============================] - 0s 165us/step - loss: 11.5414 - mae: 2.3989 - val_loss: 14.6613 - val_mae: 2.4836\n",
      "Epoch 644/1000\n",
      "364/364 [==============================] - 0s 138us/step - loss: 11.4709 - mae: 2.3859 - val_loss: 14.7120 - val_mae: 2.5506\n",
      "Epoch 645/1000\n",
      "364/364 [==============================] - 0s 175us/step - loss: 11.6769 - mae: 2.4173 - val_loss: 14.5391 - val_mae: 2.5093\n",
      "Epoch 646/1000\n",
      "364/364 [==============================] - 0s 191us/step - loss: 11.6009 - mae: 2.3944 - val_loss: 14.5874 - val_mae: 2.5183\n",
      "Epoch 647/1000\n",
      "364/364 [==============================] - 0s 195us/step - loss: 11.2947 - mae: 2.3698 - val_loss: 14.6013 - val_mae: 2.4904\n",
      "Epoch 648/1000\n",
      "364/364 [==============================] - 0s 219us/step - loss: 11.3931 - mae: 2.3980 - val_loss: 14.7955 - val_mae: 2.5541\n",
      "Epoch 649/1000\n",
      "364/364 [==============================] - 0s 272us/step - loss: 11.2453 - mae: 2.3743 - val_loss: 14.9899 - val_mae: 2.5354\n",
      "Epoch 650/1000\n",
      "364/364 [==============================] - 0s 184us/step - loss: 11.5999 - mae: 2.4152 - val_loss: 14.9882 - val_mae: 2.5348\n",
      "Epoch 651/1000\n",
      "364/364 [==============================] - 0s 258us/step - loss: 11.3394 - mae: 2.3679 - val_loss: 15.2074 - val_mae: 2.6391\n",
      "Epoch 652/1000\n",
      "364/364 [==============================] - 0s 304us/step - loss: 11.4845 - mae: 2.3954 - val_loss: 15.6767 - val_mae: 2.7170\n",
      "Epoch 653/1000\n",
      "364/364 [==============================] - 0s 271us/step - loss: 11.5364 - mae: 2.3995 - val_loss: 14.7310 - val_mae: 2.5547\n",
      "Epoch 654/1000\n",
      "364/364 [==============================] - 0s 188us/step - loss: 11.4003 - mae: 2.3844 - val_loss: 14.7561 - val_mae: 2.5666\n",
      "Epoch 655/1000\n",
      "364/364 [==============================] - 0s 218us/step - loss: 11.3712 - mae: 2.3831 - val_loss: 14.4320 - val_mae: 2.4871\n",
      "Epoch 656/1000\n",
      "364/364 [==============================] - 0s 161us/step - loss: 11.6675 - mae: 2.4006 - val_loss: 14.4532 - val_mae: 2.5026\n",
      "Epoch 657/1000\n",
      "364/364 [==============================] - 0s 178us/step - loss: 11.5033 - mae: 2.3762 - val_loss: 14.9217 - val_mae: 2.6047\n",
      "Epoch 658/1000\n",
      "364/364 [==============================] - 0s 318us/step - loss: 11.2713 - mae: 2.3872 - val_loss: 15.4297 - val_mae: 2.7075\n",
      "Epoch 659/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 11.3712 - mae: 2.3854 - val_loss: 14.4296 - val_mae: 2.5245\n",
      "Epoch 660/1000\n",
      "364/364 [==============================] - 0s 249us/step - loss: 11.3844 - mae: 2.3765 - val_loss: 14.4378 - val_mae: 2.4981\n",
      "Epoch 661/1000\n",
      "364/364 [==============================] - 0s 165us/step - loss: 11.4945 - mae: 2.3778 - val_loss: 14.4196 - val_mae: 2.5131\n",
      "Epoch 662/1000\n",
      "364/364 [==============================] - 0s 148us/step - loss: 11.2521 - mae: 2.3749 - val_loss: 14.7446 - val_mae: 2.5915\n",
      "Epoch 663/1000\n",
      "364/364 [==============================] - 0s 145us/step - loss: 11.2756 - mae: 2.3695 - val_loss: 14.3190 - val_mae: 2.4739\n",
      "Epoch 664/1000\n",
      "364/364 [==============================] - 0s 137us/step - loss: 11.2641 - mae: 2.3676 - val_loss: 14.2985 - val_mae: 2.4815\n",
      "Epoch 665/1000\n",
      "364/364 [==============================] - 0s 102us/step - loss: 11.4080 - mae: 2.3702 - val_loss: 14.3968 - val_mae: 2.5135\n",
      "Epoch 666/1000\n",
      "364/364 [==============================] - 0s 241us/step - loss: 11.4790 - mae: 2.3875 - val_loss: 15.2742 - val_mae: 2.6822\n",
      "Epoch 667/1000\n",
      "364/364 [==============================] - 0s 208us/step - loss: 11.3299 - mae: 2.3875 - val_loss: 14.3730 - val_mae: 2.4949\n",
      "Epoch 668/1000\n",
      "364/364 [==============================] - 0s 293us/step - loss: 11.2946 - mae: 2.3569 - val_loss: 15.2557 - val_mae: 2.6826\n",
      "Epoch 669/1000\n",
      "364/364 [==============================] - 0s 194us/step - loss: 11.3357 - mae: 2.3952 - val_loss: 14.4479 - val_mae: 2.5172\n",
      "Epoch 670/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 11.4196 - mae: 2.3966 - val_loss: 14.4296 - val_mae: 2.5066\n",
      "Epoch 671/1000\n",
      "364/364 [==============================] - 0s 226us/step - loss: 11.1021 - mae: 2.3660 - val_loss: 14.9519 - val_mae: 2.5329\n",
      "Epoch 672/1000\n",
      "364/364 [==============================] - 0s 253us/step - loss: 11.3520 - mae: 2.3648 - val_loss: 14.4644 - val_mae: 2.4752\n",
      "Epoch 673/1000\n",
      "364/364 [==============================] - 0s 287us/step - loss: 11.3715 - mae: 2.3826 - val_loss: 14.8031 - val_mae: 2.5758\n",
      "Epoch 674/1000\n",
      "364/364 [==============================] - 0s 180us/step - loss: 11.2796 - mae: 2.3703 - val_loss: 14.5539 - val_mae: 2.5307\n",
      "Epoch 675/1000\n",
      "364/364 [==============================] - 0s 187us/step - loss: 11.3400 - mae: 2.3941 - val_loss: 14.3790 - val_mae: 2.4804\n",
      "Epoch 676/1000\n",
      "364/364 [==============================] - 0s 158us/step - loss: 11.2145 - mae: 2.3772 - val_loss: 14.9449 - val_mae: 2.6237\n",
      "Epoch 677/1000\n",
      "364/364 [==============================] - 0s 114us/step - loss: 11.3302 - mae: 2.3850 - val_loss: 14.3204 - val_mae: 2.4983\n",
      "Epoch 678/1000\n",
      "364/364 [==============================] - 0s 139us/step - loss: 11.1668 - mae: 2.3684 - val_loss: 14.3705 - val_mae: 2.5176\n",
      "Epoch 679/1000\n",
      "364/364 [==============================] - 0s 139us/step - loss: 11.2440 - mae: 2.3506 - val_loss: 16.2326 - val_mae: 2.8792\n",
      "Epoch 680/1000\n",
      "364/364 [==============================] - 0s 178us/step - loss: 11.3152 - mae: 2.3968 - val_loss: 14.9419 - val_mae: 2.5420\n",
      "Epoch 681/1000\n",
      "364/364 [==============================] - 0s 148us/step - loss: 11.0994 - mae: 2.3757 - val_loss: 15.9380 - val_mae: 2.8356\n",
      "Epoch 682/1000\n",
      "364/364 [==============================] - 0s 140us/step - loss: 11.0998 - mae: 2.3752 - val_loss: 14.2445 - val_mae: 2.4919\n",
      "Epoch 683/1000\n",
      "364/364 [==============================] - 0s 160us/step - loss: 11.3168 - mae: 2.3664 - val_loss: 14.2316 - val_mae: 2.4730\n",
      "Epoch 684/1000\n",
      "364/364 [==============================] - 0s 157us/step - loss: 11.1706 - mae: 2.3476 - val_loss: 14.3154 - val_mae: 2.5013\n",
      "Epoch 685/1000\n",
      "364/364 [==============================] - 0s 159us/step - loss: 11.1299 - mae: 2.3597 - val_loss: 14.3284 - val_mae: 2.4649\n",
      "Epoch 686/1000\n",
      "364/364 [==============================] - 0s 167us/step - loss: 11.3348 - mae: 2.3840 - val_loss: 14.2453 - val_mae: 2.4763\n",
      "Epoch 687/1000\n",
      "364/364 [==============================] - 0s 161us/step - loss: 11.1762 - mae: 2.3795 - val_loss: 14.4460 - val_mae: 2.4823\n",
      "Epoch 688/1000\n",
      "364/364 [==============================] - 0s 176us/step - loss: 10.9535 - mae: 2.3219 - val_loss: 15.7896 - val_mae: 2.6345\n",
      "Epoch 689/1000\n",
      "364/364 [==============================] - 0s 172us/step - loss: 11.4375 - mae: 2.3823 - val_loss: 14.7656 - val_mae: 2.5159\n",
      "Epoch 690/1000\n",
      "364/364 [==============================] - 0s 173us/step - loss: 11.3817 - mae: 2.3767 - val_loss: 15.1609 - val_mae: 2.6639\n",
      "Epoch 691/1000\n",
      "364/364 [==============================] - 0s 172us/step - loss: 11.2496 - mae: 2.3974 - val_loss: 14.4237 - val_mae: 2.4992\n",
      "Epoch 692/1000\n",
      "364/364 [==============================] - 0s 145us/step - loss: 11.0629 - mae: 2.3450 - val_loss: 14.3690 - val_mae: 2.4700\n",
      "Epoch 693/1000\n",
      "364/364 [==============================] - 0s 196us/step - loss: 11.1216 - mae: 2.3564 - val_loss: 14.4865 - val_mae: 2.5289\n",
      "Epoch 694/1000\n",
      "364/364 [==============================] - 0s 253us/step - loss: 11.0993 - mae: 2.3609 - val_loss: 14.9281 - val_mae: 2.6299\n",
      "Epoch 695/1000\n",
      "364/364 [==============================] - 0s 326us/step - loss: 11.0719 - mae: 2.3651 - val_loss: 14.3371 - val_mae: 2.4680\n",
      "Epoch 696/1000\n",
      "364/364 [==============================] - 0s 303us/step - loss: 11.2819 - mae: 2.3647 - val_loss: 15.3045 - val_mae: 2.7270\n",
      "Epoch 697/1000\n",
      "364/364 [==============================] - 0s 238us/step - loss: 11.0930 - mae: 2.3795 - val_loss: 15.1431 - val_mae: 2.5702\n",
      "Epoch 698/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 203us/step - loss: 11.3401 - mae: 2.3519 - val_loss: 14.6699 - val_mae: 2.5759\n",
      "Epoch 699/1000\n",
      "364/364 [==============================] - 0s 232us/step - loss: 11.0706 - mae: 2.3674 - val_loss: 14.0568 - val_mae: 2.4594\n",
      "Epoch 700/1000\n",
      "364/364 [==============================] - 0s 223us/step - loss: 11.0385 - mae: 2.3485 - val_loss: 14.3358 - val_mae: 2.4722\n",
      "Epoch 701/1000\n",
      "364/364 [==============================] - 0s 216us/step - loss: 10.9772 - mae: 2.3484 - val_loss: 14.0419 - val_mae: 2.4521\n",
      "Epoch 702/1000\n",
      "364/364 [==============================] - 0s 241us/step - loss: 11.0452 - mae: 2.3469 - val_loss: 15.7651 - val_mae: 2.8218\n",
      "Epoch 703/1000\n",
      "364/364 [==============================] - 0s 215us/step - loss: 11.3498 - mae: 2.3686 - val_loss: 14.1395 - val_mae: 2.4869\n",
      "Epoch 704/1000\n",
      "364/364 [==============================] - 0s 147us/step - loss: 11.0703 - mae: 2.3563 - val_loss: 14.3236 - val_mae: 2.4769\n",
      "Epoch 705/1000\n",
      "364/364 [==============================] - 0s 158us/step - loss: 10.9931 - mae: 2.3401 - val_loss: 15.4121 - val_mae: 2.7434\n",
      "Epoch 706/1000\n",
      "364/364 [==============================] - 0s 158us/step - loss: 10.9932 - mae: 2.3641 - val_loss: 14.4623 - val_mae: 2.5521\n",
      "Epoch 707/1000\n",
      "364/364 [==============================] - 0s 148us/step - loss: 11.4305 - mae: 2.4152 - val_loss: 14.1656 - val_mae: 2.4786\n",
      "Epoch 708/1000\n",
      "364/364 [==============================] - 0s 150us/step - loss: 11.0588 - mae: 2.3729 - val_loss: 14.3602 - val_mae: 2.5086\n",
      "Epoch 709/1000\n",
      "364/364 [==============================] - 0s 119us/step - loss: 10.9486 - mae: 2.3339 - val_loss: 15.5661 - val_mae: 2.7724\n",
      "Epoch 710/1000\n",
      "364/364 [==============================] - 0s 153us/step - loss: 11.0397 - mae: 2.3534 - val_loss: 14.4147 - val_mae: 2.5443\n",
      "Epoch 711/1000\n",
      "364/364 [==============================] - 0s 145us/step - loss: 10.9559 - mae: 2.3473 - val_loss: 14.3235 - val_mae: 2.4807\n",
      "Epoch 712/1000\n",
      "364/364 [==============================] - 0s 124us/step - loss: 11.2850 - mae: 2.3778 - val_loss: 14.5794 - val_mae: 2.5792\n",
      "Epoch 713/1000\n",
      "364/364 [==============================] - 0s 135us/step - loss: 10.9463 - mae: 2.3442 - val_loss: 14.2283 - val_mae: 2.5061\n",
      "Epoch 714/1000\n",
      "364/364 [==============================] - 0s 144us/step - loss: 10.8856 - mae: 2.3354 - val_loss: 17.0629 - val_mae: 3.0429\n",
      "Epoch 715/1000\n",
      "364/364 [==============================] - 0s 129us/step - loss: 11.2605 - mae: 2.3889 - val_loss: 14.1935 - val_mae: 2.4919\n",
      "Epoch 716/1000\n",
      "364/364 [==============================] - 0s 169us/step - loss: 10.8086 - mae: 2.3295 - val_loss: 14.4359 - val_mae: 2.5570\n",
      "Epoch 717/1000\n",
      "364/364 [==============================] - 0s 154us/step - loss: 10.9895 - mae: 2.3383 - val_loss: 16.5132 - val_mae: 2.9522\n",
      "Epoch 718/1000\n",
      "364/364 [==============================] - 0s 160us/step - loss: 11.1786 - mae: 2.3711 - val_loss: 14.0218 - val_mae: 2.4674\n",
      "Epoch 719/1000\n",
      "364/364 [==============================] - 0s 234us/step - loss: 11.0465 - mae: 2.3874 - val_loss: 14.3875 - val_mae: 2.5468\n",
      "Epoch 720/1000\n",
      "364/364 [==============================] - 0s 208us/step - loss: 10.9572 - mae: 2.3226 - val_loss: 14.3496 - val_mae: 2.4783\n",
      "Epoch 721/1000\n",
      "364/364 [==============================] - 0s 245us/step - loss: 11.0432 - mae: 2.3446 - val_loss: 14.0418 - val_mae: 2.4656\n",
      "Epoch 722/1000\n",
      "364/364 [==============================] - 0s 261us/step - loss: 10.9010 - mae: 2.3408 - val_loss: 14.1901 - val_mae: 2.4933\n",
      "Epoch 723/1000\n",
      "364/364 [==============================] - 0s 289us/step - loss: 10.9698 - mae: 2.3592 - val_loss: 14.3404 - val_mae: 2.5439\n",
      "Epoch 724/1000\n",
      "364/364 [==============================] - 0s 349us/step - loss: 11.1058 - mae: 2.3641 - val_loss: 14.3813 - val_mae: 2.5578\n",
      "Epoch 725/1000\n",
      "364/364 [==============================] - 0s 266us/step - loss: 11.1888 - mae: 2.3811 - val_loss: 14.3725 - val_mae: 2.5547\n",
      "Epoch 726/1000\n",
      "364/364 [==============================] - 0s 208us/step - loss: 10.8150 - mae: 2.3218 - val_loss: 14.5839 - val_mae: 2.5817\n",
      "Epoch 727/1000\n",
      "364/364 [==============================] - 0s 317us/step - loss: 10.9285 - mae: 2.3410 - val_loss: 15.0349 - val_mae: 2.5639\n",
      "Epoch 728/1000\n",
      "364/364 [==============================] - 0s 360us/step - loss: 11.2128 - mae: 2.3637 - val_loss: 14.3070 - val_mae: 2.5398\n",
      "Epoch 729/1000\n",
      "364/364 [==============================] - 0s 331us/step - loss: 10.9459 - mae: 2.3353 - val_loss: 14.3267 - val_mae: 2.5365\n",
      "Epoch 730/1000\n",
      "364/364 [==============================] - 0s 225us/step - loss: 10.8147 - mae: 2.3312 - val_loss: 14.4085 - val_mae: 2.5429\n",
      "Epoch 731/1000\n",
      "364/364 [==============================] - 0s 182us/step - loss: 10.8936 - mae: 2.3220 - val_loss: 15.9358 - val_mae: 2.8404\n",
      "Epoch 732/1000\n",
      "364/364 [==============================] - 0s 253us/step - loss: 10.9201 - mae: 2.3565 - val_loss: 14.4295 - val_mae: 2.5486\n",
      "Epoch 733/1000\n",
      "364/364 [==============================] - 0s 270us/step - loss: 10.8732 - mae: 2.3375 - val_loss: 14.2652 - val_mae: 2.5236\n",
      "Epoch 734/1000\n",
      "364/364 [==============================] - 0s 359us/step - loss: 10.8112 - mae: 2.3378 - val_loss: 14.0238 - val_mae: 2.4576\n",
      "Epoch 735/1000\n",
      "364/364 [==============================] - 0s 291us/step - loss: 11.0223 - mae: 2.3395 - val_loss: 14.4130 - val_mae: 2.4941\n",
      "Epoch 736/1000\n",
      "364/364 [==============================] - 0s 264us/step - loss: 10.8693 - mae: 2.3512 - val_loss: 14.5797 - val_mae: 2.5758\n",
      "Epoch 737/1000\n",
      "364/364 [==============================] - 0s 221us/step - loss: 10.9193 - mae: 2.3631 - val_loss: 14.4071 - val_mae: 2.5382\n",
      "Epoch 738/1000\n",
      "364/364 [==============================] - 0s 298us/step - loss: 10.7943 - mae: 2.3409 - val_loss: 14.0066 - val_mae: 2.4674\n",
      "Epoch 739/1000\n",
      "364/364 [==============================] - 0s 313us/step - loss: 10.9076 - mae: 2.3353 - val_loss: 14.0443 - val_mae: 2.4633\n",
      "Epoch 740/1000\n",
      "364/364 [==============================] - 0s 283us/step - loss: 10.8358 - mae: 2.3507 - val_loss: 14.0759 - val_mae: 2.4673\n",
      "Epoch 741/1000\n",
      "364/364 [==============================] - 0s 171us/step - loss: 11.0875 - mae: 2.3608 - val_loss: 14.0479 - val_mae: 2.4654\n",
      "Epoch 742/1000\n",
      "364/364 [==============================] - 0s 250us/step - loss: 10.7849 - mae: 2.3169 - val_loss: 14.1140 - val_mae: 2.4903\n",
      "Epoch 743/1000\n",
      "364/364 [==============================] - 0s 249us/step - loss: 10.7205 - mae: 2.3417 - val_loss: 14.0646 - val_mae: 2.4644\n",
      "Epoch 744/1000\n",
      "364/364 [==============================] - 0s 285us/step - loss: 11.0518 - mae: 2.3535 - val_loss: 14.0564 - val_mae: 2.4559\n",
      "Epoch 745/1000\n",
      "364/364 [==============================] - 0s 334us/step - loss: 10.8226 - mae: 2.3203 - val_loss: 14.0669 - val_mae: 2.4882\n",
      "Epoch 746/1000\n",
      "364/364 [==============================] - 0s 379us/step - loss: 10.9527 - mae: 2.3590 - val_loss: 14.1180 - val_mae: 2.4560\n",
      "Epoch 747/1000\n",
      "364/364 [==============================] - 0s 325us/step - loss: 10.8218 - mae: 2.3428 - val_loss: 14.2393 - val_mae: 2.5504\n",
      "Epoch 748/1000\n",
      "364/364 [==============================] - 0s 507us/step - loss: 10.7667 - mae: 2.3044 - val_loss: 13.8716 - val_mae: 2.4525\n",
      "Epoch 749/1000\n",
      "364/364 [==============================] - 0s 386us/step - loss: 10.8781 - mae: 2.3647 - val_loss: 14.0220 - val_mae: 2.4642\n",
      "Epoch 750/1000\n",
      "364/364 [==============================] - 0s 282us/step - loss: 10.7572 - mae: 2.3265 - val_loss: 14.0053 - val_mae: 2.4553\n",
      "Epoch 751/1000\n",
      "364/364 [==============================] - 0s 199us/step - loss: 10.8368 - mae: 2.3402 - val_loss: 13.9064 - val_mae: 2.4636\n",
      "Epoch 752/1000\n",
      "364/364 [==============================] - 0s 226us/step - loss: 10.8530 - mae: 2.3457 - val_loss: 13.9414 - val_mae: 2.4619\n",
      "Epoch 753/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 11.0379 - mae: 2.3592 - val_loss: 14.0568 - val_mae: 2.4717\n",
      "Epoch 754/1000\n",
      "364/364 [==============================] - 0s 272us/step - loss: 10.6381 - mae: 2.3063 - val_loss: 13.9679 - val_mae: 2.4724\n",
      "Epoch 755/1000\n",
      "364/364 [==============================] - 0s 242us/step - loss: 10.8770 - mae: 2.3451 - val_loss: 14.1159 - val_mae: 2.4947\n",
      "Epoch 756/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 248us/step - loss: 10.7725 - mae: 2.3234 - val_loss: 14.7665 - val_mae: 2.6437\n",
      "Epoch 757/1000\n",
      "364/364 [==============================] - 0s 223us/step - loss: 10.7477 - mae: 2.3386 - val_loss: 14.1617 - val_mae: 2.5307\n",
      "Epoch 758/1000\n",
      "364/364 [==============================] - 0s 396us/step - loss: 10.6413 - mae: 2.2991 - val_loss: 14.0288 - val_mae: 2.4822\n",
      "Epoch 759/1000\n",
      "364/364 [==============================] - 0s 338us/step - loss: 10.6724 - mae: 2.3382 - val_loss: 14.1678 - val_mae: 2.4788\n",
      "Epoch 760/1000\n",
      "364/364 [==============================] - 0s 340us/step - loss: 10.7433 - mae: 2.3344 - val_loss: 14.0137 - val_mae: 2.5054\n",
      "Epoch 761/1000\n",
      "364/364 [==============================] - 0s 434us/step - loss: 10.7007 - mae: 2.3152 - val_loss: 14.4829 - val_mae: 2.5909\n",
      "Epoch 762/1000\n",
      "364/364 [==============================] - 0s 379us/step - loss: 10.6687 - mae: 2.3241 - val_loss: 15.8704 - val_mae: 2.8623\n",
      "Epoch 763/1000\n",
      "364/364 [==============================] - 0s 344us/step - loss: 10.9650 - mae: 2.3855 - val_loss: 13.9723 - val_mae: 2.4615\n",
      "Epoch 764/1000\n",
      "364/364 [==============================] - 0s 256us/step - loss: 10.6159 - mae: 2.3029 - val_loss: 14.1032 - val_mae: 2.5487\n",
      "Epoch 765/1000\n",
      "364/364 [==============================] - 0s 232us/step - loss: 10.6412 - mae: 2.3199 - val_loss: 13.8938 - val_mae: 2.4825\n",
      "Epoch 766/1000\n",
      "364/364 [==============================] - 0s 271us/step - loss: 10.7956 - mae: 2.3113 - val_loss: 14.1318 - val_mae: 2.4664\n",
      "Epoch 767/1000\n",
      "364/364 [==============================] - 0s 324us/step - loss: 10.7490 - mae: 2.3449 - val_loss: 13.8864 - val_mae: 2.4765\n",
      "Epoch 768/1000\n",
      "364/364 [==============================] - 0s 151us/step - loss: 10.8229 - mae: 2.3503 - val_loss: 14.1451 - val_mae: 2.5387\n",
      "Epoch 769/1000\n",
      "364/364 [==============================] - 0s 299us/step - loss: 10.6891 - mae: 2.3281 - val_loss: 14.1125 - val_mae: 2.4693\n",
      "Epoch 770/1000\n",
      "364/364 [==============================] - 0s 424us/step - loss: 10.5924 - mae: 2.3043 - val_loss: 14.1581 - val_mae: 2.5364\n",
      "Epoch 771/1000\n",
      "364/364 [==============================] - 0s 431us/step - loss: 10.6985 - mae: 2.3282 - val_loss: 13.9102 - val_mae: 2.4604\n",
      "Epoch 772/1000\n",
      "364/364 [==============================] - 0s 303us/step - loss: 10.7169 - mae: 2.3072 - val_loss: 13.8626 - val_mae: 2.4607\n",
      "Epoch 773/1000\n",
      "364/364 [==============================] - 0s 305us/step - loss: 10.9406 - mae: 2.3242 - val_loss: 14.6409 - val_mae: 2.6270\n",
      "Epoch 774/1000\n",
      "364/364 [==============================] - 0s 354us/step - loss: 10.7185 - mae: 2.3313 - val_loss: 14.3756 - val_mae: 2.5863\n",
      "Epoch 775/1000\n",
      "364/364 [==============================] - 0s 285us/step - loss: 10.5812 - mae: 2.2947 - val_loss: 13.9604 - val_mae: 2.4866\n",
      "Epoch 776/1000\n",
      "364/364 [==============================] - 0s 302us/step - loss: 10.5938 - mae: 2.3049 - val_loss: 14.0814 - val_mae: 2.5210\n",
      "Epoch 777/1000\n",
      "364/364 [==============================] - 0s 355us/step - loss: 10.5673 - mae: 2.3169 - val_loss: 13.9130 - val_mae: 2.4790\n",
      "Epoch 778/1000\n",
      "364/364 [==============================] - 0s 365us/step - loss: 10.8062 - mae: 2.3352 - val_loss: 14.0542 - val_mae: 2.5008\n",
      "Epoch 779/1000\n",
      "364/364 [==============================] - 0s 301us/step - loss: 10.4524 - mae: 2.3246 - val_loss: 14.2111 - val_mae: 2.5282\n",
      "Epoch 780/1000\n",
      "364/364 [==============================] - 0s 363us/step - loss: 10.5166 - mae: 2.2955 - val_loss: 14.2742 - val_mae: 2.5640\n",
      "Epoch 781/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 10.6821 - mae: 2.3107 - val_loss: 14.6737 - val_mae: 2.6388\n",
      "Epoch 782/1000\n",
      "364/364 [==============================] - 0s 354us/step - loss: 10.5260 - mae: 2.2980 - val_loss: 13.9748 - val_mae: 2.4806\n",
      "Epoch 783/1000\n",
      "364/364 [==============================] - 0s 387us/step - loss: 10.3836 - mae: 2.3020 - val_loss: 13.8455 - val_mae: 2.4797\n",
      "Epoch 784/1000\n",
      "364/364 [==============================] - 0s 283us/step - loss: 10.6894 - mae: 2.3318 - val_loss: 14.0218 - val_mae: 2.4754\n",
      "Epoch 785/1000\n",
      "364/364 [==============================] - 0s 337us/step - loss: 10.6685 - mae: 2.3329 - val_loss: 13.9618 - val_mae: 2.5111\n",
      "Epoch 786/1000\n",
      "364/364 [==============================] - 0s 285us/step - loss: 10.5512 - mae: 2.2967 - val_loss: 14.0895 - val_mae: 2.4765\n",
      "Epoch 787/1000\n",
      "364/364 [==============================] - 0s 359us/step - loss: 10.7907 - mae: 2.3129 - val_loss: 13.8330 - val_mae: 2.4674\n",
      "Epoch 788/1000\n",
      "364/364 [==============================] - 0s 383us/step - loss: 10.5978 - mae: 2.3068 - val_loss: 13.8867 - val_mae: 2.4821\n",
      "Epoch 789/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 10.5327 - mae: 2.2863 - val_loss: 13.7861 - val_mae: 2.4726\n",
      "Epoch 790/1000\n",
      "364/364 [==============================] - 0s 237us/step - loss: 10.7605 - mae: 2.3192 - val_loss: 13.9475 - val_mae: 2.4821\n",
      "Epoch 791/1000\n",
      "364/364 [==============================] - 0s 425us/step - loss: 10.5008 - mae: 2.2900 - val_loss: 14.2445 - val_mae: 2.5707\n",
      "Epoch 792/1000\n",
      "364/364 [==============================] - 0s 294us/step - loss: 10.5694 - mae: 2.2944 - val_loss: 14.0343 - val_mae: 2.5176\n",
      "Epoch 793/1000\n",
      "364/364 [==============================] - 0s 449us/step - loss: 10.6339 - mae: 2.3199 - val_loss: 14.0222 - val_mae: 2.5217\n",
      "Epoch 794/1000\n",
      "364/364 [==============================] - 0s 221us/step - loss: 10.6995 - mae: 2.3117 - val_loss: 14.4968 - val_mae: 2.6065\n",
      "Epoch 795/1000\n",
      "364/364 [==============================] - 0s 209us/step - loss: 10.5038 - mae: 2.2819 - val_loss: 13.9237 - val_mae: 2.4972\n",
      "Epoch 796/1000\n",
      "364/364 [==============================] - 0s 427us/step - loss: 10.3735 - mae: 2.2668 - val_loss: 14.3089 - val_mae: 2.5702\n",
      "Epoch 797/1000\n",
      "364/364 [==============================] - 0s 370us/step - loss: 10.5454 - mae: 2.3089 - val_loss: 13.8489 - val_mae: 2.4607\n",
      "Epoch 798/1000\n",
      "364/364 [==============================] - 0s 404us/step - loss: 10.4478 - mae: 2.2944 - val_loss: 14.0089 - val_mae: 2.5333\n",
      "Epoch 799/1000\n",
      "364/364 [==============================] - 0s 306us/step - loss: 10.5301 - mae: 2.3096 - val_loss: 14.1813 - val_mae: 2.5483\n",
      "Epoch 800/1000\n",
      "364/364 [==============================] - 0s 308us/step - loss: 10.4824 - mae: 2.3109 - val_loss: 14.1021 - val_mae: 2.5154\n",
      "Epoch 801/1000\n",
      "364/364 [==============================] - 0s 346us/step - loss: 10.3996 - mae: 2.2808 - val_loss: 15.3441 - val_mae: 2.7600\n",
      "Epoch 802/1000\n",
      "364/364 [==============================] - 0s 342us/step - loss: 10.5181 - mae: 2.3330 - val_loss: 14.5629 - val_mae: 2.6176\n",
      "Epoch 803/1000\n",
      "364/364 [==============================] - 0s 521us/step - loss: 10.4480 - mae: 2.2898 - val_loss: 14.1094 - val_mae: 2.5013\n",
      "Epoch 804/1000\n",
      "364/364 [==============================] - 0s 330us/step - loss: 10.4081 - mae: 2.2759 - val_loss: 13.9701 - val_mae: 2.4801\n",
      "Epoch 805/1000\n",
      "364/364 [==============================] - 0s 366us/step - loss: 10.4056 - mae: 2.2875 - val_loss: 14.4389 - val_mae: 2.5952\n",
      "Epoch 806/1000\n",
      "364/364 [==============================] - 0s 454us/step - loss: 10.6214 - mae: 2.3104 - val_loss: 13.9622 - val_mae: 2.4764\n",
      "Epoch 807/1000\n",
      "364/364 [==============================] - 0s 294us/step - loss: 10.4102 - mae: 2.2905 - val_loss: 15.0696 - val_mae: 2.7147\n",
      "Epoch 808/1000\n",
      "364/364 [==============================] - 0s 283us/step - loss: 10.3472 - mae: 2.2856 - val_loss: 13.9050 - val_mae: 2.4691\n",
      "Epoch 809/1000\n",
      "364/364 [==============================] - 0s 434us/step - loss: 10.5467 - mae: 2.3034 - val_loss: 14.2515 - val_mae: 2.4964\n",
      "Epoch 810/1000\n",
      "364/364 [==============================] - 0s 355us/step - loss: 10.5776 - mae: 2.2927 - val_loss: 14.0554 - val_mae: 2.5111\n",
      "Epoch 811/1000\n",
      "364/364 [==============================] - 0s 372us/step - loss: 10.3780 - mae: 2.2762 - val_loss: 14.1191 - val_mae: 2.4897\n",
      "Epoch 812/1000\n",
      "364/364 [==============================] - 0s 329us/step - loss: 10.3603 - mae: 2.2852 - val_loss: 13.9316 - val_mae: 2.4652\n",
      "Epoch 813/1000\n",
      "364/364 [==============================] - 0s 324us/step - loss: 10.3446 - mae: 2.2899 - val_loss: 14.3607 - val_mae: 2.5072\n",
      "Epoch 814/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 309us/step - loss: 10.6863 - mae: 2.3042 - val_loss: 13.9114 - val_mae: 2.4945\n",
      "Epoch 815/1000\n",
      "364/364 [==============================] - 0s 281us/step - loss: 10.4912 - mae: 2.2980 - val_loss: 13.7616 - val_mae: 2.4655\n",
      "Epoch 816/1000\n",
      "364/364 [==============================] - 0s 251us/step - loss: 10.4067 - mae: 2.3106 - val_loss: 14.3393 - val_mae: 2.6079\n",
      "Epoch 817/1000\n",
      "364/364 [==============================] - 0s 154us/step - loss: 10.4602 - mae: 2.3022 - val_loss: 13.8350 - val_mae: 2.4976\n",
      "Epoch 818/1000\n",
      "364/364 [==============================] - 0s 158us/step - loss: 10.2849 - mae: 2.2719 - val_loss: 14.0003 - val_mae: 2.5162\n",
      "Epoch 819/1000\n",
      "364/364 [==============================] - 0s 392us/step - loss: 10.4855 - mae: 2.3069 - val_loss: 14.9864 - val_mae: 2.7113\n",
      "Epoch 820/1000\n",
      "364/364 [==============================] - 0s 316us/step - loss: 10.5511 - mae: 2.3034 - val_loss: 13.7717 - val_mae: 2.4696\n",
      "Epoch 821/1000\n",
      "364/364 [==============================] - 0s 166us/step - loss: 10.4196 - mae: 2.2755 - val_loss: 15.2580 - val_mae: 2.7670\n",
      "Epoch 822/1000\n",
      "364/364 [==============================] - 0s 309us/step - loss: 10.2688 - mae: 2.2963 - val_loss: 14.1961 - val_mae: 2.5619\n",
      "Epoch 823/1000\n",
      "364/364 [==============================] - 0s 378us/step - loss: 10.3303 - mae: 2.2782 - val_loss: 14.0752 - val_mae: 2.5383\n",
      "Epoch 824/1000\n",
      "364/364 [==============================] - 0s 304us/step - loss: 10.3566 - mae: 2.2808 - val_loss: 13.9368 - val_mae: 2.4842\n",
      "Epoch 825/1000\n",
      "364/364 [==============================] - 0s 447us/step - loss: 10.2409 - mae: 2.2623 - val_loss: 14.0534 - val_mae: 2.5335\n",
      "Epoch 826/1000\n",
      "364/364 [==============================] - 0s 331us/step - loss: 10.5388 - mae: 2.3119 - val_loss: 14.1369 - val_mae: 2.5581\n",
      "Epoch 827/1000\n",
      "364/364 [==============================] - 0s 480us/step - loss: 10.4635 - mae: 2.2858 - val_loss: 14.0320 - val_mae: 2.5267\n",
      "Epoch 828/1000\n",
      "364/364 [==============================] - 0s 356us/step - loss: 10.3126 - mae: 2.2821 - val_loss: 13.6790 - val_mae: 2.4468\n",
      "Epoch 829/1000\n",
      "364/364 [==============================] - 0s 270us/step - loss: 10.3273 - mae: 2.2660 - val_loss: 13.6669 - val_mae: 2.4660\n",
      "Epoch 830/1000\n",
      "364/364 [==============================] - 0s 222us/step - loss: 10.4865 - mae: 2.3005 - val_loss: 14.3968 - val_mae: 2.6111\n",
      "Epoch 831/1000\n",
      "364/364 [==============================] - 0s 334us/step - loss: 10.2891 - mae: 2.2863 - val_loss: 13.7852 - val_mae: 2.4873\n",
      "Epoch 832/1000\n",
      "364/364 [==============================] - 0s 165us/step - loss: 10.4252 - mae: 2.2948 - val_loss: 13.8442 - val_mae: 2.5129\n",
      "Epoch 833/1000\n",
      "364/364 [==============================] - 0s 447us/step - loss: 10.2869 - mae: 2.2659 - val_loss: 13.8977 - val_mae: 2.5220\n",
      "Epoch 834/1000\n",
      "364/364 [==============================] - 0s 353us/step - loss: 10.2943 - mae: 2.2833 - val_loss: 14.7384 - val_mae: 2.6564\n",
      "Epoch 835/1000\n",
      "364/364 [==============================] - 0s 345us/step - loss: 10.2438 - mae: 2.2831 - val_loss: 13.8463 - val_mae: 2.4578\n",
      "Epoch 836/1000\n",
      "364/364 [==============================] - 0s 334us/step - loss: 10.4029 - mae: 2.2947 - val_loss: 15.1012 - val_mae: 2.7584\n",
      "Epoch 837/1000\n",
      "364/364 [==============================] - 0s 364us/step - loss: 10.4508 - mae: 2.3022 - val_loss: 14.7672 - val_mae: 2.6925\n",
      "Epoch 838/1000\n",
      "364/364 [==============================] - 0s 326us/step - loss: 10.3063 - mae: 2.2714 - val_loss: 14.1329 - val_mae: 2.5533\n",
      "Epoch 839/1000\n",
      "364/364 [==============================] - 0s 459us/step - loss: 10.2985 - mae: 2.2840 - val_loss: 14.0109 - val_mae: 2.5385\n",
      "Epoch 840/1000\n",
      "364/364 [==============================] - 0s 464us/step - loss: 10.1843 - mae: 2.2843 - val_loss: 14.3264 - val_mae: 2.5920\n",
      "Epoch 841/1000\n",
      "364/364 [==============================] - 0s 397us/step - loss: 10.1532 - mae: 2.2942 - val_loss: 14.5583 - val_mae: 2.6266\n",
      "Epoch 842/1000\n",
      "364/364 [==============================] - 0s 483us/step - loss: 10.3346 - mae: 2.3123 - val_loss: 13.8553 - val_mae: 2.4993\n",
      "Epoch 843/1000\n",
      "364/364 [==============================] - 0s 490us/step - loss: 10.2275 - mae: 2.2495 - val_loss: 13.7712 - val_mae: 2.4778\n",
      "Epoch 844/1000\n",
      "364/364 [==============================] - 0s 288us/step - loss: 10.3976 - mae: 2.2813 - val_loss: 13.8850 - val_mae: 2.4975\n",
      "Epoch 845/1000\n",
      "364/364 [==============================] - 0s 290us/step - loss: 10.2814 - mae: 2.2809 - val_loss: 14.0883 - val_mae: 2.5530\n",
      "Epoch 846/1000\n",
      "364/364 [==============================] - ETA: 0s - loss: 10.3243 - mae: 2.24 - 0s 240us/step - loss: 10.1254 - mae: 2.2567 - val_loss: 14.5127 - val_mae: 2.6300\n",
      "Epoch 847/1000\n",
      "364/364 [==============================] - 0s 321us/step - loss: 10.2784 - mae: 2.2712 - val_loss: 14.4325 - val_mae: 2.5299\n",
      "Epoch 848/1000\n",
      "364/364 [==============================] - 0s 343us/step - loss: 10.2987 - mae: 2.3093 - val_loss: 14.3611 - val_mae: 2.6048\n",
      "Epoch 849/1000\n",
      "364/364 [==============================] - 0s 355us/step - loss: 10.2769 - mae: 2.2834 - val_loss: 13.9044 - val_mae: 2.5205\n",
      "Epoch 850/1000\n",
      "364/364 [==============================] - 0s 336us/step - loss: 9.9974 - mae: 2.2490 - val_loss: 14.4953 - val_mae: 2.6494\n",
      "Epoch 851/1000\n",
      "364/364 [==============================] - 0s 396us/step - loss: 10.2583 - mae: 2.2854 - val_loss: 13.9751 - val_mae: 2.4752\n",
      "Epoch 852/1000\n",
      "364/364 [==============================] - 0s 343us/step - loss: 10.3830 - mae: 2.2903 - val_loss: 13.9619 - val_mae: 2.5431\n",
      "Epoch 853/1000\n",
      "364/364 [==============================] - 0s 496us/step - loss: 10.0625 - mae: 2.2583 - val_loss: 13.7751 - val_mae: 2.4915\n",
      "Epoch 854/1000\n",
      "364/364 [==============================] - 0s 283us/step - loss: 10.2580 - mae: 2.2537 - val_loss: 16.0462 - val_mae: 2.9097\n",
      "Epoch 855/1000\n",
      "364/364 [==============================] - 0s 290us/step - loss: 10.3156 - mae: 2.2857 - val_loss: 14.2683 - val_mae: 2.5738\n",
      "Epoch 856/1000\n",
      "364/364 [==============================] - 0s 206us/step - loss: 10.2521 - mae: 2.2853 - val_loss: 14.0870 - val_mae: 2.4999\n",
      "Epoch 857/1000\n",
      "364/364 [==============================] - 0s 392us/step - loss: 10.1841 - mae: 2.2702 - val_loss: 13.8163 - val_mae: 2.4911\n",
      "Epoch 858/1000\n",
      "364/364 [==============================] - 0s 239us/step - loss: 9.9934 - mae: 2.2519 - val_loss: 14.1344 - val_mae: 2.4970\n",
      "Epoch 859/1000\n",
      "364/364 [==============================] - 0s 321us/step - loss: 10.4123 - mae: 2.3015 - val_loss: 13.5939 - val_mae: 2.4531\n",
      "Epoch 860/1000\n",
      "364/364 [==============================] - 0s 356us/step - loss: 10.1846 - mae: 2.2454 - val_loss: 14.1134 - val_mae: 2.5897\n",
      "Epoch 861/1000\n",
      "364/364 [==============================] - 0s 268us/step - loss: 10.2251 - mae: 2.2611 - val_loss: 13.7003 - val_mae: 2.5004\n",
      "Epoch 862/1000\n",
      "364/364 [==============================] - 0s 419us/step - loss: 10.2078 - mae: 2.2632 - val_loss: 14.1356 - val_mae: 2.5871\n",
      "Epoch 863/1000\n",
      "364/364 [==============================] - 0s 328us/step - loss: 10.4558 - mae: 2.2873 - val_loss: 14.5215 - val_mae: 2.6440\n",
      "Epoch 864/1000\n",
      "364/364 [==============================] - 0s 266us/step - loss: 10.1543 - mae: 2.2595 - val_loss: 13.6617 - val_mae: 2.4593\n",
      "Epoch 865/1000\n",
      "364/364 [==============================] - 0s 352us/step - loss: 10.1523 - mae: 2.2559 - val_loss: 13.6265 - val_mae: 2.5003\n",
      "Epoch 866/1000\n",
      "364/364 [==============================] - 0s 341us/step - loss: 10.0860 - mae: 2.2602 - val_loss: 13.8539 - val_mae: 2.5411\n",
      "Epoch 867/1000\n",
      "364/364 [==============================] - 0s 333us/step - loss: 10.1660 - mae: 2.2726 - val_loss: 13.6360 - val_mae: 2.4652\n",
      "Epoch 868/1000\n",
      "364/364 [==============================] - 0s 262us/step - loss: 10.1618 - mae: 2.2646 - val_loss: 13.6810 - val_mae: 2.4869\n",
      "Epoch 869/1000\n",
      "364/364 [==============================] - 0s 259us/step - loss: 10.1544 - mae: 2.2545 - val_loss: 13.8548 - val_mae: 2.5355\n",
      "Epoch 870/1000\n",
      "364/364 [==============================] - 0s 445us/step - loss: 10.1148 - mae: 2.2743 - val_loss: 13.5954 - val_mae: 2.4579\n",
      "Epoch 871/1000\n",
      "364/364 [==============================] - 0s 355us/step - loss: 9.9982 - mae: 2.2361 - val_loss: 13.6162 - val_mae: 2.4829\n",
      "Epoch 872/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 414us/step - loss: 10.0787 - mae: 2.2412 - val_loss: 13.5875 - val_mae: 2.4692\n",
      "Epoch 873/1000\n",
      "364/364 [==============================] - 0s 436us/step - loss: 10.3133 - mae: 2.2718 - val_loss: 13.8606 - val_mae: 2.5242\n",
      "Epoch 874/1000\n",
      "364/364 [==============================] - 0s 160us/step - loss: 10.1597 - mae: 2.2866 - val_loss: 13.7245 - val_mae: 2.4709\n",
      "Epoch 875/1000\n",
      "364/364 [==============================] - 0s 324us/step - loss: 10.0412 - mae: 2.2474 - val_loss: 14.1806 - val_mae: 2.6000\n",
      "Epoch 876/1000\n",
      "364/364 [==============================] - 0s 300us/step - loss: 10.0697 - mae: 2.2453 - val_loss: 13.7321 - val_mae: 2.4607\n",
      "Epoch 877/1000\n",
      "364/364 [==============================] - 0s 306us/step - loss: 10.2533 - mae: 2.2735 - val_loss: 13.6570 - val_mae: 2.4808\n",
      "Epoch 878/1000\n",
      "364/364 [==============================] - 0s 209us/step - loss: 9.9192 - mae: 2.2445 - val_loss: 13.9100 - val_mae: 2.5398\n",
      "Epoch 879/1000\n",
      "364/364 [==============================] - 0s 261us/step - loss: 10.1799 - mae: 2.2731 - val_loss: 14.0212 - val_mae: 2.5826\n",
      "Epoch 880/1000\n",
      "364/364 [==============================] - 0s 208us/step - loss: 9.9532 - mae: 2.2597 - val_loss: 13.7305 - val_mae: 2.5240\n",
      "Epoch 881/1000\n",
      "364/364 [==============================] - 0s 181us/step - loss: 10.0428 - mae: 2.2591 - val_loss: 13.6555 - val_mae: 2.4657\n",
      "Epoch 882/1000\n",
      "364/364 [==============================] - 0s 170us/step - loss: 10.3020 - mae: 2.2627 - val_loss: 14.5135 - val_mae: 2.6478\n",
      "Epoch 883/1000\n",
      "364/364 [==============================] - 0s 209us/step - loss: 10.0763 - mae: 2.2646 - val_loss: 13.6357 - val_mae: 2.4740\n",
      "Epoch 884/1000\n",
      "364/364 [==============================] - 0s 233us/step - loss: 9.9482 - mae: 2.2467 - val_loss: 13.7895 - val_mae: 2.4870\n",
      "Epoch 885/1000\n",
      "364/364 [==============================] - 0s 228us/step - loss: 9.9306 - mae: 2.2254 - val_loss: 14.0734 - val_mae: 2.5740\n",
      "Epoch 886/1000\n",
      "364/364 [==============================] - 0s 346us/step - loss: 10.1267 - mae: 2.2492 - val_loss: 13.5324 - val_mae: 2.4458\n",
      "Epoch 887/1000\n",
      "364/364 [==============================] - 0s 443us/step - loss: 10.1310 - mae: 2.2314 - val_loss: 13.5644 - val_mae: 2.4465\n",
      "Epoch 888/1000\n",
      "364/364 [==============================] - 0s 354us/step - loss: 9.8156 - mae: 2.2375 - val_loss: 14.9809 - val_mae: 2.5794\n",
      "Epoch 889/1000\n",
      "364/364 [==============================] - 0s 282us/step - loss: 10.2733 - mae: 2.3069 - val_loss: 13.4010 - val_mae: 2.4384\n",
      "Epoch 890/1000\n",
      "364/364 [==============================] - 0s 228us/step - loss: 10.1704 - mae: 2.2607 - val_loss: 13.5032 - val_mae: 2.4531\n",
      "Epoch 891/1000\n",
      "364/364 [==============================] - 0s 348us/step - loss: 9.9080 - mae: 2.2297 - val_loss: 13.5423 - val_mae: 2.4434\n",
      "Epoch 892/1000\n",
      "364/364 [==============================] - 0s 165us/step - loss: 10.0266 - mae: 2.2560 - val_loss: 14.5891 - val_mae: 2.7022\n",
      "Epoch 893/1000\n",
      "364/364 [==============================] - 0s 385us/step - loss: 10.1072 - mae: 2.2689 - val_loss: 13.8834 - val_mae: 2.5542\n",
      "Epoch 894/1000\n",
      "364/364 [==============================] - 0s 443us/step - loss: 9.8247 - mae: 2.2143 - val_loss: 13.9476 - val_mae: 2.4920\n",
      "Epoch 895/1000\n",
      "364/364 [==============================] - 0s 257us/step - loss: 10.1266 - mae: 2.2621 - val_loss: 13.4897 - val_mae: 2.4657\n",
      "Epoch 896/1000\n",
      "364/364 [==============================] - 0s 319us/step - loss: 10.0665 - mae: 2.2686 - val_loss: 13.6369 - val_mae: 2.5056\n",
      "Epoch 897/1000\n",
      "364/364 [==============================] - 0s 234us/step - loss: 9.8862 - mae: 2.2355 - val_loss: 13.4257 - val_mae: 2.4645\n",
      "Epoch 898/1000\n",
      "364/364 [==============================] - 0s 479us/step - loss: 10.0407 - mae: 2.2511 - val_loss: 13.4029 - val_mae: 2.4566\n",
      "Epoch 899/1000\n",
      "364/364 [==============================] - 0s 442us/step - loss: 10.0992 - mae: 2.2594 - val_loss: 13.6155 - val_mae: 2.5045\n",
      "Epoch 900/1000\n",
      "364/364 [==============================] - 0s 264us/step - loss: 9.9563 - mae: 2.2358 - val_loss: 13.5487 - val_mae: 2.4637\n",
      "Epoch 901/1000\n",
      "364/364 [==============================] - 0s 302us/step - loss: 9.8941 - mae: 2.2209 - val_loss: 14.9298 - val_mae: 2.7321\n",
      "Epoch 902/1000\n",
      "364/364 [==============================] - 0s 580us/step - loss: 9.8538 - mae: 2.2638 - val_loss: 14.0601 - val_mae: 2.5596\n",
      "Epoch 903/1000\n",
      "364/364 [==============================] - 0s 266us/step - loss: 9.9208 - mae: 2.2467 - val_loss: 14.6454 - val_mae: 2.6901\n",
      "Epoch 904/1000\n",
      "364/364 [==============================] - 0s 403us/step - loss: 9.9598 - mae: 2.2727 - val_loss: 13.5561 - val_mae: 2.4698\n",
      "Epoch 905/1000\n",
      "364/364 [==============================] - 0s 251us/step - loss: 10.0256 - mae: 2.2407 - val_loss: 13.4433 - val_mae: 2.4550\n",
      "Epoch 906/1000\n",
      "364/364 [==============================] - 0s 340us/step - loss: 10.0189 - mae: 2.2515 - val_loss: 13.6251 - val_mae: 2.4910\n",
      "Epoch 907/1000\n",
      "364/364 [==============================] - 0s 394us/step - loss: 10.0866 - mae: 2.2682 - val_loss: 13.6305 - val_mae: 2.5080\n",
      "Epoch 908/1000\n",
      "364/364 [==============================] - 0s 304us/step - loss: 9.7330 - mae: 2.2297 - val_loss: 13.6299 - val_mae: 2.4631\n",
      "Epoch 909/1000\n",
      "364/364 [==============================] - 0s 310us/step - loss: 9.8486 - mae: 2.2315 - val_loss: 14.2982 - val_mae: 2.6288\n",
      "Epoch 910/1000\n",
      "364/364 [==============================] - 0s 423us/step - loss: 10.0761 - mae: 2.2796 - val_loss: 13.5966 - val_mae: 2.4961\n",
      "Epoch 911/1000\n",
      "364/364 [==============================] - 0s 293us/step - loss: 9.7972 - mae: 2.2549 - val_loss: 13.5560 - val_mae: 2.4574\n",
      "Epoch 912/1000\n",
      "364/364 [==============================] - 0s 313us/step - loss: 9.8097 - mae: 2.2362 - val_loss: 13.5894 - val_mae: 2.4627\n",
      "Epoch 913/1000\n",
      "364/364 [==============================] - 0s 443us/step - loss: 9.7974 - mae: 2.2175 - val_loss: 14.0420 - val_mae: 2.5610\n",
      "Epoch 914/1000\n",
      "364/364 [==============================] - 0s 302us/step - loss: 9.8905 - mae: 2.2532 - val_loss: 13.5401 - val_mae: 2.4647\n",
      "Epoch 915/1000\n",
      "364/364 [==============================] - 0s 344us/step - loss: 9.9581 - mae: 2.2370 - val_loss: 13.8917 - val_mae: 2.5629\n",
      "Epoch 916/1000\n",
      "364/364 [==============================] - 0s 317us/step - loss: 9.7004 - mae: 2.2498 - val_loss: 13.5767 - val_mae: 2.4849\n",
      "Epoch 917/1000\n",
      "364/364 [==============================] - 0s 198us/step - loss: 9.7561 - mae: 2.2096 - val_loss: 13.6590 - val_mae: 2.5096\n",
      "Epoch 918/1000\n",
      "364/364 [==============================] - 0s 302us/step - loss: 9.9566 - mae: 2.2677 - val_loss: 13.7095 - val_mae: 2.4742\n",
      "Epoch 919/1000\n",
      "364/364 [==============================] - 0s 365us/step - loss: 9.9717 - mae: 2.2281 - val_loss: 13.3680 - val_mae: 2.4476\n",
      "Epoch 920/1000\n",
      "364/364 [==============================] - 0s 341us/step - loss: 9.7858 - mae: 2.2088 - val_loss: 13.3338 - val_mae: 2.4350\n",
      "Epoch 921/1000\n",
      "364/364 [==============================] - 0s 305us/step - loss: 9.9749 - mae: 2.2479 - val_loss: 13.2370 - val_mae: 2.4394\n",
      "Epoch 922/1000\n",
      "364/364 [==============================] - 0s 319us/step - loss: 9.7675 - mae: 2.2067 - val_loss: 13.4730 - val_mae: 2.4776\n",
      "Epoch 923/1000\n",
      "364/364 [==============================] - 0s 263us/step - loss: 9.8317 - mae: 2.2348 - val_loss: 13.3975 - val_mae: 2.4734\n",
      "Epoch 924/1000\n",
      "364/364 [==============================] - 0s 316us/step - loss: 9.9170 - mae: 2.2400 - val_loss: 13.3424 - val_mae: 2.4445\n",
      "Epoch 925/1000\n",
      "364/364 [==============================] - 0s 346us/step - loss: 9.7598 - mae: 2.2137 - val_loss: 13.4897 - val_mae: 2.4571\n",
      "Epoch 926/1000\n",
      "364/364 [==============================] - 0s 219us/step - loss: 9.7838 - mae: 2.2121 - val_loss: 13.3654 - val_mae: 2.4452\n",
      "Epoch 927/1000\n",
      "364/364 [==============================] - 0s 279us/step - loss: 9.8048 - mae: 2.2166 - val_loss: 14.1114 - val_mae: 2.5108\n",
      "Epoch 928/1000\n",
      "364/364 [==============================] - 0s 313us/step - loss: 9.8263 - mae: 2.2249 - val_loss: 13.5552 - val_mae: 2.5011\n",
      "Epoch 929/1000\n",
      "364/364 [==============================] - 0s 430us/step - loss: 9.8818 - mae: 2.2496 - val_loss: 13.4884 - val_mae: 2.4484\n",
      "Epoch 930/1000\n",
      "364/364 [==============================] - 0s 442us/step - loss: 9.8147 - mae: 2.2494 - val_loss: 13.5662 - val_mae: 2.4625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 931/1000\n",
      "364/364 [==============================] - 0s 310us/step - loss: 9.8784 - mae: 2.2237 - val_loss: 13.2780 - val_mae: 2.4470\n",
      "Epoch 932/1000\n",
      "364/364 [==============================] - 0s 204us/step - loss: 9.9609 - mae: 2.2427 - val_loss: 13.2123 - val_mae: 2.4374\n",
      "Epoch 933/1000\n",
      "364/364 [==============================] - 0s 454us/step - loss: 9.6967 - mae: 2.2083 - val_loss: 13.2999 - val_mae: 2.4304\n",
      "Epoch 934/1000\n",
      "364/364 [==============================] - 0s 487us/step - loss: 9.9524 - mae: 2.2214 - val_loss: 13.7072 - val_mae: 2.5491\n",
      "Epoch 935/1000\n",
      "364/364 [==============================] - 0s 166us/step - loss: 9.7695 - mae: 2.2073 - val_loss: 13.4931 - val_mae: 2.4556\n",
      "Epoch 936/1000\n",
      "364/364 [==============================] - 0s 188us/step - loss: 9.8900 - mae: 2.2366 - val_loss: 13.3351 - val_mae: 2.4605\n",
      "Epoch 937/1000\n",
      "364/364 [==============================] - 0s 259us/step - loss: 9.7930 - mae: 2.2392 - val_loss: 13.4061 - val_mae: 2.4445\n",
      "Epoch 938/1000\n",
      "364/364 [==============================] - 0s 245us/step - loss: 9.7799 - mae: 2.2275 - val_loss: 13.6527 - val_mae: 2.5195\n",
      "Epoch 939/1000\n",
      "364/364 [==============================] - 0s 244us/step - loss: 9.7170 - mae: 2.2305 - val_loss: 13.4916 - val_mae: 2.4748\n",
      "Epoch 940/1000\n",
      "364/364 [==============================] - 0s 351us/step - loss: 9.4604 - mae: 2.2002 - val_loss: 14.3845 - val_mae: 2.6670\n",
      "Epoch 941/1000\n",
      "364/364 [==============================] - 0s 367us/step - loss: 9.8333 - mae: 2.2421 - val_loss: 14.6274 - val_mae: 2.7000\n",
      "Epoch 942/1000\n",
      "364/364 [==============================] - 0s 377us/step - loss: 9.7979 - mae: 2.2357 - val_loss: 14.6594 - val_mae: 2.7226\n",
      "Epoch 943/1000\n",
      "364/364 [==============================] - 0s 257us/step - loss: 9.9423 - mae: 2.2574 - val_loss: 13.4223 - val_mae: 2.4886\n",
      "Epoch 944/1000\n",
      "364/364 [==============================] - 0s 294us/step - loss: 9.8079 - mae: 2.2214 - val_loss: 13.9475 - val_mae: 2.6018\n",
      "Epoch 945/1000\n",
      "364/364 [==============================] - 0s 361us/step - loss: 9.6967 - mae: 2.2141 - val_loss: 13.3217 - val_mae: 2.4441\n",
      "Epoch 946/1000\n",
      "364/364 [==============================] - 0s 411us/step - loss: 9.8821 - mae: 2.2276 - val_loss: 13.3624 - val_mae: 2.4537\n",
      "Epoch 947/1000\n",
      "364/364 [==============================] - 0s 364us/step - loss: 9.6402 - mae: 2.2023 - val_loss: 13.3526 - val_mae: 2.4623\n",
      "Epoch 948/1000\n",
      "364/364 [==============================] - 0s 533us/step - loss: 9.7380 - mae: 2.2293 - val_loss: 13.5958 - val_mae: 2.4655\n",
      "Epoch 949/1000\n",
      "364/364 [==============================] - 0s 357us/step - loss: 9.6754 - mae: 2.2069 - val_loss: 13.9725 - val_mae: 2.6083\n",
      "Epoch 950/1000\n",
      "364/364 [==============================] - 0s 343us/step - loss: 9.7415 - mae: 2.2066 - val_loss: 13.3676 - val_mae: 2.4783\n",
      "Epoch 951/1000\n",
      "364/364 [==============================] - 0s 292us/step - loss: 9.6814 - mae: 2.2238 - val_loss: 13.6699 - val_mae: 2.5467\n",
      "Epoch 952/1000\n",
      "364/364 [==============================] - 0s 264us/step - loss: 9.7191 - mae: 2.2113 - val_loss: 13.4209 - val_mae: 2.4621\n",
      "Epoch 953/1000\n",
      "364/364 [==============================] - 0s 307us/step - loss: 9.7170 - mae: 2.2260 - val_loss: 13.4626 - val_mae: 2.4990\n",
      "Epoch 954/1000\n",
      "364/364 [==============================] - 0s 316us/step - loss: 9.6381 - mae: 2.2135 - val_loss: 14.3425 - val_mae: 2.6742\n",
      "Epoch 955/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 9.6420 - mae: 2.2276 - val_loss: 13.8634 - val_mae: 2.5711\n",
      "Epoch 956/1000\n",
      "364/364 [==============================] - 0s 171us/step - loss: 9.7584 - mae: 2.2202 - val_loss: 13.3652 - val_mae: 2.4528\n",
      "Epoch 957/1000\n",
      "364/364 [==============================] - 0s 241us/step - loss: 9.7164 - mae: 2.1973 - val_loss: 13.3928 - val_mae: 2.4569\n",
      "Epoch 958/1000\n",
      "364/364 [==============================] - 0s 460us/step - loss: 9.6956 - mae: 2.2275 - val_loss: 13.4345 - val_mae: 2.4964\n",
      "Epoch 959/1000\n",
      "364/364 [==============================] - 0s 460us/step - loss: 9.5791 - mae: 2.1949 - val_loss: 13.2563 - val_mae: 2.4423\n",
      "Epoch 960/1000\n",
      "364/364 [==============================] - 0s 380us/step - loss: 9.7373 - mae: 2.2245 - val_loss: 13.4060 - val_mae: 2.4821\n",
      "Epoch 961/1000\n",
      "364/364 [==============================] - 0s 336us/step - loss: 9.8119 - mae: 2.2551 - val_loss: 13.4968 - val_mae: 2.5018\n",
      "Epoch 962/1000\n",
      "364/364 [==============================] - 0s 299us/step - loss: 9.5933 - mae: 2.2199 - val_loss: 14.3895 - val_mae: 2.6973\n",
      "Epoch 963/1000\n",
      "364/364 [==============================] - 0s 367us/step - loss: 9.7499 - mae: 2.2346 - val_loss: 13.0691 - val_mae: 2.4412\n",
      "Epoch 964/1000\n",
      "364/364 [==============================] - 0s 336us/step - loss: 9.5757 - mae: 2.2077 - val_loss: 13.1732 - val_mae: 2.4825\n",
      "Epoch 965/1000\n",
      "364/364 [==============================] - 0s 394us/step - loss: 9.7039 - mae: 2.2006 - val_loss: 13.9228 - val_mae: 2.6262\n",
      "Epoch 966/1000\n",
      "364/364 [==============================] - 0s 376us/step - loss: 9.6205 - mae: 2.2123 - val_loss: 13.0915 - val_mae: 2.4364\n",
      "Epoch 967/1000\n",
      "364/364 [==============================] - 0s 431us/step - loss: 9.5527 - mae: 2.1920 - val_loss: 13.1401 - val_mae: 2.4532\n",
      "Epoch 968/1000\n",
      "364/364 [==============================] - 0s 536us/step - loss: 9.8463 - mae: 2.2325 - val_loss: 13.4077 - val_mae: 2.4552\n",
      "Epoch 969/1000\n",
      "364/364 [==============================] - 0s 273us/step - loss: 9.6513 - mae: 2.2057 - val_loss: 13.2761 - val_mae: 2.4754\n",
      "Epoch 970/1000\n",
      "364/364 [==============================] - 0s 217us/step - loss: 9.5078 - mae: 2.2210 - val_loss: 13.4535 - val_mae: 2.4725\n",
      "Epoch 971/1000\n",
      "364/364 [==============================] - 0s 176us/step - loss: 9.4695 - mae: 2.2051 - val_loss: 15.1898 - val_mae: 2.8583\n",
      "Epoch 972/1000\n",
      "364/364 [==============================] - 0s 260us/step - loss: 9.4838 - mae: 2.1938 - val_loss: 12.8844 - val_mae: 2.4115\n",
      "Epoch 973/1000\n",
      "364/364 [==============================] - 0s 307us/step - loss: 9.6399 - mae: 2.2002 - val_loss: 13.9459 - val_mae: 2.6210\n",
      "Epoch 974/1000\n",
      "364/364 [==============================] - 0s 339us/step - loss: 9.5986 - mae: 2.2146 - val_loss: 13.2501 - val_mae: 2.4371\n",
      "Epoch 975/1000\n",
      "364/364 [==============================] - 0s 239us/step - loss: 9.5920 - mae: 2.2146 - val_loss: 13.2406 - val_mae: 2.4394\n",
      "Epoch 976/1000\n",
      "364/364 [==============================] - 0s 289us/step - loss: 9.6763 - mae: 2.2128 - val_loss: 13.7923 - val_mae: 2.4835\n",
      "Epoch 977/1000\n",
      "364/364 [==============================] - 0s 230us/step - loss: 9.7426 - mae: 2.2154 - val_loss: 13.2788 - val_mae: 2.4696\n",
      "Epoch 978/1000\n",
      "364/364 [==============================] - 0s 283us/step - loss: 9.4655 - mae: 2.1875 - val_loss: 13.3570 - val_mae: 2.4848\n",
      "Epoch 979/1000\n",
      "364/364 [==============================] - 0s 372us/step - loss: 9.6399 - mae: 2.2143 - val_loss: 13.3705 - val_mae: 2.4902\n",
      "Epoch 980/1000\n",
      "364/364 [==============================] - 0s 403us/step - loss: 9.5664 - mae: 2.2005 - val_loss: 13.1220 - val_mae: 2.4493\n",
      "Epoch 981/1000\n",
      "364/364 [==============================] - 0s 362us/step - loss: 9.4189 - mae: 2.1942 - val_loss: 13.8252 - val_mae: 2.6170\n",
      "Epoch 982/1000\n",
      "364/364 [==============================] - 0s 295us/step - loss: 9.5810 - mae: 2.2187 - val_loss: 13.5255 - val_mae: 2.4718\n",
      "Epoch 983/1000\n",
      "364/364 [==============================] - 0s 438us/step - loss: 9.4830 - mae: 2.1924 - val_loss: 13.1582 - val_mae: 2.4566\n",
      "Epoch 984/1000\n",
      "364/364 [==============================] - 0s 314us/step - loss: 9.3757 - mae: 2.1802 - val_loss: 14.7713 - val_mae: 2.7684\n",
      "Epoch 985/1000\n",
      "364/364 [==============================] - 0s 421us/step - loss: 9.6789 - mae: 2.2071 - val_loss: 13.2642 - val_mae: 2.4856\n",
      "Epoch 986/1000\n",
      "364/364 [==============================] - 0s 333us/step - loss: 9.5171 - mae: 2.1981 - val_loss: 13.2046 - val_mae: 2.4723\n",
      "Epoch 987/1000\n",
      "364/364 [==============================] - 0s 331us/step - loss: 9.6191 - mae: 2.2167 - val_loss: 13.1992 - val_mae: 2.4633\n",
      "Epoch 988/1000\n",
      "364/364 [==============================] - 0s 240us/step - loss: 9.5593 - mae: 2.2129 - val_loss: 13.3124 - val_mae: 2.5148\n",
      "Epoch 989/1000\n",
      "364/364 [==============================] - 0s 439us/step - loss: 9.5126 - mae: 2.2017 - val_loss: 13.7327 - val_mae: 2.5824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 990/1000\n",
      "364/364 [==============================] - 0s 516us/step - loss: 9.5555 - mae: 2.2006 - val_loss: 13.7003 - val_mae: 2.5740\n",
      "Epoch 991/1000\n",
      "364/364 [==============================] - 0s 355us/step - loss: 9.4969 - mae: 2.2071 - val_loss: 13.6547 - val_mae: 2.5579\n",
      "Epoch 992/1000\n",
      "364/364 [==============================] - 0s 371us/step - loss: 9.5932 - mae: 2.2191 - val_loss: 13.2460 - val_mae: 2.4480\n",
      "Epoch 993/1000\n",
      "364/364 [==============================] - 0s 255us/step - loss: 9.5640 - mae: 2.1835 - val_loss: 13.4812 - val_mae: 2.5419\n",
      "Epoch 994/1000\n",
      "364/364 [==============================] - 0s 169us/step - loss: 9.4245 - mae: 2.2061 - val_loss: 13.2011 - val_mae: 2.4803\n",
      "Epoch 995/1000\n",
      "364/364 [==============================] - 0s 379us/step - loss: 9.1934 - mae: 2.1502 - val_loss: 14.5351 - val_mae: 2.7336\n",
      "Epoch 996/1000\n",
      "364/364 [==============================] - 0s 311us/step - loss: 9.7802 - mae: 2.2303 - val_loss: 13.3955 - val_mae: 2.5159\n",
      "Epoch 997/1000\n",
      "364/364 [==============================] - 0s 382us/step - loss: 9.5652 - mae: 2.2092 - val_loss: 13.4407 - val_mae: 2.5290\n",
      "Epoch 998/1000\n",
      "364/364 [==============================] - 0s 321us/step - loss: 9.1831 - mae: 2.1723 - val_loss: 14.1945 - val_mae: 2.5311\n",
      "Epoch 999/1000\n",
      "364/364 [==============================] - 0s 229us/step - loss: 9.5407 - mae: 2.2029 - val_loss: 13.8407 - val_mae: 2.5105\n",
      "Epoch 1000/1000\n",
      "364/364 [==============================] - 0s 241us/step - loss: 9.5139 - mae: 2.2080 - val_loss: 13.3043 - val_mae: 2.4540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3538207690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train, batch_size=32, validation_split=0.2, epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Measuring Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.496393 , 36.025948 , 14.063124 , 24.278326 , 15.068211 ,\n",
       "       20.638186 , 17.57974  , 14.477937 , 21.959684 , 18.481707 ,\n",
       "       20.600206 , 18.15425  ,  5.494549 , 20.310112 , 19.414373 ,\n",
       "       23.563515 , 19.313713 , 10.2195835, 45.95739  , 14.111113 ,\n",
       "       26.563179 , 26.992327 , 14.689877 , 20.857819 , 15.712813 ,\n",
       "       15.719543 , 21.437971 , 11.223034 , 19.773966 , 18.16343  ,\n",
       "       20.22563  , 22.85093  , 23.43547  , 23.092295 , 14.81353  ,\n",
       "       15.395044 , 33.402622 , 20.602297 , 20.949183 , 23.558317 ,\n",
       "       15.65296  , 30.7134   , 49.82336  , 18.90917  , 25.233942 ,\n",
       "       15.372054 , 15.913403 , 24.302462 , 17.754839 , 30.484282 ,\n",
       "       19.371096 ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict unseen house prices\n",
    "Y_pred = clf.predict(X_test)\n",
    "Y_pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.6, 32.4, 13.6, 22.8, 16.1, 20. , 17.8, 14. , 19.6, 16.8, 21.5,\n",
       "       18.9,  7. , 21.2, 18.5, 29.8, 18.8, 10.2, 50. , 14.1, 25.2, 29.1,\n",
       "       12.7, 22.4, 14.2, 13.8, 20.3, 14.9, 21.7, 18.3, 23.1, 23.8, 15. ,\n",
       "       20.8, 19.1, 19.4, 34.7, 19.5, 24.4, 23.4, 19.7, 28.2, 50. , 17.4,\n",
       "       22.6, 15.1, 13.1, 24.2, 19.9, 24. , 18.9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual unseen house prices\n",
    "Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>25.496393</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1.896393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>36.025948</td>\n",
       "      <td>32.4</td>\n",
       "      <td>3.625948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.063124</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.463124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>24.278326</td>\n",
       "      <td>22.8</td>\n",
       "      <td>1.478326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15.068211</td>\n",
       "      <td>16.1</td>\n",
       "      <td>1.031789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>20.638186</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.638186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>17.579741</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.220259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14.477937</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.477937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>21.959684</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.359684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>18.481707</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1.681707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>20.600206</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.899794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>18.154249</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.745751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.494549</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.505451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>20.310112</td>\n",
       "      <td>21.2</td>\n",
       "      <td>0.889888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>19.414373</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.914373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>23.563515</td>\n",
       "      <td>29.8</td>\n",
       "      <td>6.236485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>19.313713</td>\n",
       "      <td>18.8</td>\n",
       "      <td>0.513713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>10.219584</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.019584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>45.957390</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.042610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>14.111113</td>\n",
       "      <td>14.1</td>\n",
       "      <td>0.011113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>26.563179</td>\n",
       "      <td>25.2</td>\n",
       "      <td>1.363179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>26.992327</td>\n",
       "      <td>29.1</td>\n",
       "      <td>2.107673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>14.689877</td>\n",
       "      <td>12.7</td>\n",
       "      <td>1.989877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>20.857819</td>\n",
       "      <td>22.4</td>\n",
       "      <td>1.542181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>15.712813</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.512813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>15.719543</td>\n",
       "      <td>13.8</td>\n",
       "      <td>1.919543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>21.437971</td>\n",
       "      <td>20.3</td>\n",
       "      <td>1.137971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>11.223034</td>\n",
       "      <td>14.9</td>\n",
       "      <td>3.676966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>19.773966</td>\n",
       "      <td>21.7</td>\n",
       "      <td>1.926034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>18.163429</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.136571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>20.225630</td>\n",
       "      <td>23.1</td>\n",
       "      <td>2.874370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>22.850929</td>\n",
       "      <td>23.8</td>\n",
       "      <td>0.949071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>23.435471</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.435471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>23.092295</td>\n",
       "      <td>20.8</td>\n",
       "      <td>2.292295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>14.813530</td>\n",
       "      <td>19.1</td>\n",
       "      <td>4.286470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>15.395044</td>\n",
       "      <td>19.4</td>\n",
       "      <td>4.004956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>33.402622</td>\n",
       "      <td>34.7</td>\n",
       "      <td>1.297378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>20.602297</td>\n",
       "      <td>19.5</td>\n",
       "      <td>1.102297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>20.949183</td>\n",
       "      <td>24.4</td>\n",
       "      <td>3.450817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>23.558317</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.158317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>15.652960</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.047040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>30.713400</td>\n",
       "      <td>28.2</td>\n",
       "      <td>2.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>49.823360</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.176640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>18.909170</td>\n",
       "      <td>17.4</td>\n",
       "      <td>1.509170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>25.233942</td>\n",
       "      <td>22.6</td>\n",
       "      <td>2.633942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>15.372054</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.272054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>15.913403</td>\n",
       "      <td>13.1</td>\n",
       "      <td>2.813403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>24.302462</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.102462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>17.754839</td>\n",
       "      <td>19.9</td>\n",
       "      <td>2.145161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>30.484282</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.484282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>19.371096</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.471096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predicted  Actual       MAE\n",
       "0   25.496393    23.6  1.896393\n",
       "1   36.025948    32.4  3.625948\n",
       "2   14.063124    13.6  0.463124\n",
       "3   24.278326    22.8  1.478326\n",
       "4   15.068211    16.1  1.031789\n",
       "5   20.638186    20.0  0.638186\n",
       "6   17.579741    17.8  0.220259\n",
       "7   14.477937    14.0  0.477937\n",
       "8   21.959684    19.6  2.359684\n",
       "9   18.481707    16.8  1.681707\n",
       "10  20.600206    21.5  0.899794\n",
       "11  18.154249    18.9  0.745751\n",
       "12   5.494549     7.0  1.505451\n",
       "13  20.310112    21.2  0.889888\n",
       "14  19.414373    18.5  0.914373\n",
       "15  23.563515    29.8  6.236485\n",
       "16  19.313713    18.8  0.513713\n",
       "17  10.219584    10.2  0.019584\n",
       "18  45.957390    50.0  4.042610\n",
       "19  14.111113    14.1  0.011113\n",
       "20  26.563179    25.2  1.363179\n",
       "21  26.992327    29.1  2.107673\n",
       "22  14.689877    12.7  1.989877\n",
       "23  20.857819    22.4  1.542181\n",
       "24  15.712813    14.2  1.512813\n",
       "25  15.719543    13.8  1.919543\n",
       "26  21.437971    20.3  1.137971\n",
       "27  11.223034    14.9  3.676966\n",
       "28  19.773966    21.7  1.926034\n",
       "29  18.163429    18.3  0.136571\n",
       "30  20.225630    23.1  2.874370\n",
       "31  22.850929    23.8  0.949071\n",
       "32  23.435471    15.0  8.435471\n",
       "33  23.092295    20.8  2.292295\n",
       "34  14.813530    19.1  4.286470\n",
       "35  15.395044    19.4  4.004956\n",
       "36  33.402622    34.7  1.297378\n",
       "37  20.602297    19.5  1.102297\n",
       "38  20.949183    24.4  3.450817\n",
       "39  23.558317    23.4  0.158317\n",
       "40  15.652960    19.7  4.047040\n",
       "41  30.713400    28.2  2.513400\n",
       "42  49.823360    50.0  0.176640\n",
       "43  18.909170    17.4  1.509170\n",
       "44  25.233942    22.6  2.633942\n",
       "45  15.372054    15.1  0.272054\n",
       "46  15.913403    13.1  2.813403\n",
       "47  24.302462    24.2  0.102462\n",
       "48  17.754839    19.9  2.145161\n",
       "49  30.484282    24.0  6.484282\n",
       "50  19.371096    18.9  0.471096"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.hstack([Y_pred, Y_test.values[:, np.newaxis], np.abs(Y_pred-Y_test.values[:, np.newaxis])]),\n",
    "             columns=['Predicted', 'Actual', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s 102us/step\n",
      "51/51 [==============================] - 0s 184us/step\n",
      "Train Loss = 10.293074724700425, mae = 2.2441749572753906 \n",
      "Test Loss = 6.8399164723415, mae = 1.9408825635910034 \n"
     ]
    }
   ],
   "source": [
    "train_metrics = clf.evaluate(X_train, Y_train)\n",
    "test_metrics = clf.evaluate(X_test, Y_test)\n",
    "print(f'Train Loss = {train_metrics[0]}, mae = {train_metrics[1]} ')\n",
    "print(f'Test Loss = {test_metrics[0]}, mae = {test_metrics[1]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Displaying Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curve(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.title('Training MAE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.show()\n",
    "plot_learning_curve(clf.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Exploring Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights = \n",
      " [array([[-0.34176114, -0.20444523, -0.33484566, ...,  0.04795467,\n",
      "        -0.21849991,  0.05898421],\n",
      "       [ 0.13970768,  0.06154178,  0.18185054, ..., -0.02471063,\n",
      "        -0.06726149, -0.10609649],\n",
      "       [ 0.05101351, -0.16674873,  0.14792036, ..., -0.09442123,\n",
      "         0.16163407,  0.12129083],\n",
      "       ...,\n",
      "       [ 0.10537621, -0.04935675, -0.00300199, ..., -0.14046764,\n",
      "        -0.11122495, -0.10730931],\n",
      "       [ 0.04949609,  0.28859273, -0.09462877, ...,  0.06117075,\n",
      "         0.04202529, -0.12291943],\n",
      "       [-0.13994485, -0.3743731 , -0.1893397 , ..., -0.03356833,\n",
      "        -0.01659575, -0.12861831]], dtype=float32), array([ 0.51652867,  0.32447895,  0.51400536, -0.3358101 ,  0.        ,\n",
      "        0.        , -0.01603675,  0.32504892, -0.03174132, -0.00316219,\n",
      "       -0.63227093,  0.50998026,  0.506888  ,  0.51385075, -0.04994085,\n",
      "        0.5080765 ,  0.51409274,  0.        , -0.29002947,  0.508451  ,\n",
      "       -0.01513533, -0.03451186,  0.51210225, -0.01604765, -0.01507193,\n",
      "        0.        ,  0.5168831 , -0.06943209,  0.15646036, -0.04260274,\n",
      "        0.519817  ,  0.50961465,  0.5203419 ,  0.51098365,  0.        ,\n",
      "       -0.04810547,  0.5031495 ,  0.        ,  0.47135362, -0.33498597,\n",
      "       -0.05900201,  0.        , -0.02602404,  0.        , -0.034079  ,\n",
      "        0.5089618 ,  0.16915153,  0.53011775,  0.5099674 ,  0.51176006,\n",
      "       -0.02084038,  0.        ,  0.51197284,  0.50764984,  0.5129017 ,\n",
      "        0.5183734 ,  0.        ,  0.34554356,  0.5088827 ,  0.5205466 ,\n",
      "       -0.27371538,  0.4539908 ,  0.        ,  0.24771741, -0.04849861,\n",
      "       -0.26175427, -0.01239962,  0.5178317 ,  0.5144494 , -0.01445475,\n",
      "       -0.00586881,  0.5121471 ,  0.        , -0.02702501,  0.0586832 ,\n",
      "        0.5083593 , -0.2891988 ,  0.36944142, -0.2999245 ,  0.14808325,\n",
      "        0.5171354 , -0.31460813, -0.02485855,  0.        ,  0.513669  ,\n",
      "        0.50337017,  0.506226  ,  0.        ,  0.17583808, -0.0053084 ,\n",
      "        0.51922476, -0.01067775, -0.04307305,  0.51397985, -0.02741222,\n",
      "       -0.04468613,  0.49269328,  0.5185468 ,  0.        , -0.02730978,\n",
      "        0.5132807 ,  0.50843346, -0.36518395, -0.040658  , -0.31707597,\n",
      "        0.50994444, -0.00316224,  0.5252887 ,  0.5120452 , -0.06341849,\n",
      "       -0.2901333 ,  0.51256114,  0.50881404,  0.3717318 , -0.02079511,\n",
      "        0.27040032,  0.5142637 , -0.01031177,  0.51042193,  0.        ,\n",
      "        0.        ,  0.51113313,  0.50982463,  0.51791155,  0.        ,\n",
      "        0.        ,  0.5177421 ,  0.        , -0.02097527,  0.49736694,\n",
      "       -0.30479544,  0.        ,  0.51732075,  0.51594883, -0.01547685,\n",
      "       -0.00316227,  0.5188148 , -0.03623001,  0.32469434,  0.52691406,\n",
      "        0.5154948 , -0.01361064, -0.01530128,  0.39015782,  0.51383615,\n",
      "        0.5297976 , -0.02188366, -0.04242533,  0.51610845,  0.5295104 ,\n",
      "        0.48719218, -0.04073857,  0.51243335,  0.52888227,  0.5092511 ,\n",
      "        0.510055  , -0.31631005, -0.02331548,  0.5175093 ,  0.        ,\n",
      "        0.        ,  0.52211314,  0.        ,  0.        , -0.03821979,\n",
      "        0.5086257 ,  0.5121355 ,  0.        , -0.01929088,  0.        ,\n",
      "        0.01393945,  0.41367373, -0.31049058,  0.5075989 ,  0.51052076,\n",
      "       -0.01200454,  0.        ,  0.511974  ,  0.        , -0.28021786,\n",
      "        0.5073436 ,  0.51016665,  0.24626812,  0.        , -0.01592983,\n",
      "        0.5164694 , -0.29045534, -0.28393152, -0.02836958,  0.        ,\n",
      "       -0.29913738,  0.51494163,  0.        ,  0.50940704, -0.00516251,\n",
      "        0.16386499, -0.00316227, -0.31232178,  0.517477  , -0.35219178,\n",
      "       -0.32150176,  0.51908547, -0.33767077, -0.01801314,  0.51798433,\n",
      "        0.5096434 , -0.30891743,  0.50690013, -0.01182203,  0.5289656 ,\n",
      "        0.5076584 ,  0.        ,  0.50823975, -0.03215265,  0.51093364,\n",
      "        0.51693904,  0.5200866 , -0.34825045,  0.        ,  0.4763777 ,\n",
      "       -0.01726495,  0.5280949 ,  0.51519257, -0.02768152,  0.01551741,\n",
      "        0.50738335,  0.5187329 , -0.2951025 ,  0.        , -0.2764567 ,\n",
      "        0.        ,  0.51142204,  0.5079581 , -0.04325142, -0.04884176,\n",
      "        0.        ,  0.5164188 ,  0.50876784,  0.52255   , -0.31529427,\n",
      "        0.24924958,  0.5153724 , -0.3190655 ,  0.5115705 , -0.02421423,\n",
      "        0.50734097,  0.14725237,  0.51485497,  0.51241964,  0.        ,\n",
      "        0.5113278 ,  0.        ,  0.51634884, -0.03156208,  0.50804573,\n",
      "        0.        ], dtype=float32), array([[ 0.34940293],\n",
      "       [ 0.72085375],\n",
      "       [ 0.36608586],\n",
      "       [-0.3362142 ],\n",
      "       [ 0.01355445],\n",
      "       [-0.00535086],\n",
      "       [ 3.631726  ],\n",
      "       [ 0.8246985 ],\n",
      "       [-0.06664918],\n",
      "       [-0.00884983],\n",
      "       [-1.2606187 ],\n",
      "       [ 0.30342123],\n",
      "       [ 0.30753422],\n",
      "       [ 0.3866892 ],\n",
      "       [-0.02093068],\n",
      "       [ 0.36743534],\n",
      "       [ 0.3815066 ],\n",
      "       [ 0.10395421],\n",
      "       [-0.2947112 ],\n",
      "       [ 0.35848784],\n",
      "       [-0.12313587],\n",
      "       [-0.04922609],\n",
      "       [ 0.3270125 ],\n",
      "       [-0.12334523],\n",
      "       [-0.04828177],\n",
      "       [ 0.00522397],\n",
      "       [ 0.2517664 ],\n",
      "       [-0.01139878],\n",
      "       [ 1.8986729 ],\n",
      "       [-0.05139025],\n",
      "       [ 0.4313668 ],\n",
      "       [ 0.35228595],\n",
      "       [ 0.29047945],\n",
      "       [ 0.32191333],\n",
      "       [ 0.07340065],\n",
      "       [-0.02147922],\n",
      "       [ 0.18305556],\n",
      "       [-0.11958502],\n",
      "       [ 0.19180052],\n",
      "       [-0.49558616],\n",
      "       [-0.10069721],\n",
      "       [-0.08523294],\n",
      "       [-0.09569398],\n",
      "       [ 0.11982881],\n",
      "       [-0.0418933 ],\n",
      "       [ 0.34087625],\n",
      "       [ 2.3839505 ],\n",
      "       [ 0.2872666 ],\n",
      "       [ 0.28187665],\n",
      "       [ 0.2688273 ],\n",
      "       [-0.08258996],\n",
      "       [ 0.09512113],\n",
      "       [ 0.32746074],\n",
      "       [ 0.34629503],\n",
      "       [ 0.22635257],\n",
      "       [ 0.30459195],\n",
      "       [ 0.03923713],\n",
      "       [ 2.1320224 ],\n",
      "       [ 0.1927921 ],\n",
      "       [ 0.3245839 ],\n",
      "       [-0.300987  ],\n",
      "       [ 0.19838905],\n",
      "       [-0.06206873],\n",
      "       [ 0.99773514],\n",
      "       [-0.0523915 ],\n",
      "       [-0.36623484],\n",
      "       [-0.12399267],\n",
      "       [ 0.43337473],\n",
      "       [ 0.2303888 ],\n",
      "       [-0.11640384],\n",
      "       [-0.11019596],\n",
      "       [ 0.33985025],\n",
      "       [ 0.10280307],\n",
      "       [-0.09055413],\n",
      "       [ 1.8881692 ],\n",
      "       [ 0.37058634],\n",
      "       [-0.30375686],\n",
      "       [ 0.80246174],\n",
      "       [-0.31718785],\n",
      "       [ 2.2175112 ],\n",
      "       [ 0.2924823 ],\n",
      "       [-0.37979385],\n",
      "       [-0.10654506],\n",
      "       [ 0.13595061],\n",
      "       [ 0.4011248 ],\n",
      "       [ 0.16796891],\n",
      "       [ 0.2853126 ],\n",
      "       [ 0.08485477],\n",
      "       [ 1.9345765 ],\n",
      "       [-0.03262069],\n",
      "       [ 0.44667095],\n",
      "       [-0.01702644],\n",
      "       [-0.10504759],\n",
      "       [ 0.40644577],\n",
      "       [-0.06432816],\n",
      "       [-0.05008188],\n",
      "       [ 0.2360698 ],\n",
      "       [ 0.24298984],\n",
      "       [ 0.02842057],\n",
      "       [ 3.6752756 ],\n",
      "       [ 0.4253174 ],\n",
      "       [ 0.20924395],\n",
      "       [-0.6376832 ],\n",
      "       [-0.08347303],\n",
      "       [-0.3913567 ],\n",
      "       [ 0.36462688],\n",
      "       [-0.01895597],\n",
      "       [ 0.3632554 ],\n",
      "       [ 0.3729356 ],\n",
      "       [-0.01457675],\n",
      "       [-0.27332416],\n",
      "       [ 0.39503747],\n",
      "       [ 0.3322928 ],\n",
      "       [ 2.2526665 ],\n",
      "       [-0.12555058],\n",
      "       [ 2.1812162 ],\n",
      "       [ 0.38132417],\n",
      "       [-0.04763169],\n",
      "       [ 0.38840607],\n",
      "       [ 0.03213194],\n",
      "       [-0.06879017],\n",
      "       [ 0.3321564 ],\n",
      "       [ 0.28522104],\n",
      "       [ 0.4227357 ],\n",
      "       [-0.02060947],\n",
      "       [-0.04662153],\n",
      "       [ 0.37277892],\n",
      "       [ 0.11618428],\n",
      "       [-0.05146757],\n",
      "       [ 0.26861238],\n",
      "       [-0.3541769 ],\n",
      "       [ 0.02602127],\n",
      "       [ 0.4213828 ],\n",
      "       [ 0.30667588],\n",
      "       [-0.08961233],\n",
      "       [-0.09218664],\n",
      "       [ 0.37331063],\n",
      "       [-0.12748118],\n",
      "       [ 0.81176645],\n",
      "       [ 0.31824335],\n",
      "       [ 0.40133336],\n",
      "       [-0.05825423],\n",
      "       [-0.05057995],\n",
      "       [ 0.5784928 ],\n",
      "       [ 0.3182165 ],\n",
      "       [ 0.2888521 ],\n",
      "       [-0.06558435],\n",
      "       [-0.0235869 ],\n",
      "       [ 0.21941538],\n",
      "       [ 0.28287426],\n",
      "       [ 0.19066261],\n",
      "       [-0.12507817],\n",
      "       [ 0.19453771],\n",
      "       [ 0.3042665 ],\n",
      "       [ 0.34185466],\n",
      "       [ 0.33227944],\n",
      "       [-0.38957664],\n",
      "       [-0.08251843],\n",
      "       [ 0.40345275],\n",
      "       [-0.09431779],\n",
      "       [ 0.06503607],\n",
      "       [ 0.2432412 ],\n",
      "       [ 0.02167608],\n",
      "       [-0.0619778 ],\n",
      "       [-0.03809524],\n",
      "       [ 0.27805105],\n",
      "       [ 0.2951858 ],\n",
      "       [-0.09833229],\n",
      "       [-0.10887548],\n",
      "       [ 0.14224716],\n",
      "       [ 3.866486  ],\n",
      "       [ 0.9503502 ],\n",
      "       [-0.38142523],\n",
      "       [ 0.34041575],\n",
      "       [ 0.32841125],\n",
      "       [-0.04379197],\n",
      "       [ 0.07493193],\n",
      "       [ 0.38063872],\n",
      "       [ 0.07110076],\n",
      "       [-0.31702903],\n",
      "       [ 0.33773416],\n",
      "       [ 0.36094415],\n",
      "       [ 2.0470572 ],\n",
      "       [ 0.01078489],\n",
      "       [-0.03278018],\n",
      "       [ 0.32759356],\n",
      "       [-0.30500677],\n",
      "       [-0.3254832 ],\n",
      "       [-0.06665472],\n",
      "       [ 0.10786305],\n",
      "       [-0.34885156],\n",
      "       [ 0.35850036],\n",
      "       [-0.01190217],\n",
      "       [ 0.34220687],\n",
      "       [-0.14147706],\n",
      "       [ 1.2082989 ],\n",
      "       [-0.13190825],\n",
      "       [-0.35463917],\n",
      "       [ 0.33754325],\n",
      "       [-0.3592172 ],\n",
      "       [-0.558391  ],\n",
      "       [ 0.4771622 ],\n",
      "       [-0.5088136 ],\n",
      "       [-0.06789118],\n",
      "       [ 0.3269712 ],\n",
      "       [ 0.3765172 ],\n",
      "       [-0.338628  ],\n",
      "       [ 0.3191609 ],\n",
      "       [-0.05591682],\n",
      "       [ 0.31366593],\n",
      "       [ 0.22893627],\n",
      "       [ 0.02288614],\n",
      "       [ 0.23310897],\n",
      "       [-0.03394995],\n",
      "       [ 0.27781257],\n",
      "       [ 0.27976832],\n",
      "       [ 0.40190566],\n",
      "       [-0.4774357 ],\n",
      "       [ 0.08770004],\n",
      "       [ 0.19173513],\n",
      "       [-0.05181949],\n",
      "       [ 0.3884898 ],\n",
      "       [ 0.22510563],\n",
      "       [-0.11910752],\n",
      "       [ 3.5653415 ],\n",
      "       [ 0.36406285],\n",
      "       [ 0.45804045],\n",
      "       [-1.1396583 ],\n",
      "       [-0.0323998 ],\n",
      "       [-0.39244112],\n",
      "       [-0.12805039],\n",
      "       [ 0.28356606],\n",
      "       [ 0.37569037],\n",
      "       [-0.07975299],\n",
      "       [-0.10497671],\n",
      "       [ 0.1223187 ],\n",
      "       [ 0.29186785],\n",
      "       [ 0.2910831 ],\n",
      "       [ 0.36625966],\n",
      "       [-0.44052866],\n",
      "       [ 2.507198  ],\n",
      "       [ 0.4411135 ],\n",
      "       [-0.31195858],\n",
      "       [ 0.35046554],\n",
      "       [-0.12541324],\n",
      "       [ 0.36392918],\n",
      "       [ 1.3653017 ],\n",
      "       [ 0.36830807],\n",
      "       [ 0.3375325 ],\n",
      "       [ 0.10321631],\n",
      "       [ 0.25923562],\n",
      "       [ 0.03532171],\n",
      "       [ 0.22531667],\n",
      "       [-0.05494479],\n",
      "       [ 0.29210705],\n",
      "       [-0.05438236]], dtype=float32), array([0.5123017], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Model weights = \\n {clf.get_weights()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Tuning Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X_train, Y_train, hidden_layers=1, neurons=256, epochs=1000, learning_rate=0.001):\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(units=neurons, activation='relu', input_shape=(13,)))\n",
    "    for i in range(hidden_layers-1):\n",
    "        clf.add(Dense(units=neurons, activation='relu'))\n",
    "        \n",
    "    clf.add(Dense(units=1, activation='linear'))\n",
    "    clf.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])\n",
    "    clf.fit(X_train, Y_train, batch_size=32, validation_split=0.2, epochs=epochs, verbose=False)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s 311us/step\n",
      "51/51 [==============================] - 0s 826us/step\n",
      "Train Loss = 2.1430981028210985, mae = 0.925952672958374 \n",
      "Test Loss = 11.533160639744178, mae = 2.1609060764312744 \n"
     ]
    }
   ],
   "source": [
    "# Increase number of hidden layers\n",
    "clf = nn_model(X_train, Y_train, hidden_layers=2 , neurons=256, epochs=1000)\n",
    "train_metrics = clf.evaluate(X_train, Y_train)\n",
    "test_metrics = clf.evaluate(X_test, Y_test)\n",
    "print(f'Train Loss = {train_metrics[0]}, mae = {train_metrics[1]} ')\n",
    "print(f'Test Loss = {test_metrics[0]}, mae = {test_metrics[1]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>27.453541</td>\n",
       "      <td>23.6</td>\n",
       "      <td>3.853541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>32.694942</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.294942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.635893</td>\n",
       "      <td>13.6</td>\n",
       "      <td>3.035893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>23.341984</td>\n",
       "      <td>22.8</td>\n",
       "      <td>0.541984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>18.489191</td>\n",
       "      <td>16.1</td>\n",
       "      <td>2.389191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>21.760431</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.760431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>20.193823</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2.393823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>16.392029</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.392029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>24.314812</td>\n",
       "      <td>19.6</td>\n",
       "      <td>4.714812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>20.494827</td>\n",
       "      <td>16.8</td>\n",
       "      <td>3.694827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>20.814663</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.685337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>20.256863</td>\n",
       "      <td>18.9</td>\n",
       "      <td>1.356863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>8.025888</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.025888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>21.791687</td>\n",
       "      <td>21.2</td>\n",
       "      <td>0.591687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>17.999260</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.500740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>23.985065</td>\n",
       "      <td>29.8</td>\n",
       "      <td>5.814935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>20.128740</td>\n",
       "      <td>18.8</td>\n",
       "      <td>1.328740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>9.886791</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.313209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>48.125256</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.874744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>13.856108</td>\n",
       "      <td>14.1</td>\n",
       "      <td>0.243892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>25.377720</td>\n",
       "      <td>25.2</td>\n",
       "      <td>0.177720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>26.297569</td>\n",
       "      <td>29.1</td>\n",
       "      <td>2.802431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>14.970384</td>\n",
       "      <td>12.7</td>\n",
       "      <td>2.270384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>24.774868</td>\n",
       "      <td>22.4</td>\n",
       "      <td>2.374868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>15.750255</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.550255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>16.166185</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.366185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>21.032156</td>\n",
       "      <td>20.3</td>\n",
       "      <td>0.732156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>12.577521</td>\n",
       "      <td>14.9</td>\n",
       "      <td>2.322479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>21.426344</td>\n",
       "      <td>21.7</td>\n",
       "      <td>0.273656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>20.024221</td>\n",
       "      <td>18.3</td>\n",
       "      <td>1.724221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>23.411949</td>\n",
       "      <td>23.1</td>\n",
       "      <td>0.311949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>24.069139</td>\n",
       "      <td>23.8</td>\n",
       "      <td>0.269139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>33.403851</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.403851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>22.900505</td>\n",
       "      <td>20.8</td>\n",
       "      <td>2.100505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>15.193877</td>\n",
       "      <td>19.1</td>\n",
       "      <td>3.906123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>17.981977</td>\n",
       "      <td>19.4</td>\n",
       "      <td>1.418023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>30.979424</td>\n",
       "      <td>34.7</td>\n",
       "      <td>3.720576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>19.658335</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.158335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>22.864075</td>\n",
       "      <td>24.4</td>\n",
       "      <td>1.535925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>25.032871</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1.632871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>18.282930</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1.417070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>31.380468</td>\n",
       "      <td>28.2</td>\n",
       "      <td>3.180468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>52.004894</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.004894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>20.143690</td>\n",
       "      <td>17.4</td>\n",
       "      <td>2.743690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>25.344990</td>\n",
       "      <td>22.6</td>\n",
       "      <td>2.744990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>13.598077</td>\n",
       "      <td>15.1</td>\n",
       "      <td>1.501923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>13.951177</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.851177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>25.729080</td>\n",
       "      <td>24.2</td>\n",
       "      <td>1.529080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>21.387789</td>\n",
       "      <td>19.9</td>\n",
       "      <td>1.487789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>24.719292</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.719292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>22.066677</td>\n",
       "      <td>18.9</td>\n",
       "      <td>3.166677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predicted  Actual        MAE\n",
       "0   27.453541    23.6   3.853541\n",
       "1   32.694942    32.4   0.294942\n",
       "2   16.635893    13.6   3.035893\n",
       "3   23.341984    22.8   0.541984\n",
       "4   18.489191    16.1   2.389191\n",
       "5   21.760431    20.0   1.760431\n",
       "6   20.193823    17.8   2.393823\n",
       "7   16.392029    14.0   2.392029\n",
       "8   24.314812    19.6   4.714812\n",
       "9   20.494827    16.8   3.694827\n",
       "10  20.814663    21.5   0.685337\n",
       "11  20.256863    18.9   1.356863\n",
       "12   8.025888     7.0   1.025888\n",
       "13  21.791687    21.2   0.591687\n",
       "14  17.999260    18.5   0.500740\n",
       "15  23.985065    29.8   5.814935\n",
       "16  20.128740    18.8   1.328740\n",
       "17   9.886791    10.2   0.313209\n",
       "18  48.125256    50.0   1.874744\n",
       "19  13.856108    14.1   0.243892\n",
       "20  25.377720    25.2   0.177720\n",
       "21  26.297569    29.1   2.802431\n",
       "22  14.970384    12.7   2.270384\n",
       "23  24.774868    22.4   2.374868\n",
       "24  15.750255    14.2   1.550255\n",
       "25  16.166185    13.8   2.366185\n",
       "26  21.032156    20.3   0.732156\n",
       "27  12.577521    14.9   2.322479\n",
       "28  21.426344    21.7   0.273656\n",
       "29  20.024221    18.3   1.724221\n",
       "30  23.411949    23.1   0.311949\n",
       "31  24.069139    23.8   0.269139\n",
       "32  33.403851    15.0  18.403851\n",
       "33  22.900505    20.8   2.100505\n",
       "34  15.193877    19.1   3.906123\n",
       "35  17.981977    19.4   1.418023\n",
       "36  30.979424    34.7   3.720576\n",
       "37  19.658335    19.5   0.158335\n",
       "38  22.864075    24.4   1.535925\n",
       "39  25.032871    23.4   1.632871\n",
       "40  18.282930    19.7   1.417070\n",
       "41  31.380468    28.2   3.180468\n",
       "42  52.004894    50.0   2.004894\n",
       "43  20.143690    17.4   2.743690\n",
       "44  25.344990    22.6   2.744990\n",
       "45  13.598077    15.1   1.501923\n",
       "46  13.951177    13.1   0.851177\n",
       "47  25.729080    24.2   1.529080\n",
       "48  21.387789    19.9   1.487789\n",
       "49  24.719292    24.0   0.719292\n",
       "50  22.066677    18.9   3.166677"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "pd.DataFrame(np.hstack([Y_pred, Y_test.values[:, np.newaxis], np.abs(Y_pred-Y_test.values[:, np.newaxis])]),\n",
    "             columns=['Predicted', 'Actual', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s 177us/step\n",
      "51/51 [==============================] - 0s 206us/step\n",
      "Train Loss = 6.086328955011053, mae = 1.7768644094467163 \n",
      "Test Loss = 8.401299121333103, mae = 2.142043113708496 \n"
     ]
    }
   ],
   "source": [
    "# Decrease number of hidden neurons\n",
    "clf = nn_model(X_train, Y_train, hidden_layers=2 , neurons=32, epochs=1000)\n",
    "train_metrics = clf.evaluate(X_train, Y_train)\n",
    "test_metrics = clf.evaluate(X_test, Y_test)\n",
    "print(f'Train Loss = {train_metrics[0]}, mae = {train_metrics[1]} ')\n",
    "print(f'Test Loss = {test_metrics[0]}, mae = {test_metrics[1]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>26.988781</td>\n",
       "      <td>23.6</td>\n",
       "      <td>3.388781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>31.367445</td>\n",
       "      <td>32.4</td>\n",
       "      <td>1.032555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15.406576</td>\n",
       "      <td>13.6</td>\n",
       "      <td>1.806576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>25.094824</td>\n",
       "      <td>22.8</td>\n",
       "      <td>2.294824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>16.518213</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.418213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>21.577982</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.577982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>19.102295</td>\n",
       "      <td>17.8</td>\n",
       "      <td>1.302295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>15.513959</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.513959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>23.144897</td>\n",
       "      <td>19.6</td>\n",
       "      <td>3.544897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>19.304686</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2.504686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>20.700109</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.799891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>18.609316</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.290684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>8.002965</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.002965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>21.186060</td>\n",
       "      <td>21.2</td>\n",
       "      <td>0.013940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>18.970507</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.470507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>22.853096</td>\n",
       "      <td>29.8</td>\n",
       "      <td>6.946904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>20.737587</td>\n",
       "      <td>18.8</td>\n",
       "      <td>1.937587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>11.275033</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1.075033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>48.110592</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.889408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>14.804264</td>\n",
       "      <td>14.1</td>\n",
       "      <td>0.704264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>23.621489</td>\n",
       "      <td>25.2</td>\n",
       "      <td>1.578511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>23.944052</td>\n",
       "      <td>29.1</td>\n",
       "      <td>5.155948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>14.056279</td>\n",
       "      <td>12.7</td>\n",
       "      <td>1.356279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>22.324974</td>\n",
       "      <td>22.4</td>\n",
       "      <td>0.075026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>16.365438</td>\n",
       "      <td>14.2</td>\n",
       "      <td>2.165438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>16.233809</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.433809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>21.925934</td>\n",
       "      <td>20.3</td>\n",
       "      <td>1.625934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>11.632559</td>\n",
       "      <td>14.9</td>\n",
       "      <td>3.267441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>20.299643</td>\n",
       "      <td>21.7</td>\n",
       "      <td>1.400357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>18.744364</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.444364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>22.057060</td>\n",
       "      <td>23.1</td>\n",
       "      <td>1.042940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>23.323938</td>\n",
       "      <td>23.8</td>\n",
       "      <td>0.476062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>26.320871</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.320871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>21.029921</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.229921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>15.193184</td>\n",
       "      <td>19.1</td>\n",
       "      <td>3.906816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>16.569036</td>\n",
       "      <td>19.4</td>\n",
       "      <td>2.830964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>29.807596</td>\n",
       "      <td>34.7</td>\n",
       "      <td>4.892404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>20.966785</td>\n",
       "      <td>19.5</td>\n",
       "      <td>1.466785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>22.226875</td>\n",
       "      <td>24.4</td>\n",
       "      <td>2.173125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>24.988567</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1.588567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>15.892850</td>\n",
       "      <td>19.7</td>\n",
       "      <td>3.807150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>33.143791</td>\n",
       "      <td>28.2</td>\n",
       "      <td>4.943791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>50.232891</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.232891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>21.044710</td>\n",
       "      <td>17.4</td>\n",
       "      <td>3.644710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>25.862906</td>\n",
       "      <td>22.6</td>\n",
       "      <td>3.262906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>16.683372</td>\n",
       "      <td>15.1</td>\n",
       "      <td>1.583372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>14.773271</td>\n",
       "      <td>13.1</td>\n",
       "      <td>1.673271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>25.611593</td>\n",
       "      <td>24.2</td>\n",
       "      <td>1.411593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>18.812170</td>\n",
       "      <td>19.9</td>\n",
       "      <td>1.087830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>26.688808</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.688808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>19.860365</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.960365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predicted  Actual        MAE\n",
       "0   26.988781    23.6   3.388781\n",
       "1   31.367445    32.4   1.032555\n",
       "2   15.406576    13.6   1.806576\n",
       "3   25.094824    22.8   2.294824\n",
       "4   16.518213    16.1   0.418213\n",
       "5   21.577982    20.0   1.577982\n",
       "6   19.102295    17.8   1.302295\n",
       "7   15.513959    14.0   1.513959\n",
       "8   23.144897    19.6   3.544897\n",
       "9   19.304686    16.8   2.504686\n",
       "10  20.700109    21.5   0.799891\n",
       "11  18.609316    18.9   0.290684\n",
       "12   8.002965     7.0   1.002965\n",
       "13  21.186060    21.2   0.013940\n",
       "14  18.970507    18.5   0.470507\n",
       "15  22.853096    29.8   6.946904\n",
       "16  20.737587    18.8   1.937587\n",
       "17  11.275033    10.2   1.075033\n",
       "18  48.110592    50.0   1.889408\n",
       "19  14.804264    14.1   0.704264\n",
       "20  23.621489    25.2   1.578511\n",
       "21  23.944052    29.1   5.155948\n",
       "22  14.056279    12.7   1.356279\n",
       "23  22.324974    22.4   0.075026\n",
       "24  16.365438    14.2   2.165438\n",
       "25  16.233809    13.8   2.433809\n",
       "26  21.925934    20.3   1.625934\n",
       "27  11.632559    14.9   3.267441\n",
       "28  20.299643    21.7   1.400357\n",
       "29  18.744364    18.3   0.444364\n",
       "30  22.057060    23.1   1.042940\n",
       "31  23.323938    23.8   0.476062\n",
       "32  26.320871    15.0  11.320871\n",
       "33  21.029921    20.8   0.229921\n",
       "34  15.193184    19.1   3.906816\n",
       "35  16.569036    19.4   2.830964\n",
       "36  29.807596    34.7   4.892404\n",
       "37  20.966785    19.5   1.466785\n",
       "38  22.226875    24.4   2.173125\n",
       "39  24.988567    23.4   1.588567\n",
       "40  15.892850    19.7   3.807150\n",
       "41  33.143791    28.2   4.943791\n",
       "42  50.232891    50.0   0.232891\n",
       "43  21.044710    17.4   3.644710\n",
       "44  25.862906    22.6   3.262906\n",
       "45  16.683372    15.1   1.583372\n",
       "46  14.773271    13.1   1.673271\n",
       "47  25.611593    24.2   1.411593\n",
       "48  18.812170    19.9   1.087830\n",
       "49  26.688808    24.0   2.688808\n",
       "50  19.860365    18.9   0.960365"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "pd.DataFrame(np.hstack([Y_pred, Y_test.values[:, np.newaxis], np.abs(Y_pred-Y_test.values[:, np.newaxis])]),\n",
    "             columns=['Predicted', 'Actual', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s 139us/step\n",
      "51/51 [==============================] - 0s 173us/step\n",
      "Train Loss = 7.345876667525742, mae = 1.9173424243927002 \n",
      "Test Loss = 6.298856604333017, mae = 1.8177618980407715 \n"
     ]
    }
   ],
   "source": [
    "# Increase number of hidden layers and Decrease number of hidden neurons! \n",
    "clf = nn_model(X_train, Y_train, hidden_layers=3 , neurons=16, epochs=1000)\n",
    "train_metrics = clf.evaluate(X_train, Y_train)\n",
    "test_metrics = clf.evaluate(X_test, Y_test)\n",
    "print(f'Train Loss = {train_metrics[0]}, mae = {train_metrics[1]} ')\n",
    "print(f'Test Loss = {test_metrics[0]}, mae = {test_metrics[1]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>24.654657</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1.054657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>31.528280</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.871720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.646070</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.953930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>22.472269</td>\n",
       "      <td>22.8</td>\n",
       "      <td>0.327731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>16.408958</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.308958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>19.212172</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.787828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>17.815571</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.015571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13.489441</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.510559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>23.637598</td>\n",
       "      <td>19.6</td>\n",
       "      <td>4.037598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>19.240616</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2.440616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>20.512531</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.987469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17.341719</td>\n",
       "      <td>18.9</td>\n",
       "      <td>1.558281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>8.299414</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.299414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>18.929043</td>\n",
       "      <td>21.2</td>\n",
       "      <td>2.270957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>17.855413</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.644587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>23.568462</td>\n",
       "      <td>29.8</td>\n",
       "      <td>6.231538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>19.220367</td>\n",
       "      <td>18.8</td>\n",
       "      <td>0.420367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>10.418386</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.218386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>45.454643</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.545357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>14.743210</td>\n",
       "      <td>14.1</td>\n",
       "      <td>0.643210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>26.085329</td>\n",
       "      <td>25.2</td>\n",
       "      <td>0.885329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>25.954712</td>\n",
       "      <td>29.1</td>\n",
       "      <td>3.145288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>11.814768</td>\n",
       "      <td>12.7</td>\n",
       "      <td>0.885232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>24.248915</td>\n",
       "      <td>22.4</td>\n",
       "      <td>1.848915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>15.512203</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.312203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>14.958467</td>\n",
       "      <td>13.8</td>\n",
       "      <td>1.158467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>18.432352</td>\n",
       "      <td>20.3</td>\n",
       "      <td>1.867648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>10.753948</td>\n",
       "      <td>14.9</td>\n",
       "      <td>4.146052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>21.495802</td>\n",
       "      <td>21.7</td>\n",
       "      <td>0.204198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>18.002338</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.297662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>19.413502</td>\n",
       "      <td>23.1</td>\n",
       "      <td>3.686498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>23.207787</td>\n",
       "      <td>23.8</td>\n",
       "      <td>0.592213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>21.769140</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.769140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>20.301870</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.498130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>14.651910</td>\n",
       "      <td>19.1</td>\n",
       "      <td>4.448090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>16.198610</td>\n",
       "      <td>19.4</td>\n",
       "      <td>3.201390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>31.012062</td>\n",
       "      <td>34.7</td>\n",
       "      <td>3.687938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>21.125477</td>\n",
       "      <td>19.5</td>\n",
       "      <td>1.625477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>21.860136</td>\n",
       "      <td>24.4</td>\n",
       "      <td>2.539864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>23.321764</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.078236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>13.067485</td>\n",
       "      <td>19.7</td>\n",
       "      <td>6.632515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>29.861059</td>\n",
       "      <td>28.2</td>\n",
       "      <td>1.661059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>53.235943</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.235943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>17.584295</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.184295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>25.831135</td>\n",
       "      <td>22.6</td>\n",
       "      <td>3.231135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>15.437763</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.337763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>14.098562</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.998562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>24.402208</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.202208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>19.201481</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.698519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>25.660192</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.660192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>18.043043</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.856957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predicted  Actual       MAE\n",
       "0   24.654657    23.6  1.054657\n",
       "1   31.528280    32.4  0.871720\n",
       "2   12.646070    13.6  0.953930\n",
       "3   22.472269    22.8  0.327731\n",
       "4   16.408958    16.1  0.308958\n",
       "5   19.212172    20.0  0.787828\n",
       "6   17.815571    17.8  0.015571\n",
       "7   13.489441    14.0  0.510559\n",
       "8   23.637598    19.6  4.037598\n",
       "9   19.240616    16.8  2.440616\n",
       "10  20.512531    21.5  0.987469\n",
       "11  17.341719    18.9  1.558281\n",
       "12   8.299414     7.0  1.299414\n",
       "13  18.929043    21.2  2.270957\n",
       "14  17.855413    18.5  0.644587\n",
       "15  23.568462    29.8  6.231538\n",
       "16  19.220367    18.8  0.420367\n",
       "17  10.418386    10.2  0.218386\n",
       "18  45.454643    50.0  4.545357\n",
       "19  14.743210    14.1  0.643210\n",
       "20  26.085329    25.2  0.885329\n",
       "21  25.954712    29.1  3.145288\n",
       "22  11.814768    12.7  0.885232\n",
       "23  24.248915    22.4  1.848915\n",
       "24  15.512203    14.2  1.312203\n",
       "25  14.958467    13.8  1.158467\n",
       "26  18.432352    20.3  1.867648\n",
       "27  10.753948    14.9  4.146052\n",
       "28  21.495802    21.7  0.204198\n",
       "29  18.002338    18.3  0.297662\n",
       "30  19.413502    23.1  3.686498\n",
       "31  23.207787    23.8  0.592213\n",
       "32  21.769140    15.0  6.769140\n",
       "33  20.301870    20.8  0.498130\n",
       "34  14.651910    19.1  4.448090\n",
       "35  16.198610    19.4  3.201390\n",
       "36  31.012062    34.7  3.687938\n",
       "37  21.125477    19.5  1.625477\n",
       "38  21.860136    24.4  2.539864\n",
       "39  23.321764    23.4  0.078236\n",
       "40  13.067485    19.7  6.632515\n",
       "41  29.861059    28.2  1.661059\n",
       "42  53.235943    50.0  3.235943\n",
       "43  17.584295    17.4  0.184295\n",
       "44  25.831135    22.6  3.231135\n",
       "45  15.437763    15.1  0.337763\n",
       "46  14.098562    13.1  0.998562\n",
       "47  24.402208    24.2  0.202208\n",
       "48  19.201481    19.9  0.698519\n",
       "49  25.660192    24.0  1.660192\n",
       "50  18.043043    18.9  0.856957"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "pd.DataFrame(np.hstack([Y_pred, Y_test.values[:, np.newaxis], np.abs(Y_pred-Y_test.values[:, np.newaxis])]),\n",
    "             columns=['Predicted', 'Actual', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
